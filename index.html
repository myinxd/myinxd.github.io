<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Canal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Valar morghulis, valar dohaeris.">
<meta property="og:type" content="website">
<meta property="og:title" content="Canal">
<meta property="og:url" content="http://www.mazhixian.me/index.html">
<meta property="og:site_name" content="Canal">
<meta property="og:description" content="Valar morghulis, valar dohaeris.">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Canal">
<meta name="twitter:description" content="Valar morghulis, valar dohaeris.">
  
    <link rel="alternate" href="/atom.xml" title="Canal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Canal</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Valar morghulis, valar dohaeris.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://www.mazhixian.me"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-20170917" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/17/20170917/" class="article-date">
  <time datetime="2017-09-17T08:47:48.000Z" itemprop="datePublished">2017-09-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/17/20170917/">20170917</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>没想好要起什么名字，姑且用日期来代替吧，记录一下此刻的感受。</p>
<p>在喧闹的咖啡馆里，周围有大一的孩子在玩三国杀; 有老外跟着中国妹子学中文; 有周末来聊工作的; 也有我这种逃离实验室来打酱油的。</p>
<p>有时候得承认，在喧闹的环境下工作效率并不一定低，好像还有相关的叫“白噪声”的理论。我的理解是，噪声一定程度上分散了我们的注意力，反而更容易关注工作本身了，诡异的理论，lol。</p>
<p>刚刚打印的路上路过东转，又遇到了那只白猫，晒着太阳，高冷而慵懒。这次她终于同意我拍照了，来看看异瞳的白猫(手动坏笑)。</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-170917/fig1.jpg?raw=true" height="640" width="360">
</center>

<p>最近试着恢复夜跑和走路上班，既然没有斗志工作，至少要把身体调整好吧。取悦别人很难，获得认可也很难，每个人有自己的生活，自己的圈子。为了证明自己而炫耀，反而成了小丑。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/17/20170917/" data-id="cj7ow858600003867gc50ui4z" class="article-share-link">Share</a>
      
        <a href="http://www.mazhixian.me/2017/09/17/20170917/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/life/">life</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Tensorflow-namespace-and-network-restoration" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/14/Tensorflow-namespace-and-network-restoration/" class="article-date">
  <time datetime="2017-09-14T08:20:37.000Z" itemprop="datePublished">2017-09-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/14/Tensorflow-namespace-and-network-restoration/">Tensorflow namespace and network restoration</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>昨天讨论了基于TensorFlow的迁移学习，提到了网络的存储和恢复的问题，然而我并没有说清楚，而且网络的恢复要考虑的问题其实挺多的。。。</p>
<p>概括为如下三个问题：</p>
<ol>
<li>变量的命名空间问题;</li>
<li>网络的恢复问题;</li>
<li>网络中tensor名称和数据的获取</li>
</ol>
<h4 id="变量命名空间"><a href="#变量命名空间" class="headerlink" title="变量命名空间"></a>变量命名空间</h4><p>首先，来说明命名空间的问题。通常我们利用<code>tf.Variable</code>或<code>tf.placeholder</code>等初始话一个变量，这两个类都有缺省的命名参数，例如<code>tf.Variable</code>的参数<code>name=Variable</code>。相应的，tf在我们新建变量的过程中按顺序在<code>Variable</code>后添加数字，例如<code>Variable_1</code>,<code>Variable_2</code>等。</p>
<p>这种缺省命名虽然方便，但对网络的存储和恢复会造成很大的影响，所以如果要保存我们的网络，最好的方法是给每个变量设置命名空间。这些变量包括<code>Variables</code>,<code>placeholder</code>以及<code>optimizer</code>。</p>
<h4 id="网络的恢复问题"><a href="#网络的恢复问题" class="headerlink" title="网络的恢复问题"></a>网络的恢复问题</h4><p>如上一篇<a href="http://www.mazhixian.me/2017/09/13/Transfer-learning-with-TensorFlow/">博客</a>所述，tensorflow的<code>tf.train.Saver</code>类既可以存储网络，也可以恢复网络。其中Saver保存的<code>.ckpt</code>文件包含<strong>checkpoint</strong>和<strong>metadata</strong>，分别存储了graph的命名空间和元数据。恢复的时候便是基于他们读取数据到网络中。</p>
<p><strong>但是</strong>，单有checkpoint和metadata是没有用的，tensorflow的核心就是graph，所以我们需要在恢复网络前重新搭建graph，并且这个graph的命名空间要与checkpoint的相同。因此，一定要养成好习惯，在定义网络的时候，给每个变量都设置固定的名称。</p>
<p>除此之外，在一个ipython环境或者notebook下，如果要保存网络，建议只搭建一个graph。因为我发现，<code>tf.train.Saver</code>类在保存checkpoint的时候，会将目前存在的graph全部保存。</p>
<p>下面给一段代码，从这篇<a href="http://blog.csdn.net/lwplwf/article/details/62419087" target="_blank" rel="external">文章</a>复制来的。。。</p>
<h5 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">v1 = tf.Variable(tf.random_normal([<span class="number">1</span>, <span class="number">2</span>]), name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">"v2"</span>)</div><div class="line">init_op = tf.global_variables_initializer() </div><div class="line">saver = tf.train.Saver()</div><div class="line">sess = tf.InteractiveSession()</div><div class="line">sess.run(init_op)</div><div class="line">savepath = <span class="string">"./model.ckpt"</span></div><div class="line">saver.save(sess, savepath)</div></pre></td></tr></table></figure>
<h5 id="恢复模型"><a href="#恢复模型" class="headerlink" title="恢复模型"></a>恢复模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">v1 = tf.Variable(tf.random_normal([<span class="number">1</span>, <span class="number">2</span>]), name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">"v2"</span>)</div><div class="line">saver = tf.train.Saver()</div><div class="line">sess = tf.InteractiveSession()</div><div class="line">modelpath = <span class="string">"./model.ckpt"</span></div><div class="line">sess = saver.restore(sess, modelpath)</div></pre></td></tr></table></figure>
<p>从上面可以看出，二者的区别在于<strong>是否需要初始化变量</strong>，这也是网络恢复的核心问题，因为我们的目的就是恢复参数。</p>
<h4 id="网络中tensor名称和数据的获取"><a href="#网络中tensor名称和数据的获取" class="headerlink" title="网络中tensor名称和数据的获取"></a>网络中tensor名称和数据的获取</h4><p>如何从存储的网络中提取变量的命名及其数据，可以参考下面的程序，也是<a href="http://blog.csdn.net/helei001/article/details/56489658" target="_blank" rel="external">抄来的</a>。。。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tensorflow.python <span class="keyword">import</span> pywrap_tensorflow  </div><div class="line">checkpoint_path = os.path.join(model_dir, <span class="string">"model.ckpt"</span>)  </div><div class="line">reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)  </div><div class="line">var_to_shape_map = reader.get_variable_to_shape_map()  </div><div class="line"><span class="keyword">for</span> key <span class="keyword">in</span> var_to_shape_map:  </div><div class="line">    print(<span class="string">"tensor_name: "</span>, key)  <span class="comment"># Ouput variables name</span></div><div class="line">    print(reader.get_tensor(key)) <span class="comment"># Output variables data</span></div></pre></td></tr></table></figure></p>
<p>输出结果类似这样，<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">tensor_name:  Conv_En_b2/cae-optimizer</div><div class="line">tensor_name:  Conv_De_b1/cae-optimizer_1</div><div class="line">tensor_name:  Conv_En_W0</div><div class="line">tensor_name:  De_b/cae-optimizer</div></pre></td></tr></table></figure></p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ul>
<li><a href="http://blog.csdn.net/lwplwf/article/details/62419087" target="_blank" rel="external">TensorFlow学习笔记（8）—网络模型的保存和读取</a></li>
<li><a href="http://blog.csdn.net/helei001/article/details/56489658" target="_blank" rel="external">查看TensorFlow checkpoint文件中的变量名和对应值</a></li>
<li><a href="http://www.mazhixian.me/2017/09/13/Transfer-learning-with-TensorFlow/">Transfer learning with TensorFlow</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/14/Tensorflow-namespace-and-network-restoration/" data-id="cj7ow858t000k38676nekuwlh" class="article-share-link">Share</a>
      
        <a href="http://www.mazhixian.me/2017/09/14/Tensorflow-namespace-and-network-restoration/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/">deep-learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Transfer-learning-with-TensorFlow" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/13/Transfer-learning-with-TensorFlow/" class="article-date">
  <time datetime="2017-09-13T08:07:45.000Z" itemprop="datePublished">2017-09-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/13/Transfer-learning-with-TensorFlow/">Transfer learning with TensorFlow</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Today, let’s talk about transfer learning based on TensorFlow. Firstly, what is transfer learning? It is a strategy of building your own deeplearning based project with existing well-trained networks, so as to avoid some risks like overfitting, time-consuming and etc.</p>
<p>In our work, we are trying to classify some astronomical images with convolutional neural networks (CNN). As we know, the CNN networks should be trained with billions of samples to achieve optimum the parameters of weights and biases. However, only thousands of labelled samples do we have, which can’t activate the performance of the network. Since that, we propose to train the network by transfer learning. </p>
<p>Here we come to the main body, i.e. how to realize transfer learning with your computer? In this blog, I’m going to tell you some tricks to realize this staff with <a href="https://www.tensorflow.org" target="_blank" rel="external">TensorFlow</a>, a famous deeplearning framework based on Python.</p>
<h4 id="1-How-to-save-and-restore-the-net"><a href="#1-How-to-save-and-restore-the-net" class="headerlink" title="1. How to save and restore the net?"></a>1. How to save and restore the net?</h4><p>In order to realize the transfer learning, your script should pocess the ability to save and restore the network. Bellow are the code example of this two processes.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># Save the session as a check point</span></div><div class="line">sess = tf.InteractiveSession() <span class="comment"># Instance a session</span></div><div class="line">saver = tf.train.Saver() <span class="comment"># Instance a saver</span></div><div class="line">saver.save(sess, savepath) <span class="comment"># save the session, i.e. the network</span></div><div class="line"></div><div class="line"><span class="comment"># Restore</span></div><div class="line"><span class="comment"># [Note] The saver should be initialized after the graph defined...</span></div><div class="line">graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> graph.as_default():</div><div class="line">    <span class="comment"># [Variable and model creation goes here.]</span></div><div class="line">    saver = tf.train.Saver()</div><div class="line">sess = tf. InteractiveSession()</div><div class="line">saver.restore(sess, netpath)</div></pre></td></tr></table></figure></p>
<p>It should be noted that, the instancing of <code>saver</code> should be after defination of the network. I find that though the restored session <code>sess</code> saved the graph of the newtork, including tensors, variables, and operations, the variables still need to be initialized when training after restoration.</p>
<h4 id="2-How-to-realize-the-transfer-learning"><a href="#2-How-to-realize-the-transfer-learning" class="headerlink" title="2. How to realize the transfer learning?"></a>2. How to realize the transfer learning?</h4><p>Here we come to the transfer learning. A typical process is that we save the paramters of the convolutional layers (ConvLayer), and replace the fully connected and output layers with new weights and biases. This is because the ConvLayers are the part to <strong>extract features</strong> of those samples, while the fully connected layers composing the <strong>classifier</strong> itself. Our target is to save the feature represantation part and update the classifier according to our project.</p>
<p>Denote the last output of the ConvLayers is a tensor with name <code>ConvLayer_output</code>, and suppose a session namely <code>sess</code> has been restored, then we can build our own classifier, see as follows,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Get tensor 'ConvLayer_output' from sess</span></div><div class="line">l_conv_output = sess.graph.get_tensor_by_name(<span class="string">"ConvLayer_output"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Add new fully connected layers and softmax layer</span></div><div class="line"><span class="comment"># Suppose we have 10 class to be classied</span></div><div class="line">numclass = <span class="number">10</span></div><div class="line">y_ = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, numclass], name=<span class="string">"cnn-softmax"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Add a fully connected as an example</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">	initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</div><div class="line">    <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</div><div class="line">    <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line">last_conv_shape = l_conv_output.get_shape().as_list()</div><div class="line">input_shape = last_conv_shape[<span class="number">1</span>] * last_conv_shape[<span class="number">2</span>] * last_conv_shape[<span class="number">3</span>]</div><div class="line">output_shape = <span class="number">1024</span></div><div class="line">W_fc = weight_variable(shape = [input_shape, output_shape])</div><div class="line">b_fc = bias_variable(shape=[output_shape])</div><div class="line">l_fc = tf.nn.relu(tf.matmul(l_conv_output, W_fc) + b_fc)</div><div class="line"></div><div class="line"><span class="comment"># Fully connected to softmax</span></div><div class="line">input_shape = <span class="number">1024</span></div><div class="line">output_shape = numclass</div><div class="line">W_soft = weight_variable(shape = [input_shape, output_shape])</div><div class="line">b_soft = bias_variable(shape = [output_shape])</div><div class="line">l_y = tf.nn.softmax(tf.matmul(l_fc, W_soft) + b_soft)</div></pre></td></tr></table></figure></p>
<p>After that, our new network can be trained on our samples,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Initialize all the parameters, including the pretrained net and concated layers</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line">sess.run(init_op)</div><div class="line"></div><div class="line"><span class="comment"># [Training lines goes here]</span></div></pre></td></tr></table></figure></p>
<p>Finally, we obtain the network, and evaluations can be conducted, enjoy yourselves.</p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ul>
<li><a href="https://stackoverflow.com/questions/36281129/no-variable-to-save-error-in-tensorflow" target="_blank" rel="external">No variable to save error in Tensorflow</a></li>
<li><a href="https://www.tensorflow.org" target="_blank" rel="external">Tensorflow</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/13/Transfer-learning-with-TensorFlow/" data-id="cj7ow858u000m3867umgvoyy3" class="article-share-link">Share</a>
      
        <a href="http://www.mazhixian.me/2017/09/13/Transfer-learning-with-TensorFlow/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/">deep-learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Fourier-transform-on-2D-image" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/12/Fourier-transform-on-2D-image/" class="article-date">
  <time datetime="2017-09-12T14:55:34.000Z" itemprop="datePublished">2017-09-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/12/Fourier-transform-on-2D-image/">Fourier transform on 2D image</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天讨论二维矩阵 (图像) 的Fourier transoform，这个问题缘起师兄的工作，“当图像的灰度波动较小，或者说只有大尺度结构时，它的傅立叶变换的图像会在中间(低频)区域出现一个十字。”而这个诡异的十字严重影响了后续的工作。为什么会有这个十字？我们有一个猜想，也试着证明了一下，但不确定是不是这个原因，先挖个坑再说。。。</p>
<p>基本的傅立叶变换我就不介绍了。它的定义很简单，<strong>用正交的余弦函数作为base，对时域或空域的信号进行分解，并在频域描述信号.</strong> 这样做的好处很多，例如能够实现信号的滤波、能够压缩信号、能够将卷积转化为乘法等。</p>
<p>我准备先零散地抛几个“有(wu)趣(liao)”的点出来，第一个就是二维傅立叶变换的<code>时移问题</code>，这里我觉得用<code>bias</code>来描述更好，很无聊，但它的推导还是挺有价值的。设二维实数空间<script type="math/tex">f(x,y), x,y\in R</script>，其傅立叶变换为<script type="math/tex">F(m,n)</script>，则有，</p>
<script type="math/tex; mode=display">
\begin{equation}
F(m,n) = \iint{f(x,y)e^{-j2\pi mx}e^{-j2\pi ny}}dydx,
\end{equation}</script><p>现假设在$x$和$y$方向的偏移(时移)分别为$b_x$和$b_y$，那么，Eq.(1)变为，</p>
<script type="math/tex; mode=display">
\begin{align}
F(m,n) &= \iint{f(x,y)e^{-j2\pi m(x-b_x)}e^{-j2\pi n(y-b_y)}}dydx \notag \\
       &= e^{-j2\pi(mb_x + nb_y)}\iint{f(x,y)e^{-j2\pi mx}e^{-j2\pi ny}}dydx.
\end{align}</script><p>而上式中的<script type="math/tex">e^{-j2\pi(mb_x + nb_y)}</script>利用欧拉公式展开，我们得到，</p>
<script type="math/tex; mode=display">
\begin{align}
e^{-j2\pi(mb_x + nb_y)} &= cos(2\pi(mb_x+nb_y)) - jsin(2\pi(mb_x+nb_y)).
\end{align}</script><p>显然，<script type="math/tex">mb_x+nb_y</script>可以拆分成向量$(m,n)$与$(b_x,b_y)$的内积，代表频域空间的向量<script type="math/tex">(m,n)</script>在偏移方向<script type="math/tex">(b_x,b_y)</script>上的投影，而<script type="math/tex">(b_x,b_y)</script>又共同构成了该种投影的周期性，导致在傅立叶变换的图像上出现周期性变化的明暗条纹。</p>
<p>说完时移的问题，我们来提出图像傅立叶变换以后<strong>诡异十字</strong>问题的猜想，<strong>图像边界处有明显的灰度变化会导致傅立叶变换后的频域图像出现十字。</strong>可以理解为在图像边界处人为做了<code>truncated</code>，即给图像加了矩形窗，导致在频域的<strong>vertical</strong>和<strong>horizontal</strong>两个方向乘了一个<code>sinc</code>函数，而由于这个窗很大，所以<code>sinc</code>的影响集中在靠近中心(低频)的区域。</p>
<p>先留个坑，明天加图。。。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/12/Fourier-transform-on-2D-image/" data-id="cj7ow858o000d3867imo801dw" class="article-share-link">Share</a>
      
        <a href="http://www.mazhixian.me/2017/09/12/Fourier-transform-on-2D-image/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/signal-processing/">signal-processing</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Data-visualization-with-t-SNE" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/11/Data-visualization-with-t-SNE/" class="article-date">
  <time datetime="2017-09-11T06:21:55.000Z" itemprop="datePublished">2017-09-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/11/Data-visualization-with-t-SNE/">Data visualization with t-SNE</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天我们讨论数据可视化(data visualization)算法t-SNE (t-distributed Stochastic Neighbor Embedding)，该方法的目的是<strong>映射高维数据向量到低维，并保留向量的相似性或者距离</strong>。通常我们描述距离会使用类似欧式距离或黎曼距离等概念，映射的方法也多为线性，如PCA。而t-SNE不同，它是用联合概率来描述样本点的相似程度，是非线性的。</p>
<p>SNE算法最早由 <a href="http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding" target="_blank" rel="external">Hinton &amp; Roweis</a> 在2002年提出。他们定义高维空间两个点<script type="math/tex">\bf{x_i}</script>和<script type="math/tex">\bf{x_j}</script>相似度的联合概率分布<script type="math/tex">P = p_{ij}</script>，和要映射到的低维空间对应两点<script type="math/tex">\bf{y_i}</script>和<script type="math/tex">\bf{y_j}</script>的联合概率<script type="math/tex">Q = q_{ij}</script>. 利用<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="external">Kullback-Leibler divergence</a>衡量分布$P$和$Q$的相似性，进而最小化这一距离来确定低维空间的映射点<script type="math/tex">\bf{y_i}</script>.</p>
<p>考虑到SNE算法存在三个缺陷,</p>
<ol>
<li>KL-divergence 不是对称的;</li>
<li>从高维空间映射到低维空间，距离会发生变化;</li>
<li>高斯分布不是长尾的，对小概率的异常点描述能力较差;</li>
</ol>
<p>为了解决这一问题，<a href="http://jmlr.csail.mit.edu/papers/v9/vandermaaten08a.html" target="_blank" rel="external">Van der Maaten &amp; Hinton</a> 在2008年提出了改进算法，利用自由度为1的t分布替换高斯分布来定义低维空间的距离，避免异常点，保留高维空间两点的距离关系；设计了具有对称性的概率分布函数。最后，分布$P$和$Q$，KL-divergence $\mathrm{KL}$ 以及对应的梯度公式<script type="math/tex">\partial{\mathrm{KL}}/\partial{y_i}</script>如下，</p>
<h5 id="高维空间分布-P"><a href="#高维空间分布-P" class="headerlink" title="高维空间分布$P$"></a>高维空间分布$P$</h5><script type="math/tex; mode=display">
\begin{equation}
p_{j|i} = \frac{\exp{(-\lVert{\bf{x_{i}} - \bf{x_{j}}}\rVert}^2 / 2\sigma^2_{i})}{\sum_{k\neq i }{\exp{(-\lVert{\bf{x_{i}} - \bf{x_{k}}}\rVert}^2 / 2\sigma^2_{i})}},
\end{equation}</script><p>其中，<script type="math/tex">p_{j|i}</script>表示<script type="math/tex">\bf{x_i}</script>接受<script type="math/tex">\bf{x_j}</script>为其同类点的条件概率，相应的<script type="math/tex">p_{ij}</script>由下式给出，</p>
<script type="math/tex; mode=display">
\begin{equation}
p_{ij} = \frac{p_{j|i} + p_{i|j}} {2N}.
\end{equation}</script><p>其中$N$表示样本点的个数。</p>
<h5 id="低维空间分布-Q"><a href="#低维空间分布-Q" class="headerlink" title="低维空间分布$Q$"></a>低维空间分布$Q$</h5><script type="math/tex; mode=display">
\begin{equation}
q_{ij} = \frac{(1 + {\lVert {\bf{y_{i}} - \bf{y_{j}}} \rVert}^2)^{-1}}{\sum_{k\neq m}{(1 + {\lVert {\bf{y_{k}} - \bf{y_{m}}} \rVert}^2)^{-1}}}.
\end{equation}</script><h5 id="KL-divergence"><a href="#KL-divergence" class="headerlink" title="KL-divergence"></a>KL-divergence</h5><script type="math/tex; mode=display">
\begin{equation}
\mathrm{KL}(P||Q) = \sum_{i\neq j}{p_{ij}\log{\frac{p_{ij}}{q_{ij}}}}.
\end{equation}</script><h5 id="partial-difference-of-KL-over-bf-y-i"><a href="#partial-difference-of-KL-over-bf-y-i" class="headerlink" title="partial difference of KL over $\bf{y_i}$"></a>partial difference of KL over $\bf{y_i}$</h5><script type="math/tex; mode=display">
\begin{equation}
\frac{\partial{\mathrm{KL}}}{\partial{\bf{y_{i}}}} = 4\sum_{j}{(p_{ij}-q_{ij})(y_{i}-y_{j})(1+(\lVert{y_{i} - y_{j}})^{2})^{-1}},
\end{equation}</script><p>基于以上公式，利用梯度进行优化，求解局部最优，即可对$\bf{y_i}$进行定位。</p>
<h4 id="t-SNE的python实现"><a href="#t-SNE的python实现" class="headerlink" title="t-SNE的python实现"></a>t-SNE的python实现</h4><p>python的<a href="http://scikit-learn.org/" target="_blank" rel="external">Scikit-learn</a>提供了TSNE类，用于自动化实现数据可视化，代码样例如下，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dimension_reduction_tSNE</span><span class="params">(code,params=None)</span>:</span></div><div class="line">	tsne = TSNE()</div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> params.keys():</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            setattr(tsne, key, params[<span class="string">'key'</span>])</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            <span class="keyword">continue</span></div><div class="line">    code_dim = tsne.fit_transform(code)</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> code_dim</div><div class="line"></div><div class="line">code = xxx</div><div class="line">params=&#123;<span class="string">'n_components'</span>: <span class="number">2</span>, <span class="string">'learning_rate'</span>: <span class="number">100</span>&#125;</div><div class="line"></div><div class="line">code_dim = dimension_reduction_tSNE(code, params)</div></pre></td></tr></table></figure></p>
<p>其中<code>code</code>表示高维空间的样本矩阵，每行对应一个样本。参数<code>params</code>里的<code>n_components</code>表示低维空间的维数。</p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ul>
<li><a href="http://bindog.github.io/blog/2016/06/04/from-sne-to-tsne-to-largevis/" target="_blank" rel="external">从SNE到t-SNE再到LargeVis</a></li>
<li><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" target="_blank" rel="external">t-distributed stochastic neighbor embedding</a></li>
<li><a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution" target="_blank" rel="external">Student’s t-distrubiton</a> </li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/11/Data-visualization-with-t-SNE/" data-id="cj7ow858l00083867bbj3wrpj" class="article-share-link">Share</a>
      
        <a href="http://www.mazhixian.me/2017/09/11/Data-visualization-with-t-SNE/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/">deep-learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Catch-data-with-provided-keys" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/09/Catch-data-with-provided-keys/" class="article-date">
  <time datetime="2017-09-09T14:51:49.000Z" itemprop="datePublished">2017-09-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/09/Catch-data-with-provided-keys/">Catch data with provided keys</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>晚上粗去吃大餐了，走了很多路，超级开心。。。今天还是写个短博客，讲一下怎么从文件里根据关键词提取相关信息。举个例子，比如我们订阅了<a href="https://www.arxiv.org" target="_blank" rel="external">arXiv</a>的arXiv.cs板块，那么每天会收到一封邮件，如果想批量且自动化地下载这些论文，应该怎么做呢？ 有一些解决方案，比如提取每篇摘要文末的链接，利用python的url,requests等包进行下载; 比如利用linux的wget工具进行下载，但相对来说，这两种方案是比较难入门的，我到现在都写不好。。。那么，是否有更简单的解决方案呢？ 答案是用正则表达式<code>re</code>.</p>
<p>以arXiv为例，每篇文章都有一个确定的ID,形如<code>17MM.xxxxx</code>，其中MM表示月份，xxxxx表示五位数的编号。那么，其对应的正则表达式可以用如下字符串表示，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">s = <span class="string">r"[0-9][0-9][0-9][0-9].[0-9][0-9][0-9][0-9][0-9]"</span></div></pre></td></tr></table></figure></p>
<p>然后利用python的正则表达式包<a href="https://docs.python.org/3/library/re.html" target="_blank" rel="external">re</a>，用如下的语句从邮件里抓取即可，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">key = <span class="string">r"[0-9][0-9][0-9][0-9].[0-9][0-9][0-9][0-9][0-9]"</span></div><div class="line">filename = <span class="string">"cs-arxiv.txt"</span></div><div class="line"><span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> fp:</div><div class="line">	lines = fp.readlines()</div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> lines:</div><div class="line">    	s = re.findall(key,l)</div><div class="line">        <span class="keyword">if</span> len(s):</div><div class="line">        	print(<span class="string">"Find id %s"</span> % s[<span class="number">0</span>])</div></pre></td></tr></table></figure></p>
<p>其中最关键的语句是第7行，即在邮件中按行搜索key制定的关键词或者关键词格式。详细的下载程序可以参考我的程序<a href="https://github.com/myinxd/canal-tools/blob/master/python-based/download-arxiv-paper.py" target="_blank" rel="external">download-arxiv-paper.py</a>. 根据提供的关键词进行数据提取，可以参考这个程序<a href="https://github.com/myinxd/gastrack/blob/master/utils/catch-with-keys.py" target="_blank" rel="external">catch-with-keys.py</a>.</p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ul>
<li>[1] <a href="https://docs.python.org/3/library/re.html" target="_blank" rel="external">https://docs.python.org/3/library/re.html</a></li>
<li>[2] <a href="http://www.cnblogs.com/jiu0821/p/6275685.html" target="_blank" rel="external">http://www.cnblogs.com/jiu0821/p/6275685.html</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/09/Catch-data-with-provided-keys/" data-id="cj7ow858i00053867gg1dffor" class="article-share-link">Share</a>
      
        <a href="http://www.mazhixian.me/2017/09/09/Catch-data-with-provided-keys/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-matplotlib-tips-I" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/08/matplotlib-tips-I/" class="article-date">
  <time datetime="2017-09-08T15:00:34.000Z" itemprop="datePublished">2017-09-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/08/matplotlib-tips-I/">matplotlib tips I</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>感冒差不多好了，又可以出去跑步啦！下周要开学了，路上看到好多新生。从他们身边路过，感觉离他们的世界好远，好像自己是隐身的，祝他们能找到自己想要过的生活吧。言归正传，睡前还是要水一篇博客的，虽然是周五，虽然明天还是要加班。。。今天的主题是<code>matplotlib.pyplot</code>做图的tips，准备写成一个系列，先开个头。积少成多吧。</p>
<h5 id="1-matplotlib的默认字体"><a href="#1-matplotlib的默认字体" class="headerlink" title="1. matplotlib的默认字体"></a>1. matplotlib的默认字体</h5><p>首先，我们从字体开始。matplotlib的默认字体和MATLAB一样，是<strong>Helvetica</strong>，一种和<strong>Arial</strong>长得非常像的字体，<a href="https://www.zhihu.com/question/19562456" target="_blank" rel="external">知乎</a>上有类似的讨论，多数认为Helvetica比较好看，LOL. 我倒是两种都挺喜欢的，论文里也经常用。但这里要注意， 这两种字体在IEEE的pdf格式审查里会出问题，可能事因为Adobe并不会自动嵌入这两种字体。。。</p>
<h5 id="2-colorbar、坐标轴标注的问题"><a href="#2-colorbar、坐标轴标注的问题" class="headerlink" title="2. colorbar、坐标轴标注的问题"></a>2. colorbar、坐标轴标注的问题</h5><p>我们知道，matplotlib和matlab中colorbar、坐标轴的标签(label)和刻度(tick)的参数名称和配置方法是类似的，通常都是<code>set_xxx</code>的格式。在matplotlib中tick的缺省值是根据数据自动添加的，而label默认没有，他们的设置样例如下，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">plt.set_xticks([<span class="number">0.0</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,]) <span class="comment"># 修改刻度</span></div><div class="line">plt.set_xlabel(<span class="string">"X axis"</span>) <span class="comment"># 修改label</span></div></pre></td></tr></table></figure></p>
<p>这里要注意，类似colorbar这种对象，它的设置等同于坐标轴，也有<code>set_label</code>方法。</p>
<p>先写两点吧，前面写过一篇关于<a href="http://www.mazhixian.me/2017/08/24/matplotlib-AxesGrid/">AxesGrid</a>的，有兴趣也可以看看。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/08/matplotlib-tips-I/" data-id="cj7ow858x000s3867rmki7y4p" class="article-share-link">Share</a>
      
        <a href="http://www.mazhixian.me/2017/09/08/matplotlib-tips-I/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Install-R-on-Ubuntu16-04-and-configure-jupyter-notebook" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/06/Install-R-on-Ubuntu16-04-and-configure-jupyter-notebook/" class="article-date">
  <time datetime="2017-09-06T15:52:22.000Z" itemprop="datePublished">2017-09-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/06/Install-R-on-Ubuntu16-04-and-configure-jupyter-notebook/">Install R on Ubuntu16.04 and configure jupyter-notebook</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这两天在写论文，天天写程序，没什么新脑洞，而且最近也遇到不少事。手动转载一下几篇新浪博客的文章，有几篇的阅读量和转载还挺不错。。。只是当时的文风也是醉了。</p>
<p>第二篇是“[R]Ubuntu-16.04 R 安装及Jupyter notebook 配置” （<a href="http://blog.sina.com.cn/s/blog_9b6253b10102xu8s.html" target="_blank" rel="external">原文链接</a>)。</p>
<h4 id="问题：-Ubuntu-16-04-及-16-10-配置R语言环境，-以及在Jupyter中添加R-kernel"><a href="#问题：-Ubuntu-16-04-及-16-10-配置R语言环境，-以及在Jupyter中添加R-kernel" class="headerlink" title="问题： Ubuntu-16.04 及 16.10 配置R语言环境， 以及在Jupyter中添加R kernel."></a>问题： Ubuntu-16.04 及 16.10 配置R语言环境， 以及在Jupyter中添加R kernel.</h4><p>解决方法如下，</p>
<h5 id="1-R-安装"><a href="#1-R-安装" class="headerlink" title="1. R 安装"></a>1. R 安装</h5><p>通常在Terminal下直接apt-get 即可，在16.10下可以get到R-3.3.1，目前最新好像是 R-3.4.2，可以去官方网站下载源码编译 (<a href="https://www.r-project.org)​" target="_blank" rel="external">https://www.r-project.org)​</a><br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install r-base</div></pre></td></tr></table></figure></p>
<h5 id="2-在jupyter-notebook中配置R的kernel"><a href="#2-在jupyter-notebook中配置R的kernel" class="headerlink" title="2. 在jupyter notebook中配置R的kernel"></a>2. 在jupyter notebook中配置R的kernel</h5><p>近期在Jupyter下跑python，方便边调试边记笔记。因为Jupyter也支持R语言，所以尝试添加R kernel到Jupyter中。 主要依赖 “IRkernel” 包。(有些包可能需要翻墙才能下载，具体的方法请DIY…) 配置方法如下：</p>
<p>(1) 安装必要的lib</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install libzmq3-dev libssl-dev libcurl​4-openssl-dev</div><div class="line">$ sudo R <span class="comment"># 进入R环境</span></div></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; install.packages(c(<span class="string">'repr'</span>, <span class="string">'IRdisplay'</span>, <span class="string">'evaluate'</span>, <span class="string">'crayon'</span>, <span class="string">'pbdZMQ'</span>, <span class="string">'devtools'</span>, <span class="string">'uuid'</span>, <span class="string">'digest'</span>))</div></pre></td></tr></table></figure>
<p><strong>注意</strong>： 会提示选择mirrors， 建议选择 0-cloud<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; devtools::install_github(<span class="string">'IRkernel/IRkernel'</span>)</div><div class="line">&gt; IRkernel::installspec()</div></pre></td></tr></table></figure></p>
<p>如果显示​如下结果，说明配置成功。<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[InstallKernelSpec] Installed kernelspec ir <span class="keyword">in</span> /home/xxx/.local/share/jupyter/kernels/ir​</div></pre></td></tr></table></figure></p>
<h5 id="References"><a href="#References" class="headerlink" title="References"></a>References</h5><p>[1] <a href="https://irkernel.github.io/installation/" target="_blank" rel="external">https://irkernel.github.io/installation/</a></p>
<p>[2] <a href="http://blog.csdn.net/reallocing1/article/details/51396539" target="_blank" rel="external">http://blog.csdn.net/reallocing1/article/details/51396539</a></p>
<p>[3] <a href="http://blog.csdn.net/songying2012/article/details/51123475" target="_blank" rel="external">http://blog.csdn.net/songying2012/article/details/51123475</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/06/Install-R-on-Ubuntu16-04-and-configure-jupyter-notebook/" data-id="cj7ow858q000g3867342rnu6k" class="article-share-link">Share</a>
      
        <a href="http://www.mazhixian.me/2017/09/06/Install-R-on-Ubuntu16-04-and-configure-jupyter-notebook/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/R/">R</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-BP神经网络sim函数的MATLAB和C实现" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/06/BP神经网络sim函数的MATLAB和C实现/" class="article-date">
  <time datetime="2017-09-06T14:24:10.000Z" itemprop="datePublished">2017-09-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/06/BP神经网络sim函数的MATLAB和C实现/">BP神经网络sim函数的MATLAB和C实现</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这两天在写论文，天天写程序，没什么新脑洞，而且最近也遇到不少事。手动转载一下几篇新浪博客的文章，有几篇的阅读量和转载还挺不错。。。只是当时的文风也是醉了。</p>
<p>第一篇是“BP神经网络sim函数的MATLAB和C实现” （<a href="http://blog.sina.com.cn/s/blog_9b6253b10101fvyq.html" target="_blank" rel="external">原文链接</a>)，争取做一些改写，尤其在排版上规范一点。</p>
<h5 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h5><p>BP神经网络属于ANN的一种，相对于DNN，CNN而言，是比较浅层的分类器。不过BPNN由于参数比较少，网络简单，还是有广泛的应用，尤其在传统的<strong>人工特征提取加分类器</strong>的算法中。因为这篇文章是在本科毕业设计的时候写的，当时还用了SVM，这里插一句我的观点，“SVM虽然调参比较复杂，网格法耗时，GA又存在局部最优的问题，但如果特征选择比较合理，它的表现还是非常有竞争力的”。</p>
<p>言归正传，由于要使用C++写界面，很希望实现神经网络的MATLAB和C的混合编程，问了度娘以后，发现不可以，因为MATLAB的神经网络库有专利，具体的解释见<a href="http://www.ilovematlab.cn/thread-103075-1-1.html" target="_blank" rel="external">http://www.ilovematlab.cn/thread-103075-1-1.html</a></p>
<p>经过多次尝试后，决定采用“曲线救国”的方式，即利用MATLAB进行训练，再手动编写C代码，实现用于预测结果的sim函数。本次网络，采用newff函数建立神经网络net，输入层的传递函数为tansig，隐含层的传递函数为purelin.(若输入层采用logsig函数，方法类似)。<strong>注意</strong>: newff在新的MATLAB版本中已经有更新，具体更新可以去官网看。推荐<a href="https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/dp/B00EEWKKM8/" target="_blank" rel="external">《MATLAB神经网络43个案例分析》</a></p>
<p>下面以三层BP神经网络为例，sim函数的MATLAB实现如下：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">out</span> = <span class="title">myBPSim</span><span class="params">(net, Test_data)</span></span></div><div class="line"><span class="comment">% out = myBPSim(net, Test_data)</span></div><div class="line"><span class="comment">% Test_data，待分类的数据，每行表示一个特征向量</span></div><div class="line">IW = net.IW&#123;<span class="number">1</span>,<span class="number">1</span>&#125;;  <span class="comment">% net是训练得到的网络，IW表示隐含层的权矩阵</span></div><div class="line">                   <span class="comment">% 维数 = 隐含层神经元个数 * 特征数</span></div><div class="line">LW = net.LW&#123;<span class="number">2</span>,<span class="number">1</span>&#125;;  <span class="comment">% LW表示隐含层权矩阵，维数 = 1 * 隐含层神经元个数</span></div><div class="line">b1 = net.b&#123;<span class="number">1</span>,<span class="number">1</span>&#125;    <span class="comment">% 输入层的阈值</span></div><div class="line">b2 = net.b&#123;<span class="number">2</span>,<span class="number">1</span>&#125;    <span class="comment">% 隐含层的阈值</span></div><div class="line">n1 = (IW * Test_data) + b1;</div><div class="line">out1 = <span class="number">2</span>/(<span class="number">1</span> + <span class="built_in">exp</span>(<span class="number">2</span> * n1)) - <span class="number">1</span>;    <span class="comment">% tansig函数的表达式，out1表示输入层的输出结果</span></div><div class="line">out2 = LW * out1 + b2;             <span class="comment">% purelin函数就是形如 y = x，所以直接可以得到out2</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>有一点需要注意的是，<a href="https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/dp/B0142K5QM0/" target="_blank" rel="external">《MATLAB智能算法——30个案例分析》</a> 这本书的第25章P238存在错误，其中的式25-3和式25-4中的<strong>减号</strong>应改为<strong>加号</strong>，否则与实际结果有较大误差。</p>
<p>C语言对sim的实现与以上程序类似，需要注意的是矩阵的乘法，由于matlab采用了优化，所以其计算矩阵乘法的速度相对于C要快很多，我没有找到很好的解决方法，采用的仍然是无脑的for循环, C++代码样例如下，<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span> CClassify::DoClassify(IplImage *Src)</div><div class="line">&#123;	</div><div class="line">    xxx <span class="comment">// Some lines for feature generation</span></div><div class="line">	<span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; <span class="number">9</span>; i++)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; <span class="number">89</span>; j++)</div><div class="line">		&#123;</div><div class="line">			net1[i] += img_mat[j]*IW[j][i];</div><div class="line">		&#125;</div><div class="line">		net1[i] = <span class="number">2.00</span> /(<span class="number">1</span>+<span class="built_in">exp</span>(<span class="number">-2</span>*(net1[i] + b1[i]))) <span class="number">-1</span>;  <span class="comment">// tansig</span></div><div class="line">	&#125;</div><div class="line">	<span class="keyword">for</span>(i =<span class="number">0</span>; i &lt; <span class="number">9</span>; i++)</div><div class="line">	&#123;</div><div class="line">		result += net1[i]*LW[i];</div><div class="line">	&#125;</div><div class="line">	result = result + b2;</div><div class="line"></div><div class="line">	<span class="keyword">if</span>(result &gt; <span class="number">0</span>) Alm_flag = <span class="number">1</span>;</div><div class="line">	<span class="keyword">else</span> Alm_flag = <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/06/BP神经网络sim函数的MATLAB和C实现/" data-id="cj7ow858g0003386795ph9ojr" class="article-share-link">Share</a>
      
        <a href="http://www.mazhixian.me/2017/09/06/BP神经网络sim函数的MATLAB和C实现/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MATLAB/">MATLAB</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Two-typical-tensorflow-exceptions" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/04/Two-typical-tensorflow-exceptions/" class="article-date">
  <time datetime="2017-09-04T12:32:22.000Z" itemprop="datePublished">2017-09-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/04/Two-typical-tensorflow-exceptions/">Two typical tensorflow exceptions</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>几天没写东西了，想写自己的故事，又太矫情。今天的故事不长，主要是程序中碰到的两个Bug，记录一下解决方法。这两天在做CAE的pretrain和fine-tuning，白天给老板解释了一遍，感觉他听懂了。。。</p>
<p>两个Bug的Exception如下，我会逐个解释，</p>
<ol>
<li>ValueError: No gradients provided for any variable</li>
<li>FailedPreconditionError: Attempting to use uninitialized value xxx</li>
</ol>
<h4 id="ValueError-No-gradients-provided-for-any-variable"><a href="#ValueError-No-gradients-provided-for-any-variable" class="headerlink" title="ValueError: No gradients provided for any variable"></a>ValueError: No gradients provided for any variable</h4><p>首先交代一下出错的语境，这里我在用<code>tf.nn.softmax_cross_entropy_with_logits</code>定义网络的损失函数，我的写法如下, 其中<code>y</code>表示网络预测的(输出的)labels, <code>y_</code>表示真实的labels。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.nn.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_))</div></pre></td></tr></table></figure></p>
<p>紧接着，实例化网络并进行测试，然后程序报错了，<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ValueError: No gradients provided <span class="keyword">for</span> any variable</div></pre></td></tr></table></figure></p>
<p>具体的内容我就不截图了，因为论文还没投稿。这个错误的含义是网络中存在不能进行<strong>梯度传递</strong>的变量或者说是tensor。而导致这一错误的问题是我将<code>labels</code>和<code>logits</code>两个参数理解反了，这里的正确写法是，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.nn.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))</div></pre></td></tr></table></figure></p>
<p>也即<code>logits</code>代表网络的输出。除此之外，还要注意<code>y</code>和<code>y_</code>均为one hot 形式，下面提供了向量形式转one hot形式的代码，实现方法比较naive。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vec2onehot</span><span class="params">(label,numclass)</span>:</span></div><div class="line">    label_onehot = np.zeros((len(label),numclass))</div><div class="line">    <span class="keyword">for</span> i,l <span class="keyword">in</span> enumerate(label):</div><div class="line">        label_onehot[i, int(l)] = <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> label_onehot</div></pre></td></tr></table></figure></p>
<h4 id="FailedPreconditionError-Attempting-to-use-uninitialized-value-xxx"><a href="#FailedPreconditionError-Attempting-to-use-uninitialized-value-xxx" class="headerlink" title="FailedPreconditionError: Attempting to use uninitialized value xxx"></a>FailedPreconditionError: Attempting to use uninitialized value xxx</h4><p>这个错误隐藏地比较深，诱发它的机制是我在做fine tuning时定义了新的需要训练的变量，但没有在<code>session</code>中进行初始化。报错的样例如下所示，<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">FailedPreconditionError: Attempting to use uninitialized value beta1_power_1</div><div class="line">	[[Node: beta1_power_1/<span class="built_in">read</span> = Identity[T=DT_FLOAT, _class=[]...]]]</div></pre></td></tr></table></figure></p>
<p>我的解决方法比较暴力，在做预训练之前定义好fine-tuning要用到的变量，与其他的变量一起做初始化。这样后面在重新训练的时候，直接调用<code>sess.run</code>即可。</p>
<h4 id="Tip"><a href="#Tip" class="headerlink" title="Tip"></a>Tip</h4><p>单独列一个tip, 如果要多次训练网络，或者运行tensorflow的graph,建议在实例化session的时候用<code>InteractiveSession</code>而不是<code>Session</code>。</p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ul>
<li>[1] <a href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits" target="_blank" rel="external">tf.nn.softmax_cross_entropy_with_logits</a></li>
<li>[2] <a href="https://stackoverflow.com/questions/38778760/tensorflow-no-gradients-provided-for-any-variable" target="_blank" rel="external">Tensorflow: No gradients provided for any variable</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/04/Two-typical-tensorflow-exceptions/" data-id="cj7ow858v000p3867s89yzo3i" class="article-share-link">Share</a>
      
        <a href="http://www.mazhixian.me/2017/09/04/Two-typical-tensorflow-exceptions/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/">deep-learning</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/MATLAB/">MATLAB</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learning/">deep-learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/life/">life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/signal-processing/">signal-processing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/strange-hobbies/">strange-hobbies</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/MATLAB/" style="font-size: 10px;">MATLAB</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/deep-learning/" style="font-size: 20px;">deep-learning</a> <a href="/tags/hexo/" style="font-size: 13.33px;">hexo</a> <a href="/tags/life/" style="font-size: 16.67px;">life</a> <a href="/tags/python/" style="font-size: 16.67px;">python</a> <a href="/tags/signal-processing/" style="font-size: 10px;">signal-processing</a> <a href="/tags/strange-hobbies/" style="font-size: 10px;">strange-hobbies</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/09/17/20170917/">20170917</a>
          </li>
        
          <li>
            <a href="/2017/09/14/Tensorflow-namespace-and-network-restoration/">Tensorflow namespace and network restoration</a>
          </li>
        
          <li>
            <a href="/2017/09/13/Transfer-learning-with-TensorFlow/">Transfer learning with TensorFlow</a>
          </li>
        
          <li>
            <a href="/2017/09/12/Fourier-transform-on-2D-image/">Fourier transform on 2D image</a>
          </li>
        
          <li>
            <a href="/2017/09/11/Data-visualization-with-t-SNE/">Data visualization with t-SNE</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Jason Ma<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'mazhixian';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>