
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jason&#39;s blog">
    <title>Jason&#39;s blog</title>
    <meta name="author" content="Jason Ma">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <meta name="description" content="Valar morghulis, valar dohaeris.">
<meta property="og:type" content="blog">
<meta property="og:title" content="Jason&#39;s blog">
<meta property="og:url" content="http://www.mazhixian.me/index.html">
<meta property="og:site_name" content="Jason&#39;s blog">
<meta property="og:description" content="Valar morghulis, valar dohaeris.">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jason&#39;s blog">
<meta name="twitter:description" content="Valar morghulis, valar dohaeris.">
    
    
        
    
    
        <meta property="og:image" content="http://www.mazhixian.me/assets/images/profile.jpg"/>
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-pz4cc6y13wt2trzqa8l3n9v0yykr0sstdaheem7qj628nhjmhp9pfawvqawz.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-112822605-1', 'auto');
        ga('send', 'pageview');
    </script>


    
    <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?20f3bcc8683b066ff79f4e36134e3982";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="1">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Jason&#39;s blog</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
            <img class="header-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="1">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="/#about">
                    <img class="sidebar-profile-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Jason Ma</h4>
                
                    <h5 class="sidebar-profile-bio"><p>We are in the same story.</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="Categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="Search"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Search</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/myinxd" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fa fa-lg fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:zx@mazhixian.me" target="_blank" rel="noopener" title="Mail">
                    
                        <i class="sidebar-button-icon fa fa-lg fa-envelope-o" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/404.html"
                            
                            title="Naive"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-hourglass-start" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Naive</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="RSS"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="1"
                 class="
                        hasCoverMetaIn
                        ">
                <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/17/confusion-matrix-and-generation-with-tensorflow/">
                            Confusion matrix and generation with TensorFlow
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-17T14:45:29+08:00">
	
		    Mar 17, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>简单记录一下多分类问题中的一种评价方法及其可视化，混淆矩阵 (confusion matrix, CM)。首先给出其定义及作用，然后给出样例。</p>
<h4 id="Confusion-matrix"><a href="#Confusion-matrix" class="headerlink" title="Confusion matrix"></a>Confusion matrix</h4><p>混淆矩阵通过比较分类器的<strong>预测</strong>和<strong>真实</strong>标签，评估分类器效果，通常用于多分类问题。TF的<a href="https://www.tensorflow.org/tutorials/audio_recognition" target="_blank" rel="external">audio recognition tutorial</a>里对混淆矩阵的评价是, <em>This matrix can be more useful than just a single accuracy score because it gives a good summary of what mistakes the network is making.</em> 即混淆矩阵可以帮助分析分类器在哪些类上的表现最差。相对于单一的准确率这种衡量指标，更加直观。</p>
<p>混淆矩阵是一个二维的方阵，横轴代表真实标签，枞轴代表预测标签，其中的元素<script type="math/tex">CM_{ij}</script>代表实际为第<script type="math/tex">i</script>，被分成第<script type="math/tex">j</script>类的样本数目。<strong>显然矩阵的非零元素集中在对角线时，分类器的表现更优异。</strong></p>
<h4 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h4><p>下面给出一个样例，这是一个六分类的问题，结果来自我们近期的工作。其中A的样本量约1000, B的样本量约10000，A random是随机产生预测标签的CM矩阵。如下所示，A的CM map中，多数样本集中在对角线；B的表现也不错，但(6,6)的色块显然淡了很多，说明分类器对第六类的分类效果不好；而A random，因为是随机生成的，其CM的各个元素的样本数比较均衡，因此分类准确率也非常差。</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/images/fig-confmat.png?raw=true" height="200" width="720">
</center>

<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] <a href="https://www.tensorflow.org/tutorials/audio_recognition" target="_blank" rel="external">Simple Audio Recognition</a><br>[2] <a href="https://www.tensorflow.org/api_docs/python/tf/confusion_matrix" target="_blank" rel="external">tf.confusion_matrix</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/17/confusion-matrix-and-generation-with-tensorflow/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/17/simple-speech-recognition-example-by-tensorflow/">
                            A simple audio recognition example by tensorflow
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-17T11:17:02+08:00">
	
		    Mar 17, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    
                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/17/simple-speech-recognition-example-by-tensorflow/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/15/recurrent-neural-network-and-lstm/">
                            Recurrent neural network and LSTM
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-15T09:56:04+08:00">
	
		    Mar 15, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>简单记录一下循环神经网络 (recurrent neural network, RNN)， 另一种RNN，主要关注时间序列的预测、分类和识别等问题。这里卖个瓜，前面有讨论过残差神经网络，感兴趣地可以去围观，链接见文末。<br>本文首先讨论RNN的motivation及其特点；然后是为了解决长程依赖 (long-term dependencies) 而提出的long short term memory (LSTM) 结构； 最后是二者在tensorflow中的简单样例。参考文献很多，这里强烈案例下面两篇文章！</p>
<ul>
<li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM</a></li>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
</ul>
<h4 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a>Recurrent neural network</h4><p>循环神经网络的动机是<em>刻画一个时间序列当前输入与此前信息的联系</em>，从网络结构上，循环神经网络通过称为<strong>循环体</strong>的模块 (如下图) 实现对信息的记忆，即该层在<script type="math/tex">t-1</script>时刻的输出状态<script type="math/tex">\mathbf{h_{t-1}}</script>会被记录，并作为<script type="math/tex">t</script>时刻该模块输入的一部分，以级联的形式与<script type="math/tex">\mathbf{x_t}</script>构成此刻的输入 <script type="math/tex">[\mathbf{h_{t-1}}, \mathbf{x_{t}}]</script>.</p>
<p>显然，循环体中的循环理论上是无穷的，但在实际应用中会限制循环的次数以避免<strong>梯度消失 (gradient vanishing) </strong>的问题，用<code>num_step</code>来定义，即循环体的基本模块被复制并展开为<code>num_step</code>个。如<br>文献[4]所述，循环体结构是RNN的基础，在RNN中对于复制展开的循环体，其参数是共享的。这一点与卷积神经网络中的权值共享有类似之处。在<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">这篇</a>文章里给出了非常多的RNN的应用场景，我很喜欢里面关于手写体识别的问题，体现了权值共享的效果。</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/images/fig-rnn.png?raw=true" height="140" width="500">
</center>

<p>设<script type="math/tex">t</script>时刻循环体的输入为<script type="math/tex">\mathbf{x}(t)</script>，$t-1$时刻循环体的输出状态为<script type="math/tex">\mathbf{h(t)}</script>，则RNN中<script type="math/tex">t</script>时刻的输出<script type="math/tex">\mathbf{h}(t)</script>为，</p>
<script type="math/tex; mode=display">
\begin{equation}
\mathbf{h}(t) = \rm{tanh}\left(W\cdot[\mathbf{h}(t-1),\mathbf{x}(t)] + b\right).
\end{equation}</script><p>其中<script type="math/tex">W</script>是权值矩阵，其shape为<script type="math/tex">[\mathrm{len}(\mathbf{h}) + \mathrm{len}(\mathbf{x}), \mathrm{len}(\mathbf{h})]</script>, <script type="math/tex">b</script>为偏置。这里采用的激活函数是tanh，将数据限制到<script type="math/tex">[-1,1]</script>之间。</p>
<p>那么，为什么用tanh，而不是有high reputation的ReLU? 知乎的<a href="https://www.zhihu.com/question/61265076/answer/186644426" target="_blank" rel="external">这个讨论</a>给出了很棒的解释，参考Hinton<a href="https://arxiv.org/abs/1504.00941" target="_blank" rel="external">论文</a>中的观点 <em>ReLUs seem inappropriate for RNNs because they can have very large outputs so they might be expected to be far more likely to explode than units thathave bounded values.</em> ReLU将输出的值限制在<script type="math/tex">[0, \infty)</script>之间，而RNN中循环体之间的权值是共享的，经过公式(1)的多次作用，相当于对<script type="math/tex">W</script>做了连乘，ReLU函数会导致梯度爆炸的问题。因此，采用tanh可以将每层的输出空控制在限定的范围内，既避免了梯度消失，也避免了梯度爆炸的问题。</p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>在时间序列的预测中存在长期依赖 (long-term dependencies) 的问题，即网络需要记住离时间<script type="math/tex">t</script>很远的某个时刻的信息，固定的<code>num_step</code>将不适用于这一情形，并且长时间间隔下的梯度消失问题将无法处理。因此，需要对RNN的循环体模块进行修改，即长短时记忆网络 (long short term memory, LSTM). </p>
<p>LSTM的基本模块如下图所示，参考<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM</a>的解释，<em>The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt.</em> 即LSTM的核心是组成为<code>cell state</code>，用于存储和记忆上文的信息，类似传送带的功能。</p>
<p>而<code>cell state</code>的更新，通过三个逻辑门以及<script type="math/tex">\mathbf{x_t}</script>和<script type="math/tex">\mathbf{h_{t-1}}</script>共同完成。它们分别称为 (1) forget gate, (2) input gate, (3) output gate. 采用sigmoid函数将数值规范化到<script type="math/tex">[0,1]</script>区间，并与待处理信号进行点乘，本质上实现软判决.</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/images/fig-lstm.png?raw=true" height="400" width="640">
</center>

<p>与RNN类似，首先将<script type="math/tex">t-1</script>时刻LSTM cell的输出<script type="math/tex">\mathbf{h}_{t-1}</script>与<script type="math/tex">t</script>时刻的输入<script type="math/tex">\mathbf{x}_{t}</script>进行级联，逐一通过三个门，</p>
<ul>
<li>遗忘门 (Forget gate)<script type="math/tex; mode=display">
\begin{equation}
  \mathbf{f_t} = \sigma\left(W_f \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_f\right) 
\end{equation}</script></li>
<li>输入门 (Input gate)<script type="math/tex; mode=display">
\begin{equation}
\mathbf{i_t} = \sigma\left(W_i \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_i \right).
\end{equation}</script></li>
<li>输出门 (Output gate)<script type="math/tex; mode=display">
\begin{equation}
\mathbf{o_t} = \sigma\left(W_o \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_o \right).
\end{equation}</script></li>
</ul>
<p>其中遗忘门的作用是<strong>抛弃cell state中不需要的信息</strong>，与<script type="math/tex">\mathbf{C_{t-1}}</script>作用； 输入门则是<strong>决定cell state中待更新的信息</strong>，与<script type="math/tex">\mathbf{\tilde{C}_t}</script>，即state candidates作用；输出门则从更新后的<code>cell state</code>中<strong>决定输出的状态</strong>。结合以上三个门结构，便可以更新<code>cell state</code>以及<code>cell output</code>,</p>
<ul>
<li>cell state update<script type="math/tex; mode=display">
\begin{align}
\mathbf{\tilde{C_{t}}} &= \rm{tanh}\left(W_c \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_c \right) \\ 
\mathbf{C_t} &= \mathbf{f_t}\cdot\mathbf{C_{t-1}} + \mathbf{i_t} \cdot \mathbf{\tilde{C_{t}}}
\end{align}</script></li>
<li>cell output upadte<script type="math/tex; mode=display">
\begin{equation}
\mathbf{h_{t}} = \rm{tanh}(\mathbf{C_{t}}) \cdot \mathbf{o_t}
\end{equation}</script></li>
</ul>
<h4 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h4><p>参考《TensorFlow实战》第7章的LSTM基于<a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz" target="_blank" rel="external">PTB数据集</a>的语言预测样例，以及TensorFlow的<a href="https://www.tensorflow.org/tutorials/recurrent" target="_blank" rel="external">Tutorial</a>，设计了一个小实验，对比RNN和LSTM的performance，以及激活函数对于RNN的影响。(详细的notebooks见这里: <a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook-rnn-tanh.ipynb" target="_blank" rel="external">RNN</a>, <a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook-lstm.ipynb" target="_blank" rel="external">LSTM</a>)</p>
<p>这里给出<code>tf.contrib.rnn</code>中提供的用于搭建RNN和LSTM cell的类的实例化方法，以及如何构建多个Recurrent层，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># RNN</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell</span><span class="params">(num_units, activation, reuse=None)</span>:</span></div><div class="line">   <span class="keyword">return</span> tf.contrib.rnn.BasicLSTMCell(</div><div class="line">       num_units=num_units,  </div><div class="line">       activation=activation,</div><div class="line">       reuse=reuse)</div><div class="line"></div><div class="line"><span class="comment"># LSTM</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell</span><span class="params">(num_units, forget_bias=<span class="number">0.0</span>, state_in_tuple=True, reuse=None)</span>:</span></div><div class="line">	<span class="keyword">return</span> tf.contrib.rnn.BasicLSTMCell(</div><div class="line">    	num_units=size, </div><div class="line">        forget_bias=forget_bias, </div><div class="line">        state_is_tuple=state_in_tuple,</div><div class="line">        reuse=reuse)</div><div class="line"></div><div class="line"><span class="comment"># Multiple layers</span></div><div class="line">attn_cell = rnn_cell</div><div class="line">numlayers = <span class="number">2</span></div><div class="line">cell = tf.contrib.rnn.MultiRNNCell(</div><div class="line">	[attn_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(numlayers)],</div><div class="line">    state_is_tuple=<span class="keyword">True</span>)</div></pre></td></tr></table></figure></p>
<p>另外，在PTB的TF教程里，设置了可变的学习率以及梯度的clipping用于抑制梯度爆炸 (gradient explosion) 的问题，代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Adjustable learning rate</span></div><div class="line">new_lr = tf.placeholder(tf.float32, shape=[], name=<span class="string">"new_learning_rate"</span>)</div><div class="line">lr_update = tf.assign(self._lr, self._new_lr) <span class="comment"># use tf.assign to transfer the updated lr</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">assign_lr</span><span class="params">(session, lr_value)</span>:</span></div><div class="line">	sess.run(self._lr_update, feed_dict=&#123;new_lr: lr_value&#125;)</div><div class="line"></div><div class="line"><span class="comment"># Gradient clipping</span></div><div class="line">...</div><div class="line">max_grad_norm = <span class="number">5.0</span> <span class="comment"># maximum gradient</span></div><div class="line">tvars = tf.trainable_variables()  <span class="comment"># Get all trainable variables</span></div><div class="line">grads, _ = tf.clip_by_global_norm(</div><div class="line">	tf.gradients(cost, tvars), max_grad_norm)</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(self._lr)</div><div class="line">train_op = optimizer.apply_gradients(</div><div class="line">    zip(grads, tvars),</div><div class="line">    global_step = tf.contrib.framework.get_or_create_global_step())</div></pre></td></tr></table></figure></p>
<p>下面来比较一下RNN和LSTM的效果。比较的指标是perplexity (复杂度)，用于刻画该模型能够估计出某一句话的概率，其数值越小，模型表现越好。</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/images/fig-rnn-lstm-cmp.png?raw=true" height="300" width="500">
</center>

<p>容易看出，LSTM的表现是优于RNN的。除此之外，采用tanh函数的RNN要显著好于采用ReLU，在训练中也出现了<em>RuntimeWarning: overflow encountered in exp</em>的警告，说明出现了gradient explosion的问题。最后，我尝试增加了RNN的层数，但是效果并没有变好，也许是参数多了？也有可能是我偷懒了，没多训测试几次。。。</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM</a><br>[2] <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a><br>[3] <a href="https://www.tensorflow.org/tutorials/recurrent" target="_blank" rel="external">Tensorflow tutorial</a><br>[4] TensorFlow实战Google深度学习框架<br>[5] TensorFlow实战<br>[6] <a href="https://www.zhihu.com/question/61265076/answer/186644426" target="_blank" rel="external">RNN中为什么要采用tanh而不是ReLu作为激活函数？</a></p>
<h3 id="广告位"><a href="#广告位" class="headerlink" title="广告位"></a>广告位</h3><p><a href="http://www.mazhixian.me/2018/01/21/resnet-with-tensorflow/">Residual network I — block and bottleneck</a><br><a href="http://www.mazhixian.me/2018/01/27/resnet-with-tensorflow-2/">Residual network II — realize with tensorflow</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/15/recurrent-neural-network-and-lstm/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/13/expectation-maximization-algorithm/">
                            Expectation maximization algorithm
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-13T15:16:35+08:00">
	
		    Mar 13, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>回顾一下GMM以及HMM等模型的求解方法，即著名的Expectation Maximization (EM) 算法。参考李航老师的《统计学习方法》，EM算法是一种迭代算法，用于含有隐含变量的概率模型参数的极大似然估计（MLE），或极大后验概率估计 (MPE).</p>
<h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>定义<script type="math/tex">Y</script>为观测随机变量，<script type="math/tex">Z</script>为隐含随机变量，则<script type="math/tex">Y</script>和<script type="math/tex">Z</script>一起构成完全数据 (complete-data)。假设待估计的概率模型参数为<script type="math/tex">\theta</script>，则观测数据的概率分布为<script type="math/tex">P(Y|\theta)</script>, 即为其似然函数，对应的对数似然函数为<script type="math/tex">L(\theta) = \log{P(Y|\theta)}</script>. 设<script type="math/tex">Y</script>和<script type="math/tex">Z</script>Z的联合概率分布为<script type="math/tex">P(Y,Z|\theta)</script>，那么完全数据的对数似然函数为<script type="math/tex">\log{P(Y,Z|\theta)}</script>.</p>
<p>EM算法的目的就是求解参数<script type="math/tex">\theta</script>，极大化观测量的对数似然函数<script type="math/tex">L(\theta)</script>。因为包含隐含量，所以采用迭代的方法，分为E (期望) 和M (极大化)两步，求解算法如下。</p>
<ol>
<li><p>初始化参数<script type="math/tex">\theta^{(0)}</script>;</p>
</li>
<li><p>E step: 记<script type="math/tex">\theta^{(i)}</script>为第<script type="math/tex">i</script>次迭代参数<script type="math/tex">\theta</script>的估计值，则第<script type="math/tex">i+1</script>次迭代，计算如下期望.</p>
<script type="math/tex; mode=display">
\begin{align}
Q(\theta,\theta^{(i)}) &= E_{z}[\log{P(Y,Z|\theta)}|Y,\theta^{(i)}] \notag \\
&=\sum_{z}{\log{P(Y,Z|\theta)P(Z|Y,\theta^{(i)})}},
\end{align}</script><p>其中<script type="math/tex">P(Z|Y,\theta^{(i)})</script>是在给定观测数据<script type="math/tex">Y</script>和当前估计的参数$\theta^{(i)}$下隐含变量<script type="math/tex">Z</script>的条件概率分布；<script type="math/tex">Q(\theta,\theta^{(i)})</script>定义为完全数据的对数似然函数在给定观测数据和当前参数下对隐含数据<script type="math/tex">Z</script>的条件概率分布<script type="math/tex">P(Z|Y,\theta^{(i)})</script>的期望，即<script type="math/tex">\log{P(Y|\theta)}</script>。</p>
</li>
<li><p>M step: 求使<script type="math/tex">Q(\theta, \theta^{(i)})</script>极大化的参数<script type="math/tex">\theta</script>，以确定第<script type="math/tex">i+1</script>次迭代的参数的估计值<script type="math/tex">\theta^{(i+1)}</script>, 即计算</p>
<script type="math/tex; mode=display">
\begin{align}
\theta^{(i+1)} = \mathrm{arg}\max\limits_{\theta}{Q(\theta,\theta^{(i)})}
\end{align}</script></li>
<li><p>重复E和M两步，直到收敛. </p>
</li>
</ol>
<p>李航老师也强调了一点：<strong>EM算法与初值的选择有关，选择不同的初值可能得到不同的参数估计值。</strong> 我在这个<a href="https://github.com/myinxd/canal-notebooks/blob/master/machinelearning/notebook-expectation-maximum-three-coiins.ipynb" target="_blank" rel="external">notebook</a>里给了一个样例，即“三硬币模型”。</p>
<h3 id="高斯混合模型的求解"><a href="#高斯混合模型的求解" class="headerlink" title="高斯混合模型的求解"></a>高斯混合模型的求解</h3><p>EM算法最经典的应用就是高斯混合模型的参数估计，该模型是语音信号处理的基础模型之一，其定义如下，</p>
<script type="math/tex; mode=display">
\begin{align}
P(\mathbf{y}|\mathbf{\theta}) = \sum_{k=1}^{K}{c_k\phi(\mathbf{y}|\mathbf{\theta_k)}}
\end{align}</script><p>其中，<script type="math/tex">c_k</script>是系数，<script type="math/tex">c_k\geq0</script>, <script type="math/tex">\sum_{k=1}^{K}{c_k=1}</script>; <script type="math/tex">\phi(\mathbf{y}|\mathbf{\theta_k})</script>是高斯分布，<script type="math/tex">\mathbf{\theta_k} = (\mathbf{\mu_k},\Sigma_k^2)</script>,</p>
<script type="math/tex; mode=display">
\begin{equation}
\phi(\mathbf{y}|\mathbf{\theta_k}) = \frac{1}{(2\pi)^{M/2}|\Sigma_k|}{-\exp{(\mathbf{y}-\mathbf{\mu_k})^{T}{\Sigma_{k}}^{-1}(\mathbf{y}-\mathbf{\mu_k})}}.
\end{equation}</script><p>在GMM模型中有，</p>
<ul>
<li>观测变量：<script type="math/tex">\mathbf{Y}</script>，</li>
<li>隐含变量：<script type="math/tex">\gamma_k\in{0,1}, k=1,~2,~,\cdots,~K</script>，表示<script type="math/tex">\mathbf{y}</script>是否来自第<script type="math/tex">k</script>个高斯分量</li>
<li>模型参数：<script type="math/tex">\mathbf{\theta} = {c_k, \mathbf{\theta_k}}, k=1,~2,~,\cdots,~K</script>.</li>
</ul>
<p>详细的推导见《统计学习方法》，这里给出GMM模型的E和M步骤.<br>设观测数据<script type="math/tex">\mathbf{Y} = \{\mathbf{y1},~\mathbf{y2},~\cdots,~\mathbf{y_N}\}</script></p>
<ul>
<li><p>E step</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{\gamma_{jk}} = \frac{c_k\phi(\mathbf{y_j}|\mathbf{\theta_k})}{\sum_{k=1}^{K}{c_k\phi(\mathbf{y_j}|\mathbf{\theta_k})}} 
\end{align}</script><p>其中<script type="math/tex">j=1,~2,~\cdots,~,N, k=1,~2,~\cdots,~K</script>.</p>
</li>
<li><p>M step</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{\mu}_{km} &= \sum_{j=1}^{N}{\hat{\gamma}_{jk}y_{jm}}/{\sum^{N}_{j=1}{\hat{\gamma}_{jk}}} 
\notag \\
\hat{\sigma}^{2}_{km} &= \sum_{j=1}^{N}{\hat{\gamma}_{jk}(y_{jm}-\mu_{km})^2}/{\sum_{j=1}^{N}{\hat{\gamma}_{jk}}} \notag \\
\hat{c}_{k} &= {\sum_{j=1}^{N}{\hat{\gamma}_{jk}}} / N
\end{align}</script><p>其中<script type="math/tex">m=1,~2,~\cdots,~,M, k=1,~2,~\cdots,~K</script>.</p>
</li>
</ul>
<p>最后，给出一个样例，简单的二维GMM，如下图，notebook见<a href="https://github.com/myinxd/canal-notebooks/blob/master/machinelearning/notebook-EM-GMM-2D.ipynb" target="_blank" rel="external">这里</a>.</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/machinelearning/fig_gmm_2d.png?raw=true" height="600" width="600">
</center>

<p>可以看出，EM的估计效果还是不错的，并且提供初始化参数值的结果 (左下) 比随机初始化 (右下) 的结果要好。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] 李航，统计学系方法，2012 清华大学出版社<br>[2] <a href="https://stats.stackexchange.com/questions/70855/generating-random-variables-from-a-mixture-of-normal-distributions" target="_blank" rel="external">Generating random variables from a mixture of Normal distributions</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/13/expectation-maximization-algorithm/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/11/understanding-of-auc-curve/">
                            二分类及AUC的理解
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-11T13:36:59+08:00">
	
		    Mar 11, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>在机器学习中，二分类 (binary classification) 问题是最常出现且最经典的问题。本文首先解释二分类的样本标签问题，包括正例 (Positive)/反例 (Negative) 和真例 (True)/假例 (False) 这两组集合；紧接着介绍几种常用的分类器评估指标，例如精确度 (precision)、准确率 (accuracy)、敏感性 (sensitivity)、特异性 (specificity)等；最后，讨论对ROC曲线及其衍生的AUC指标的理解，并给出样例。</p>
<h4 id="二分类标签"><a href="#二分类标签" class="headerlink" title="二分类标签"></a>二分类标签</h4><p>应用于二分类的样本通常用正例 (Positive, P) 和反例 (Negative, N) 进行标注，作为实际标签。而经过分类器估计后的输出，根据其结果的正确与否，划分为真例 (True, T) 和假 (False, F)两个集合。因此，需要注意的是<code>T</code>和<code>F</code>两个集合均包含正例和反例样本。</p>
<p>根据实际标签和预测结果进行两两组合，得到四个子集，分别为真正率 (True positive, TP)、真反例 (True negative, TN)、假正率 (False positive, FP) 和假反例 (False negative, FN)，如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">预测正例</th>
<th style="text-align:center">预测反例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">实际正例</td>
<td style="text-align:center">TP</td>
<td style="text-align:center">FN</td>
</tr>
<tr>
<td style="text-align:center">实际反例</td>
<td style="text-align:center">FP</td>
<td style="text-align:center">TN</td>
</tr>
</tbody>
</table>
</div>
<p>通过对这四个子集样本的组合，可以得到一些评估指标，用于评价分类器的表现。这里我想强调一下对FN和FP的理解，其中FN指的是<em>实际为反例，但被分类器判断为正例</em>，而FP指的是<em>实际为正例，但被分类器判断为反例</em>，二者的合集为<code>F</code>。</p>
<h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><p>下面讨论常用的二分类器的评估指标 (index or measure)。定义<script type="math/tex">S</script>为所有的样本数量，<script type="math/tex">S_{P}</script>和<script type="math/tex">S_{N}</script>对应实际标注为正例和反例的样本数；<script type="math/tex">S_{T}</script>和<script type="math/tex">S_{F}</script>表示分类器估计的标签中正确和错误的样本数。相应的，定义<script type="math/tex">S_{\mathrm{TP}}</script>、<script type="math/tex">S_{\mathrm{TN}}</script>、<script type="math/tex">S_{\mathrm{FP}}</script>和<script type="math/tex">S_{\mathrm{FN}}</script>为TP、TN、FP、FN样本的数目。因此，定义如下的评估指标</p>
<ul>
<li><p>准确率 (accuracy)<br>准确率为分类器预测结果中判断正确的样本占所有样本的比例，即<script type="math/tex">\mathrm{acc} = S_{T}/S = (S_{\mathrm{TP}} + S_\mathrm{TN}) / S</script>.</p>
</li>
<li><p>精确度 (precision)<br>精确度又称为查准率，衡量分类器预测为正的样本中实际为正例的样本比例，即 <script type="math/tex">\mathrm{pre} = S_{\mathrm{TP}} / (S_{\mathrm{TP}} + S_{\mathrm{FP}})</script>.</p>
</li>
<li><p>敏感性 (sensitivity)<br>敏感性又称为召回率或真正率，衡量实际为正例的样本经过分类器预测后标记正确的样本所占比例，即<script type="math/tex">\mathrm{sen} = S_{\mathrm{TP}} / (S_{\mathrm{TP}} + S_{\mathrm{FN}})</script>.</p>
</li>
<li><p>F1-score<br>F1-score是对精确度和敏感性的结合，因为这两者本质上是矛盾的，通常敏感性越高则精确度会较低。F1-score的表达式为, <script type="math/tex">\mathrm{F1} = (2 \times \mathrm{pre} \times \mathrm{sen}) / (\mathrm{pre} + \mathrm{sen})</script>.</p>
</li>
<li><p>特异性 (specificity)<br>特异性是实际标注为反例的样本经过分类器预测正确的样本的比例，即<script type="math/tex">\mathrm{spe} = S_\mathrm{TN} / (S_\mathrm{FN} + S_\mathrm{TN})</script>.</p>
</li>
<li><p>假正率 (fasle positive rate)<br>假正率表示分类器预测为反例的样本中实际为反例的样本的比例，即<script type="math/tex">\mathrm{fpr} = S_\mathrm{TN} / (S_\mathrm{FP} + S_\mathrm{TN})</script>.</p>
</li>
</ul>
<p>对于这些评估指标，我的理解见下表，</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">评估指标</th>
<th style="text-align:center">意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">准确率</td>
<td style="text-align:center">衡量了分类器总体上的准确性，不考虑样本的实际类别.</td>
</tr>
<tr>
<td style="text-align:center">精确度</td>
<td style="text-align:center">衡量了正例的分类准确性，通常比准确率要高.</td>
</tr>
<tr>
<td style="text-align:center">敏感性</td>
<td style="text-align:center">衡量了分类器对正例的泛化能力，在异常检测中应用较多.</td>
</tr>
<tr>
<td style="text-align:center">特异性</td>
<td style="text-align:center">衡量了分类器对反例的分类准确性.</td>
</tr>
<tr>
<td style="text-align:center">假正率</td>
<td style="text-align:center">与敏感性类似，衡量分类器对反例的泛化能力.</td>
</tr>
</tbody>
</table>
</div>
<h4 id="ROC和AUC"><a href="#ROC和AUC" class="headerlink" title="ROC和AUC"></a>ROC和AUC</h4><p>以上的评估指标均要求分类器的输出为确定的标签，而分类器通常输出的是样本被判断为正例的概率，为了得到标签，需要设定概率的门限，即大于该门限的概率对应的样本判断为正例，否则为反例。门限的设定，影响分类器的泛化能力。</p>
<p>因此，人们提出了receiver operating characteristic (ROC) 的概念，最早出现二战时检测敌机的雷达分析技术。在信号处理中，有这样一组指标，即捕获率 (catch rate) 和追踪率 (follow rate)，前者衡量了系统对于目标信号的捕获能力，后者衡量系统在捕获信号后继续追踪的能力。这两个指标，对应到二分类问题中就是敏感性或真正率 (truu positive rate, TPR)和假正率。</p>
<p>通过TPR和FRP即可求解ROC曲线。对分类器输出的样本概率进行排序，设定概率门限<code>thrs</code>，将高于该门限的样本判断为正例，反之判断为反例。而后，与实际的样本标签进行比较，计算对应该门限的TPR和FPR。记录不同门限处的TPR和FRP，则得到了该分类器的ROC曲线。</p>
<p>如下为ROC曲线的求解算法，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Step1. input labels and probs estimated by classifier;</div><div class="line">Step2. set thresholding step</div><div class="line">Step3. for thrs = 1.0 : step : 0.0</div><div class="line">		labels_est = zeros(size(labels))</div><div class="line">        labels_Est[probs &gt; thrs] = 1</div><div class="line">        calculate fpr and tpr w.r.t. thrs</div><div class="line">Step4. Draw the ROC curve</div></pre></td></tr></table></figure></p>
<p>针对不同的分类器，若某个分类器的ROC曲线整体在其他分类器的上方，则可认为该分类器最优。但往往存在ROC曲线交叉的情形，此时通过计算ROC曲线下方的面积，即AUC (area under curve)值来进行对比，AUC数值越大，分类器的分类效果越好，泛化能力越强。</p>
<p>给出一个ROC曲线和AUC的求解样例，具体的代码实现见<a href="https://github.com/myinxd/canal-notebooks/blob/master/machinelearning/notebook-roc-auc.ipynb" target="_blank" rel="external">这里</a>.</p>
<p>首先，设样本的标签和二分类器输出的概率为，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">labels = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</div><div class="line">probs = [<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.1</span>, <span class="number">0.7</span>, <span class="number">0.3</span>, <span class="number">0.9</span>, <span class="number">0.5</span>]</div></pre></td></tr></table></figure></p>
<p>设定thrs的步长为0.1，则求解的TPR和FPR分别为，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tpr = [<span class="number">0.0</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]</div><div class="line">fpr = [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.33</span>, <span class="number">0.33</span>, <span class="number">0.67</span>, <span class="number">0.83</span>, <span class="number">1.0</span>]</div></pre></td></tr></table></figure></p>
<p>最后，求解得到auc=0.967. 下图为该样例的ROC曲线，</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/machinelearning/fig_roc.png?raw=true" height="250" width="360">
</center>

<p><a href="https://www.scikit-learn.org" target="_blank" rel="external">scikit-learn</a>也提供了求解auc的函数，其用法如下，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div><div class="line">fpr, tpr, thresholds = metrics.roc_curve(labels+<span class="number">1</span>, p, pos_label=<span class="number">2</span>)</div><div class="line">auc = metrics.auc(fpr, tpr)</div></pre></td></tr></table></figure></p>
<p>在我的notebook中，分别给出了我的实现和sklearn.metrics.auc的实现，二者结果相同。最后吐个槽，因为haroopad的bug，这篇是我重新写的！！！</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] 周志华，机器学习，2017 清华大学出版社<br>[2] <a href="https://www.zhihu.com/question/39840928" target="_blank" rel="external">机器学习和统计里面的auc怎么理解？</a><br>[3] <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html" target="_blank" rel="external">sklearn.metrics.auc</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/11/understanding-of-auc-curve/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/06/svm-hyperplane-visualization-based-on-libsvm/">
                            SVM hyperplane visualization based on libsvm
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-06T09:24:10+08:00">
	
		    Mar 06, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>Support vector machine (SVM), as a shallow model, has been widely applied for classification tasks. To solve the model, groups of super vectors (SVs) of corresponding classes are extracted, so as to calculate a hyperplane as the classification boarder.</p>
<h3 id="A-brief-review"><a href="#A-brief-review" class="headerlink" title="A brief review"></a>A brief review</h3><p>Denote <script type="math/tex">\mathbf{x} = \{\mathbf{x_1},~\mathbf{x_2},~\dots,~\mathbf{x_N}\}</script> as the samples to be classified, and $y = \{y_1,~y_2,~\dots,~y_N\}$ are the corresponding labels. Take binary classification as an example, </p>
<script type="math/tex; mode=display">
\begin{equation}
    y_i(\mathbf{w}\cdot\mathbf{x_i}+b) \geq 1, i = 1,~2,~\dots,~N,
\end{equation}</script><p>where <script type="math/tex">\mathbf{w}</script> are the coefficients w.r.t. features in <script type="math/tex">\mathbf{x}</script>, b is the bias.</p>
<p>The the problem becomes an optimization task, where the object is,</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{cases}
    \min\limits_{w}\frac{\left \| \mathbf{w} \right \|}{2}, \\
    \mathrm{s.t.}~y_i(\mathbf{w}\cdot\mathbf{x_i}+b) \geq 1, i = 1,~2,~\dots,~N,
\end{cases}
\end{equation}</script><p>which shall be calculated with Lagrange equation,</p>
<script type="math/tex; mode=display">
\begin{equation}
L_P = \frac{1}{2}{\left\| \mathbf{w} \right \|} - \sum^{N}_{i=1}{\lambda_i\{y_i(\mathbf{w}\cdot\mathbf{x_i}+b)-1\}}.
\end{equation}</script><p>To save time, it usually selects a subset of <script type="math/tex">\mathbf{x}</script> namely super vectors to optimize above equation, instead of all of the samples. Those SVs are samples that stand close to the classification hyperplane, i.e., the boarders of different types. They are considered on behalf of the classes they belonging to.</p>
<p>By solving the Lagrange equation, we obtain the <script type="math/tex">\mathbf{\lambda}</script>, as well as <script type="math/tex">\mathbf{w}</script> and $b$.</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{cases}
    \mathbf{w} = \sum^{N_\rm{SV}}_{i=1}{\lambda_i y_i \mathbf{x_i}}, \\
    b = - \frac{1}{2}\mathbf{w}\cdot(\mathbf{x_{c1}}+\mathbf{x_{c2}}),
\end{cases}
\end{equation}</script><p>where <script type="math/tex">\mathbf{x_c1}</script> and <script type="math/tex">\mathbf{x_c2}</script> are arbitrary super vectors of class one and two, respectively.</p>
<p>The dicision function based on those parameters are,</p>
<script type="math/tex; mode=display">
\begin{equation}
    f(\mathbf{x_s}) = \rm{sgn}\left[ \sum^{N_\rm{SV}}_{i=1}{\lambda_i y_i (\mathbf{x_i}\cdot\mathbf{x_s}) + b} \right],
\end{equation}</script><p>here <script type="math/tex">\rm{sgn}</script> is the sign function.</p>
<p>For non-linear classification, which is more general than linear case, the dot product between <script type="math/tex">\mathbf{x_i}</script> and <script type="math/tex">\mathbf{x_s}</script> are replaced by non-linear kernel functions $\Phi(\cdot)$, i.e.,</p>
<script type="math/tex; mode=display">
\begin{equation}
    f(\mathbf{x_s}) = \rm{sgn}\left[ \sum^{N_\rm{SV}}_{i=1}{\lambda_i y_i \Phi(\mathbf{x_i},\mathbf{x_s})+b} \right].
\end{equation}</script><h3 id="Realization-and-visualization"><a href="#Realization-and-visualization" class="headerlink" title="Realization and visualization"></a>Realization and visualization</h3><p>With the help of <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/" target="_blank" rel="external">libsvm</a>, it is easy to realize SVM based classification. What I want to say in this blog is how to visualize or replicate the prediction stage of the <code>svmtrain</code> function. Some comments are as follows,</p>
<ul>
<li>After training the SVM with <code>svmtrain</code>, a <code>model</code> will be generated;</li>
<li>In the <code>model</code>, super vectors, parameters like weights and bias, are archived;</li>
<li><strong>To save space, the support vectors are saved as sparse matrix.</strong></li>
<li>For multi-class classification, it can be transformed to multiple binary-classification tasks.</li>
</ul>
<p>Here is a naive two-dimensional three-type classification example (<a href="https://github.com/myinxd/svm-toy/blob/master/demos/DrawSepLine3C.m" target="_blank" rel="external">code</a> is available). I divided three-class task into three binary classifications. The linear kernel function was used, thus the classification hyperplanes were also linear.</p>
<p><center>
<img src="https://github.com/myinxd/svm-toy/raw/master/images/fig_3c.png?raw=true" height="280" width="600">
</center><br>In the right figure, only support vector points are plotted. It can be found that the SVs are those points stand at the boarder between different categories.</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/06/svm-hyperplane-visualization-based-on-libsvm/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/04/a-numpy-matrix-copy-problem/">
                            Nupy矩阵的复制问题
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-04T10:26:23+08:00">
	
		    Mar 04, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>简单记录一下最近遇到的一个bug，利用numpy生成的矩阵在复制时不能直接赋值，而是要用<code>copy</code>方法。直接赋值类似于把内存中的地址 (即指针) 给了目标变量，其与被赋值变量共享同一块内存，这样做可以节省内存空间。而<code>copy</code>则不同，会重新申请一块内存，分配给复制后的新变量，在该变量上的操作不会对愿变量产生影响。</p>
<p>下面看一个例子,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">x = np.arange(<span class="number">9.</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</div><div class="line"><span class="comment"># copy</span></div><div class="line">y = x.copy()</div><div class="line">y[y&gt;=<span class="number">5</span>] = <span class="number">0</span></div><div class="line"></div><div class="line"><span class="comment"># 赋值法</span></div><div class="line">z = x</div><div class="line">z[z&gt;=<span class="number">5</span>] = <span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>其输出结果如下,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">6</span>]: y</div><div class="line">Out[<span class="number">6</span>]: </div><div class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</div><div class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>],</div><div class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</div><div class="line"></div><div class="line">In [<span class="number">7</span>]: x</div><div class="line">Out[<span class="number">7</span>]: </div><div class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</div><div class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</div><div class="line">       [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</div><div class="line"></div><div class="line">In [<span class="number">8</span>]: z</div><div class="line">Out[<span class="number">8</span>]: </div><div class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</div><div class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>],</div><div class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</div><div class="line"></div><div class="line">In [<span class="number">9</span>]: x</div><div class="line">Out[<span class="number">9</span>]: </div><div class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</div><div class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>],</div><div class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</div></pre></td></tr></table></figure></p>
<p>可以看到，采用<code>copy</code>后，对<code>y</code>的操作不会影响到原矩阵<code>x</code>，而采用直接赋值后，对<code>z</code>的操作对<code>x</code>产生了影响。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/04/a-numpy-matrix-copy-problem/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/02/28/upgrade-ubuntu-from-1610-to-1710/">
                            Upgrade Ubuntu from 1610 to 1710
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-02-28T10:59:38+08:00">
	
		    Feb 28, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>考虑到系统的稳定性，也因为懒，一直没升级Ubuntu 1610，也没有update。最近发现官方已经不支持1610了，而且直接把<code>yakkety</code>的镜像给取消了。所以，即使是利用系统自带的update manager，也无法直接升级到1710，会出现如下的两类错误，</p>
<h5 id="无法更新"><a href="#无法更新" class="headerlink" title="无法更新"></a>无法更新</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">E:The repository <span class="string">'http://archive.canonical.com/ubuntu yakkety Release'</span> does not have a Release file., </div><div class="line">W:Updating from such a repository can<span class="string">'t be done securely, and is therefore disabled by default., </span></div><div class="line"><span class="string">W:See apt-secure(8) manpage for repository creation and user configuration details., </span></div><div class="line"><span class="string">E:The repository '</span>http://archive.ubuntu.com/ubuntu yakkety Release<span class="string">' does not have a Release file., </span></div><div class="line"><span class="string">W:Updating from such a repository can'</span>t be <span class="keyword">done</span> securely, and is therefore disabled by default., </div><div class="line">W:See apt-secure(8) manpage <span class="keyword">for</span> repository creation and user configuration details., </div><div class="line">E:The repository <span class="string">'http://archive.ubuntu.com/ubuntu yakkety-updates Release'</span> does not have a Release file., </div><div class="line">...</div></pre></td></tr></table></figure>
<h5 id="无法升级"><a href="#无法升级" class="headerlink" title="无法升级"></a>无法升级</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">An upgrade from <span class="string">'yakkety'</span> to <span class="string">'artful'</span> is not supported with this tool.</div></pre></td></tr></table></figure>
<p>参考Askubuntu上的<a href="https://askubuntu.com/questions/997047/how-to-update-ubuntu-from-16-10-to-17-10" target="_blank" rel="external">问题</a>，有如下的解决方法，<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> /etc/apt/</div><div class="line"><span class="comment"># 备份现有的sources.list</span></div><div class="line">sudo cp sources.list sources.list.bkp</div><div class="line"><span class="comment"># 替换yakkety为artful</span></div><div class="line">sudo vim sources.list</div><div class="line">:%s/yakkety/artful/g</div><div class="line"><span class="comment"># 保存并重新更新和升级</span></div><div class="line">sudo update</div><div class="line">sudo upgrade</div><div class="line"><span class="comment"># 重启系统</span></div><div class="line">reboot</div></pre></td></tr></table></figure></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] <a href="https://askubuntu.com/questions/997047/how-to-update-ubuntu-from-16-10-to-17-10" target="_blank" rel="external">How to upgrade ubuntu from 1610 to 1710</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/02/28/upgrade-ubuntu-from-1610-to-1710/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/02/27/difference-of-indexing-between-python-and-matlab/">
                            Python和MATLAB矩阵索引的一个区别
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-02-27T10:04:15+08:00">
	
		    Feb 27, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>这两天尝试将一个MATLAB工程转成Python，卡在一个bug上很久，最后发现是二者在矩阵索引上存在区别。这里我采用的是Python的<a href="https://www.numpy.org" target="_blank" rel="external">NumPy</a>库。</p>
<p>我们经常会通过一定的逻辑关系索引矩阵中的元素，并获取这些元素的位置信息，即indices。例如，<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% MATLAB实现</span></div><div class="line">x = resize(<span class="number">0</span>:<span class="number">8</span>, <span class="number">3</span>, <span class="number">3</span>);</div><div class="line"><span class="comment">% 一维索引</span></div><div class="line">idx = <span class="built_in">find</span>(x &gt;= <span class="number">3</span>);</div><div class="line"><span class="comment">% 二维索引</span></div><div class="line">[idr, idc] = <span class="built_in">find</span>(x &gt;= <span class="number">3</span>);</div></pre></td></tr></table></figure></p>
<p>对应的python实现为，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">x = np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</div><div class="line"><span class="comment"># 只有二维索引</span></div><div class="line">[idr, idc] = np.where(x &gt;= <span class="number">3</span>)</div></pre></td></tr></table></figure></p>
<p>这里可以看出，MATLAB提供一种一维索引，即将二维矩阵以列为先，行次之转为一维向量，输出的索引对应该一维向量中元素所在位置。（这种处理方法经常可以用于加速运算，缩短运行时间。）然而，NumPy的where方法根据矩阵的维数提供对应axis的索引，没有MATLAB这种一维索引的输出。</p>
<p>因此，在python中使用np.where进行矩阵元素索引时，要注意如下两点，</p>
<ol>
<li>np.where的输出是一个列表;</li>
<li>np.where的输出列表的元素个数与矩阵的维数对应。</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html" target="_blank" rel="external">numpy.where</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/02/27/difference-of-indexing-between-python-and-matlab/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/02/26/Curve-fitting-by-scipy/">
                            Curve fitting by SciPy
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-02-26T10:09:33+08:00">
	
		    Feb 26, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>简单记录一下利用python的<a href="https://www.scipy.org" target="_blank" rel="external">SciPy</a>库进行曲线拟合的方法，主要分为三个步骤，(1) 获取待拟合数据; (2) 定义函数描述待拟合曲线; （3）利用<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html" target="_blank" rel="external">Scipy.optimize.curve_fit</a>模块进行拟合。</p>
<p>获取数据的步骤不再赘述，这里从步骤二开始。以泊松分布为例，首先定义函数<code>poisson_func</code>，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">poisson_func</span><span class="params">(k, l, c)</span>:</span></div><div class="line">	<span class="string">"""Poisson function</span></div><div class="line"><span class="string">    inputs</span></div><div class="line"><span class="string">    ======</span></div><div class="line"><span class="string">    k: np.ndarray</span></div><div class="line"><span class="string">       number of accidents, i.e., the x axis;</span></div><div class="line"><span class="string">    l: double</span></div><div class="line"><span class="string">       the lambda parameter of Poisson distribution</span></div><div class="line"><span class="string">    c: double</span></div><div class="line"><span class="string">       a constant for release the optimization</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    output</span></div><div class="line"><span class="string">    ======</span></div><div class="line"><span class="string">    the fitted result according to l and c w.r.t. k</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    note</span></div><div class="line"><span class="string">    ====</span></div><div class="line"><span class="string">    l and c are the parametres to be estimated.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    k_mat = np.ones(k.shape)</div><div class="line">    <span class="keyword">for</span> x, i <span class="keyword">in</span> enumerate(k):</div><div class="line">        k_mat[i] = math.factorial(x) </div><div class="line">    <span class="keyword">return</span> l**k / k_mat*np.exp(-l) + c</div></pre></td></tr></table></figure></p>
<p>紧接着，根据待拟合的数据，对参数进行估计，如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> curve_fit</div><div class="line"></div><div class="line">popt, pcov = curve_fit(poisson_func, x, y)</div><div class="line">perr = np.sqrt(np.diag(pcov))</div></pre></td></tr></table></figure></p>
<p>其中<code>popt</code>为估计的参数，<code>pcov</code>为对应的相关矩阵，其对角线为方差，可用于计算拟合参数的误差perr。</p>
<p>下图为我的测试样例，图中橙色三角为待拟合样本点，蓝色实线为拟合结果。</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180226/fig1.png?raw=true" height="300" width="430">
</center>

<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html" target="_blank" rel="external">Scipy.optimize.curve_fit</a><br>[2] <a href="https://en.wikipedia.org/wiki/Poisson_distribution" target="_blank" rel="external">Poisson distribution</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/02/26/Curve-fitting-by-scipy/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
        
          <li class="pagination-next">
            <a class="btn btn--default btn--small" href="/page/2/">
              <span>OLDER POSTS</span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
        <li class="pagination-number">page 1 of 6</li>
    </ul>
</div>

</section>


                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Jason Ma. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Jason Ma</h4>
        
            <div id="about-card-bio"><p>We are in the same story.</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Astronomer? Software engineer</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Shanghai
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-peofhqjkzcghmndknakluequy1y6owxdwpaqyju9ntl9zxnk7rdolb3rjjoj.min.js"></script>
<!--SCRIPTS END--><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



    </body>
</html>
