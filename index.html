<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Canal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Valar morghulis, valar dohaeris.">
<meta property="og:type" content="website">
<meta property="og:title" content="Canal">
<meta property="og:url" content="http://www.mazhixian.me/index.html">
<meta property="og:site_name" content="Canal">
<meta property="og:description" content="Valar morghulis, valar dohaeris.">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Canal">
<meta name="twitter:description" content="Valar morghulis, valar dohaeris.">
  
    <link rel="alternate" href="/atom.xml" title="Canal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Canal</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Valar morghulis, valar dohaeris.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://www.mazhixian.me"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Install-tensorflow-with-gpu-library-CUDA-on-Ubuntu-16-04-x64" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/13/Install-tensorflow-with-gpu-library-CUDA-on-Ubuntu-16-04-x64/" class="article-date">
  <time datetime="2017-12-13T06:33:31.000Z" itemprop="datePublished">2017-12-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/13/Install-tensorflow-with-gpu-library-CUDA-on-Ubuntu-16-04-x64/">Install tensorflow with gpu library CUDA on Ubuntu 16.04 x64</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="System-and-Software-info"><a href="#System-and-Software-info" class="headerlink" title="System and Software info"></a>System and Software info</h3><ol>
<li>System: Ubuntu16.04</li>
<li>GPU card: Nvidia GeForce GT 620</li>
<li>tensoflow-gpu==1.2.1</li>
<li>CUDA: 8.0 <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="external">https://developer.nvidia.com/cuda-downloads</a> </li>
<li>cuDNN: v5.1 <a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="external">https://developer.nvidia.com/rdp/cudnn-download</a></li>
</ol>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li>TensofFlow <a href="http://www.tensorflow.org" target="_blank" rel="external">http://www.tensorflow.org</a></li>
<li>Chinese staffs<br>[1] <a href="http://blog.csdn.net/yichenmoyan/article/details/48679777" target="_blank" rel="external">http://blog.csdn.net/yichenmoyan/article/details/48679777</a><br>[2] <a href="http://blog.csdn.net/niuwei22007/article/details/50439478" target="_blank" rel="external">http://blog.csdn.net/niuwei22007/article/details/50439478</a></li>
</ol>
<h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><ul>
<li><p>Install cuda, and configure path and LD_LIBRARY_PATH</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo dpkg -i cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install cuda</div></pre></td></tr></table></figure>
</li>
<li><p>Configure path</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ vim ~/.bashrc</div><div class="line">$ <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/usr/<span class="built_in">local</span>/cuda-8.0/bin</div><div class="line">$ <span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-8.0/lib64:/lib</div></pre></td></tr></table></figure>
</li>
<li><p>Install cuDNN</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ tar -xvf cudnn-8.0-linux-x64-v5.1.tgz</div><div class="line">$ <span class="built_in">cd</span> cuda</div><div class="line">$ sudo cp ./lib64/* /usr/<span class="built_in">local</span>/cuda/lib64/</div><div class="line">$ sudo chmod 755 /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn*</div><div class="line">$ sudo cp ./include/cudnn.h /usr/<span class="built_in">local</span>/cuda/include/</div></pre></td></tr></table></figure>
</li>
<li><p>Install TensorFlow</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ &lt;sudo&gt; pip3 install &lt;--user&gt; &lt;--update&gt; tenforflow-gpu==1.2.1</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><p>Note that cuda 8.0 doesn’t support the default g++ version. Install an supported version and make it the default.<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install g++-4.9</div><div class="line"></div><div class="line">$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 20</div><div class="line">$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5 10</div><div class="line"></div><div class="line">$ sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 20</div><div class="line">$ sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 10</div><div class="line"></div><div class="line">$ sudo update-alternatives --install /usr/bin/cc cc /usr/bin/gcc 30</div><div class="line">$ sudo update-alternatives --<span class="built_in">set</span> cc /usr/bin/gcc</div><div class="line"></div><div class="line">$ sudo update-alternatives --install /usr/bin/c++ c++ /usr/bin/g++ 30</div><div class="line">$ sudo update-alternatives --<span class="built_in">set</span> c++ /usr/bin/g++</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/12/13/Install-tensorflow-with-gpu-library-CUDA-on-Ubuntu-16-04-x64/" data-id="cjb4opx1y001kbg67nct55y6m" class="article-share-link">Partager</a>
      
        <a href="http://www.mazhixian.me/2017/12/13/Install-tensorflow-with-gpu-library-CUDA-on-Ubuntu-16-04-x64/#disqus_thread" class="article-comment-link">Commentaires</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/">deep-learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-20171118" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/11/18/20171118/" class="article-date">
  <time datetime="2017-11-18T08:02:06.000Z" itemprop="datePublished">2017-11-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/11/18/20171118/">Happy or sad?</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>整一个月没有写过东西了，无论线上还是线下。每天重复着调程序，改论文，收发邮件的工作，突然有个周六闲下来了，反而很恐慌。好像总会在情绪很低的时候才会有记录的冲动。这一个月其实有很多故事，等理清了头绪，也许可以好好说说。</p>
<p>从标题开始说吧，”Happy or sad?”是我空间的访客问题，答案一开始是No，后来改成了未知。情绪的变化往往在一瞬间，前一秒会因为程序跑出好结果而开心，后一秒可能看到心仪的人有了新欢而难过。喜怒无偿是个贬义词，但又悲喜至少说明对周遭还有反应，还好好地活着。</p>
<p>师弟遇到了人生中的一个大槛，与他深谈的过程中，我总在强调一个观点，“如果过得很累，至少说明还活着”。这句话想想就很丧，然而也是现实。每个阶段都有每个阶段的痛苦，真的只有彻底gg了才不会再有劳累。所以，与其这么丧，不如好好做手头的事，充实起来的感觉好过无所事事。</p>
<p>再问个问题吧，“如果你喜欢的人有喜欢的人，你会如何调节自己“？又是一个好丧的问题。。。一个月前我有了这种体会，一个月后好友也遇到了这个问题。我们当然可以在释然以后说一句”C’est la vie.” 但心结真的能解开吗？ 哈哈，这就是生活吧。</p>
<p>不知道怎么结尾了，假装这里有个结尾吧，有些事只能藏在心里。</p>
<p>Nice day.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/11/18/20171118/" data-id="cjb4omtd90004bg67vxfzy4kz" class="article-share-link">Partager</a>
      
        <a href="http://www.mazhixian.me/2017/11/18/20171118/#disqus_thread" class="article-comment-link">Commentaires</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/life/">life</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-20171010" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/10/10/20171010/" class="article-date">
  <time datetime="2017-10-10T04:59:54.000Z" itemprop="datePublished">2017-10-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/10/10/20171010/">Convolution in signal processing VS. convolution in CNN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>萌节快乐，LOL!!</p>
<p>中午被老板召唤，讨论了论文的算法部分，其中有传统卷积和CNN中卷积的区别，以及他们和相关函数的关系，感觉可以讨论一下,装个B。（这里不涉及时频分析了，因为我忘记了。。。）</p>
<h3 id="Convolution-in-signal-processing"><a href="#Convolution-in-signal-processing" class="headerlink" title="Convolution in signal processing"></a>Convolution in signal processing</h3><p>首先说一下我对信号处理中的卷积的理解。信号处理的卷积表达的物理意义是<strong>信号通过系统以后的输出，是信号与系统相互作用的结果</strong>.</p>
<p>定义<script type="math/tex">f(t)</script>表示信号，<script type="math/tex">h(t)</script>表示，输出信号为<script type="math/tex">g(t)</script>，则有</p>
<script type="math/tex; mode=display">
\begin{equation}
g(t) = f(t)*h(t) = \int^{\infty}_{-\infty}{f(t-\tau)h(\tau)d\tau}.
\end{equation}</script><p>其中<script type="math/tex">\tau</script>是积分变量。</p>
<p>这里的<script type="math/tex">t-\tau</script>理解为对信号做对称 (翻折) 变换，其物理意义是在<script type="math/tex">t_0</script>时刻通过系统并输出的信号<script type="math/tex">g(t_0)</script>除了受到<script type="math/tex">f(t_0)</script>的影响，还受到<script type="math/tex">t<t_0</script>的信号通过系统以后的累积作用的影响。通俗的说，某时刻的输出信号是该时刻之前的信号与此刻信号共同作用的结果。</p>
<p>以上是卷积在信号处理中的理解。</p>
<p>推广到高维空间也是如此，最典型的就是图像空间的滤波，用某个核函数与原图做卷积，进行图像处理。例如用拉普拉斯核与原图做图像锐化。下面给出二维空间滤波的连续和离散表达。</p>
<h4 id="Continous"><a href="#Continous" class="headerlink" title="Continous"></a>Continous</h4><script type="math/tex; mode=display">
\begin{align}
g(u,v) &= f(u,v)*h(u,v) \notag \\
&=\int^{\infty}_{-\infty}{\int^{\infty}_{-\infty}}{f(u-\mu,v-\nu)h(\mu,\nu)d{\mu}d{\nu}}.
\end{align}</script><h4 id="Discrete"><a href="#Discrete" class="headerlink" title="Discrete"></a>Discrete</h4><script type="math/tex; mode=display">
\begin{align}
g(m,n) &= f(m,n)*h(n,n) \notag \\
&=\sum_{i}{\sum_{j}{f(m-i,n-j)h(i,j)}}.
\end{align}</script><h3 id="Convolution-in-CNN"><a href="#Convolution-in-CNN" class="headerlink" title="Convolution in CNN"></a>Convolution in CNN</h3><p>然后我们来说卷积神经网络 (Convolutional Neural Network, CNN) 中的卷积，这里卷积的目的除了提取图像的特征，更重要的是缩小网络的参数数量。其物理意义是<strong>利用共享的权值矩阵 (卷积核) 以小块的形式遍历图像矩阵，获取图像在该卷积核下的响应，作为图像的特征</strong>。这里获取的特征也被成为特征图 (feature map)。</p>
<p>因为图像空间不考虑时序性，也即某点与卷积核作用的输出不会受到其他时刻的影响，所以直接做图像与卷积核的相关(correlation)即可。</p>
<p>定义图像矩阵为<script type="math/tex">I(m,n), m=1,\dots,M, n=1,\dots,N</script>，卷积核<script type="math/tex">G(i,j), i,j = 1,\dots,K</script>, 输出的特征图<script type="math/tex">F(p,q), p=1,\dots,P, q=1,\dots,Q</script>,则有</p>
<script type="math/tex; mode=display">
\begin{align}
F(p,q) &= I(m-K:m+K, n-K:n+K)*G \notag \\
&=\sum^{K}_{i=1}{\sum^{K}_{j=1}{I(i,j)G(i,j)}}.
\end{align}</script><p>通常，CNN卷积运算的输出矩阵与输入矩阵大小是不同的，这里涉及到”zero padding”的问题，在离散信号处理中也很常见。我在<a href="http://www.mazhixian.me/2017/09/01/Deconvolution-or-Transpose-opposite-operation-of-CNN/">这篇文章</a>讨论过，感兴趣的可以去围观。</p>
<p>最后，我认为CNN的卷积本质上就是相关运算，与信号处理的卷积不等价。只是因为相关运算的两个矩阵通常具有相同的大小，而CNN的卷积核通常远小于图像矩阵。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/10/10/20171010/" data-id="cjb4omtdj000nbg670lrfkrh5" class="article-share-link">Partager</a>
      
        <a href="http://www.mazhixian.me/2017/10/10/20171010/#disqus_thread" class="article-comment-link">Commentaires</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/signal-processing/">signal-processing</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-20170930" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/30/20170930/" class="article-date">
  <time datetime="2017-09-30T14:04:46.000Z" itemprop="datePublished">2017-09-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/30/20170930/">喝醉了，说说故事</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>九月的最后一天，趁着醉意写点东西吧。</p>
<p>昨天熬夜赶完了老板的工作，白天整个人状态都不好。最近熬了太多通宵，陷入了恶性循环，不敢想像工作以后会是什么状态。。。希望还能有自己的生活。</p>
<p>傍晚的时候，被老板叫去讨论今天的工作，趁机喝了点酒。在坐的还有一位老师，老板在介绍我的时候，有句话让我特别感动， “这个孩子很踏实，交待他的事能够按时完成，而且每次还会多做一些。” 想到了高三的某天，因为语文听写没过，被小白叫去办公室重写。这个时候物理老师克余进来了，两人开始寒暄。小白也说了同样的话，“这个孩子很踏实，他肯定能考好的。” 那一刻很感动，即使当时状态很差，数学刚考了67。但是，后来高考考了班级第一，一步步走到现在。</p>
<p>喝酒的时候，一边听着老板吐槽 (并没有听进去。。。)，一边和好友聊天，提到成年人与孩子的不同。然后就特别感谢这几年遇到的人和事，说不上大彻大悟，但是看淡了很多事情。不再介意别人怎么看你，不去计较无足轻重的得失，每天想着就是做好工作，锻炼身体。这么想想，好像确实老了[捂脸]， 熬个夜都要缓好多天。。。</p>
<p>最后再说个想法吧，也许可以去知乎提[手动偷笑]。如果是你可以回复的消息，尽量早点回复，尤其是对上级，他会记住你的。换位思考，你发出的消息，也是希望有反馈的。就像神经网络，前向传递和后向反馈都是需要的，这样网络才会越来越理解你。什么鬼比喻，一定是喝多了。。。</p>
<p>三俗一下， 十月，请对我好一点。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/30/20170930/" data-id="cjb4omtcz0000bg67g5xdikhn" class="article-share-link">Partager</a>
      
        <a href="http://www.mazhixian.me/2017/09/30/20170930/#disqus_thread" class="article-comment-link">Commentaires</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/life/">life</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-20170926" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/26/20170926/" class="article-date">
  <time datetime="2017-09-26T04:25:01.000Z" itemprop="datePublished">2017-09-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/26/20170926/">为什么我们乐意对陌生人敞开心扉？</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>好久没写blog了，最近处于秋眠的状态。。。</p>
<p>去吃午饭的路上，看到桂花开了，感叹了一句“好香”，这时旁边开着面包车路过的小哥说了同样的话，相视一笑。不知道为什么，我们常常会被路人感动。想到初三暑假，也是午后，在老爸的办公室扒着饭。窗外传来装修工人的口哨声，虽然忘了是什么歌，但当时被感染的快乐情绪，到现在还记得。后来还把感受写进了作文，第一次得了高分。</p>
<p>似乎越亲近的人，我们会顾及所言。大家有自己的生活，多数在为了生计而奔波，你的喜悦或者不快，他们无暇顾及。换位思考，你需要多么宽容，才会真心祝福他人的成就，耐心宽慰他人的悲伤？ 所以，当你在社交网络分享自己的喜悦时，点赞的人寥寥无几，即使是你只给在乎的人看。渐渐的，我们开始把心事说给陌生人，因为陌生人很多，总有人无聊了来和你聊两句，也因为陌生人与你没有利益纠葛，你不用担心自己的隐私泄露。</p>
<p>做个精致的利己主义者其实也挺好的，既然不能包容别人的开心，为何奢求别人对你的宽容。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/26/20170926/" data-id="cjb4omtda0005bg67i3h90jx9" class="article-share-link">Partager</a>
      
        <a href="http://www.mazhixian.me/2017/09/26/20170926/#disqus_thread" class="article-comment-link">Commentaires</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/life/">life</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-20170917" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/17/20170917/" class="article-date">
  <time datetime="2017-09-17T08:47:48.000Z" itemprop="datePublished">2017-09-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/17/20170917/">20170917</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>没想好要起什么名字，姑且用日期来代替吧，记录一下此刻的感受。</p>
<p>在喧闹的咖啡馆里，周围有大一的孩子在玩三国杀; 有老外跟着中国妹子学中文; 有周末来聊工作的; 也有我这种逃离实验室来打酱油的。</p>
<p>有时候得承认，在喧闹的环境下工作效率并不一定低，好像还有相关的叫“白噪声”的理论。我的理解是，噪声一定程度上分散了我们的注意力，反而更容易关注工作本身了，诡异的理论，lol。</p>
<p>刚刚打印的路上路过东转，又遇到了那只白猫，晒着太阳，高冷而慵懒。这次她终于同意我拍照了，来看看异瞳的白猫(手动坏笑)。</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-170917/fig1.jpg?raw=true" height="640" width="360">
</center>

<p>最近试着恢复夜跑和走路上班，既然没有斗志工作，至少要把身体调整好吧。取悦别人很难，获得认可也很难，每个人有自己的生活，自己的圈子。为了证明自己而炫耀，反而成了小丑。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/17/20170917/" data-id="cjb4omtd50001bg67u1ekqqgc" class="article-share-link">Partager</a>
      
        <a href="http://www.mazhixian.me/2017/09/17/20170917/#disqus_thread" class="article-comment-link">Commentaires</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/life/">life</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Tensorflow-namespace-and-network-restoration" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/14/Tensorflow-namespace-and-network-restoration/" class="article-date">
  <time datetime="2017-09-14T08:20:37.000Z" itemprop="datePublished">2017-09-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/14/Tensorflow-namespace-and-network-restoration/">Tensorflow namespace and network restoration</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>昨天讨论了基于TensorFlow的迁移学习，提到了网络的存储和恢复的问题，然而我并没有说清楚，而且网络的恢复要考虑的问题其实挺多的。。。</p>
<p>概括为如下三个问题：</p>
<ol>
<li>变量的命名空间问题;</li>
<li>网络的恢复问题;</li>
<li>网络中tensor名称和数据的获取</li>
</ol>
<h4 id="变量命名空间"><a href="#变量命名空间" class="headerlink" title="变量命名空间"></a>变量命名空间</h4><p>首先，来说明命名空间的问题。通常我们利用<code>tf.Variable</code>或<code>tf.placeholder</code>等初始话一个变量，这两个类都有缺省的命名参数，例如<code>tf.Variable</code>的参数<code>name=Variable</code>。相应的，tf在我们新建变量的过程中按顺序在<code>Variable</code>后添加数字，例如<code>Variable_1</code>,<code>Variable_2</code>等。</p>
<p>这种缺省命名虽然方便，但对网络的存储和恢复会造成很大的影响，所以如果要保存我们的网络，最好的方法是给每个变量设置命名空间。这些变量包括<code>Variables</code>,<code>placeholder</code>以及<code>optimizer</code>。</p>
<h4 id="网络的恢复问题"><a href="#网络的恢复问题" class="headerlink" title="网络的恢复问题"></a>网络的恢复问题</h4><p>如上一篇<a href="http://www.mazhixian.me/2017/09/13/Transfer-learning-with-TensorFlow/">博客</a>所述，tensorflow的<code>tf.train.Saver</code>类既可以存储网络，也可以恢复网络。其中Saver保存的<code>.ckpt</code>文件包含<strong>checkpoint</strong>和<strong>metadata</strong>，分别存储了graph的命名空间和元数据。恢复的时候便是基于他们读取数据到网络中。</p>
<p><strong>但是</strong>，单有checkpoint和metadata是没有用的，tensorflow的核心就是graph，所以我们需要在恢复网络前重新搭建graph，并且这个graph的命名空间要与checkpoint的相同。因此，一定要养成好习惯，在定义网络的时候，给每个变量都设置固定的名称。</p>
<p>除此之外，在一个ipython环境或者notebook下，如果要保存网络，建议只搭建一个graph。因为我发现，<code>tf.train.Saver</code>类在保存checkpoint的时候，会将目前存在的graph全部保存。</p>
<p>下面给一段代码，从这篇<a href="http://blog.csdn.net/lwplwf/article/details/62419087" target="_blank" rel="external">文章</a>复制来的。。。</p>
<h5 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">v1 = tf.Variable(tf.random_normal([<span class="number">1</span>, <span class="number">2</span>]), name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">"v2"</span>)</div><div class="line">init_op = tf.global_variables_initializer() </div><div class="line">saver = tf.train.Saver()</div><div class="line">sess = tf.InteractiveSession()</div><div class="line">sess.run(init_op)</div><div class="line">savepath = <span class="string">"./model.ckpt"</span></div><div class="line">saver.save(sess, savepath)</div></pre></td></tr></table></figure>
<h5 id="恢复模型"><a href="#恢复模型" class="headerlink" title="恢复模型"></a>恢复模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">v1 = tf.Variable(tf.random_normal([<span class="number">1</span>, <span class="number">2</span>]), name=<span class="string">"v1"</span>)</div><div class="line">v2 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">"v2"</span>)</div><div class="line">saver = tf.train.Saver()</div><div class="line">sess = tf.InteractiveSession()</div><div class="line">modelpath = <span class="string">"./model.ckpt"</span></div><div class="line">sess = saver.restore(sess, modelpath)</div></pre></td></tr></table></figure>
<p>从上面可以看出，二者的区别在于<strong>是否需要初始化变量</strong>，这也是网络恢复的核心问题，因为我们的目的就是恢复参数。</p>
<h4 id="网络中tensor名称和数据的获取"><a href="#网络中tensor名称和数据的获取" class="headerlink" title="网络中tensor名称和数据的获取"></a>网络中tensor名称和数据的获取</h4><p>如何从存储的网络中提取变量的命名及其数据，可以参考下面的程序，也是<a href="http://blog.csdn.net/helei001/article/details/56489658" target="_blank" rel="external">抄来的</a>。。。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tensorflow.python <span class="keyword">import</span> pywrap_tensorflow  </div><div class="line">checkpoint_path = os.path.join(model_dir, <span class="string">"model.ckpt"</span>)  </div><div class="line">reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)  </div><div class="line">var_to_shape_map = reader.get_variable_to_shape_map()  </div><div class="line"><span class="keyword">for</span> key <span class="keyword">in</span> var_to_shape_map:  </div><div class="line">    print(<span class="string">"tensor_name: "</span>, key)  <span class="comment"># Ouput variables name</span></div><div class="line">    print(reader.get_tensor(key)) <span class="comment"># Output variables data</span></div></pre></td></tr></table></figure></p>
<p>输出结果类似这样，<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">tensor_name:  Conv_En_b2/cae-optimizer</div><div class="line">tensor_name:  Conv_De_b1/cae-optimizer_1</div><div class="line">tensor_name:  Conv_En_W0</div><div class="line">tensor_name:  De_b/cae-optimizer</div></pre></td></tr></table></figure></p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ul>
<li><a href="http://blog.csdn.net/lwplwf/article/details/62419087" target="_blank" rel="external">TensorFlow学习笔记（8）—网络模型的保存和读取</a></li>
<li><a href="http://blog.csdn.net/helei001/article/details/56489658" target="_blank" rel="external">查看TensorFlow checkpoint文件中的变量名和对应值</a></li>
<li><a href="http://www.mazhixian.me/2017/09/13/Transfer-learning-with-TensorFlow/">Transfer learning with TensorFlow</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/14/Tensorflow-namespace-and-network-restoration/" data-id="cjb4omtdm000sbg67821m8yd0" class="article-share-link">Partager</a>
      
        <a href="http://www.mazhixian.me/2017/09/14/Tensorflow-namespace-and-network-restoration/#disqus_thread" class="article-comment-link">Commentaires</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/">deep-learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Transfer-learning-with-TensorFlow" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/13/Transfer-learning-with-TensorFlow/" class="article-date">
  <time datetime="2017-09-13T08:07:45.000Z" itemprop="datePublished">2017-09-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/13/Transfer-learning-with-TensorFlow/">Transfer learning with TensorFlow</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Today, let’s talk about transfer learning based on TensorFlow. Firstly, what is transfer learning? It is a strategy of building your own deeplearning based project with existing well-trained networks, so as to avoid some risks like overfitting, time-consuming and etc.</p>
<p>In our work, we are trying to classify some astronomical images with convolutional neural networks (CNN). As we know, the CNN networks should be trained with billions of samples to achieve optimum the parameters of weights and biases. However, only thousands of labelled samples do we have, which can’t activate the performance of the network. Since that, we propose to train the network by transfer learning. </p>
<p>Here we come to the main body, i.e. how to realize transfer learning with your computer? In this blog, I’m going to tell you some tricks to realize this staff with <a href="https://www.tensorflow.org" target="_blank" rel="external">TensorFlow</a>, a famous deeplearning framework based on Python.</p>
<h4 id="1-How-to-save-and-restore-the-net"><a href="#1-How-to-save-and-restore-the-net" class="headerlink" title="1. How to save and restore the net?"></a>1. How to save and restore the net?</h4><p>In order to realize the transfer learning, your script should pocess the ability to save and restore the network. Bellow are the code example of this two processes.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># Save the session as a check point</span></div><div class="line">sess = tf.InteractiveSession() <span class="comment"># Instance a session</span></div><div class="line">saver = tf.train.Saver() <span class="comment"># Instance a saver</span></div><div class="line">saver.save(sess, savepath) <span class="comment"># save the session, i.e. the network</span></div><div class="line"></div><div class="line"><span class="comment"># Restore</span></div><div class="line"><span class="comment"># [Note] The saver should be initialized after the graph defined...</span></div><div class="line">graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> graph.as_default():</div><div class="line">    <span class="comment"># [Variable and model creation goes here.]</span></div><div class="line">    saver = tf.train.Saver()</div><div class="line">sess = tf. InteractiveSession()</div><div class="line">saver.restore(sess, netpath)</div></pre></td></tr></table></figure></p>
<p>It should be noted that, the instancing of <code>saver</code> should be after defination of the network. I find that though the restored session <code>sess</code> saved the graph of the newtork, including tensors, variables, and operations, the variables still need to be initialized when training after restoration.</p>
<h4 id="2-How-to-realize-the-transfer-learning"><a href="#2-How-to-realize-the-transfer-learning" class="headerlink" title="2. How to realize the transfer learning?"></a>2. How to realize the transfer learning?</h4><p>Here we come to the transfer learning. A typical process is that we save the paramters of the convolutional layers (ConvLayer), and replace the fully connected and output layers with new weights and biases. This is because the ConvLayers are the part to <strong>extract features</strong> of those samples, while the fully connected layers composing the <strong>classifier</strong> itself. Our target is to save the feature represantation part and update the classifier according to our project.</p>
<p>Denote the last output of the ConvLayers is a tensor with name <code>ConvLayer_output</code>, and suppose a session namely <code>sess</code> has been restored, then we can build our own classifier, see as follows,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Get tensor 'ConvLayer_output' from sess</span></div><div class="line">l_conv_output = sess.graph.get_tensor_by_name(<span class="string">"ConvLayer_output"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Add new fully connected layers and softmax layer</span></div><div class="line"><span class="comment"># Suppose we have 10 class to be classied</span></div><div class="line">numclass = <span class="number">10</span></div><div class="line">y_ = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, numclass], name=<span class="string">"cnn-softmax"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Add a fully connected as an example</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">	initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</div><div class="line">    <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</div><div class="line">    <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line">last_conv_shape = l_conv_output.get_shape().as_list()</div><div class="line">input_shape = last_conv_shape[<span class="number">1</span>] * last_conv_shape[<span class="number">2</span>] * last_conv_shape[<span class="number">3</span>]</div><div class="line">output_shape = <span class="number">1024</span></div><div class="line">W_fc = weight_variable(shape = [input_shape, output_shape])</div><div class="line">b_fc = bias_variable(shape=[output_shape])</div><div class="line">l_fc = tf.nn.relu(tf.matmul(l_conv_output, W_fc) + b_fc)</div><div class="line"></div><div class="line"><span class="comment"># Fully connected to softmax</span></div><div class="line">input_shape = <span class="number">1024</span></div><div class="line">output_shape = numclass</div><div class="line">W_soft = weight_variable(shape = [input_shape, output_shape])</div><div class="line">b_soft = bias_variable(shape = [output_shape])</div><div class="line">l_y = tf.nn.softmax(tf.matmul(l_fc, W_soft) + b_soft)</div></pre></td></tr></table></figure></p>
<p>After that, our new network can be trained on our samples,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Initialize all the parameters, including the pretrained net and concated layers</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line">sess.run(init_op)</div><div class="line"></div><div class="line"><span class="comment"># [Training lines goes here]</span></div></pre></td></tr></table></figure></p>
<p>Finally, we obtain the network, and evaluations can be conducted, enjoy yourselves.</p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ul>
<li><a href="https://stackoverflow.com/questions/36281129/no-variable-to-save-error-in-tensorflow" target="_blank" rel="external">No variable to save error in Tensorflow</a></li>
<li><a href="https://www.tensorflow.org" target="_blank" rel="external">Tensorflow</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/13/Transfer-learning-with-TensorFlow/" data-id="cjb4omtdp000ybg676sce1vpm" class="article-share-link">Partager</a>
      
        <a href="http://www.mazhixian.me/2017/09/13/Transfer-learning-with-TensorFlow/#disqus_thread" class="article-comment-link">Commentaires</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/">deep-learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Fourier-transform-on-2D-image" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/12/Fourier-transform-on-2D-image/" class="article-date">
  <time datetime="2017-09-12T14:55:34.000Z" itemprop="datePublished">2017-09-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/12/Fourier-transform-on-2D-image/">Fourier transform on 2D image</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天讨论二维矩阵 (图像) 的Fourier transoform，这个问题缘起师兄的工作，“当图像的灰度波动较小，或者说只有大尺度结构时，它的傅立叶变换的图像会在中间(低频)区域出现一个十字。”而这个诡异的十字严重影响了后续的工作。为什么会有这个十字？我们有一个猜想，也试着证明了一下，但不确定是不是这个原因，先挖个坑再说。。。</p>
<p>基本的傅立叶变换我就不介绍了。它的定义很简单，<strong>用正交的余弦函数作为base，对时域或空域的信号进行分解，并在频域描述信号.</strong> 这样做的好处很多，例如能够实现信号的滤波、能够压缩信号、能够将卷积转化为乘法等。</p>
<p>我准备先零散地抛几个“有(wu)趣(liao)”的点出来，第一个就是二维傅立叶变换的<code>时移问题</code>，这里我觉得用<code>bias</code>来描述更好，很无聊，但它的推导还是挺有价值的。设二维实数空间<script type="math/tex">f(x,y), x,y\in R</script>，其傅立叶变换为<script type="math/tex">F(m,n)</script>，则有，</p>
<script type="math/tex; mode=display">
\begin{equation}
F(m,n) = \iint{f(x,y)e^{-j2\pi mx}e^{-j2\pi ny}}dydx,
\end{equation}</script><p>现假设在$x$和$y$方向的偏移(时移)分别为$b_x$和$b_y$，那么，Eq.(1)变为，</p>
<script type="math/tex; mode=display">
\begin{align}
F(m,n) &= \iint{f(x,y)e^{-j2\pi m(x-b_x)}e^{-j2\pi n(y-b_y)}}dydx \notag \\
       &= e^{-j2\pi(mb_x + nb_y)}\iint{f(x,y)e^{-j2\pi mx}e^{-j2\pi ny}}dydx.
\end{align}</script><p>而上式中的<script type="math/tex">e^{-j2\pi(mb_x + nb_y)}</script>利用欧拉公式展开，我们得到，</p>
<script type="math/tex; mode=display">
\begin{align}
e^{-j2\pi(mb_x + nb_y)} &= cos(2\pi(mb_x+nb_y)) - jsin(2\pi(mb_x+nb_y)).
\end{align}</script><p>显然，<script type="math/tex">mb_x+nb_y</script>可以拆分成向量$(m,n)$与$(b_x,b_y)$的内积，代表频域空间的向量<script type="math/tex">(m,n)</script>在偏移方向<script type="math/tex">(b_x,b_y)</script>上的投影，而<script type="math/tex">(b_x,b_y)</script>又共同构成了该种投影的周期性，导致在傅立叶变换的图像上出现周期性变化的明暗条纹。</p>
<p>说完时移的问题，我们来提出图像傅立叶变换以后<strong>诡异十字</strong>问题的猜想，<strong>图像边界处有明显的灰度变化会导致傅立叶变换后的频域图像出现十字。</strong>可以理解为在图像边界处人为做了<code>truncated</code>，即给图像加了矩形窗，导致在频域的<strong>vertical</strong>和<strong>horizontal</strong>两个方向乘了一个<code>sinc</code>函数，而由于这个窗很大，所以<code>sinc</code>的影响集中在靠近中心(低频)的区域。</p>
<p>先留个坑，明天加图。。。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/12/Fourier-transform-on-2D-image/" data-id="cjb4omtdo000vbg670c4wb697" class="article-share-link">Partager</a>
      
        <a href="http://www.mazhixian.me/2017/09/12/Fourier-transform-on-2D-image/#disqus_thread" class="article-comment-link">Commentaires</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/signal-processing/">signal-processing</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Data-visualization-with-t-SNE" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/11/Data-visualization-with-t-SNE/" class="article-date">
  <time datetime="2017-09-11T06:21:55.000Z" itemprop="datePublished">2017-09-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/09/11/Data-visualization-with-t-SNE/">Data visualization with t-SNE</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天我们讨论数据可视化(data visualization)算法t-SNE (t-distributed Stochastic Neighbor Embedding)，该方法的目的是<strong>映射高维数据向量到低维，并保留向量的相似性或者距离</strong>。通常我们描述距离会使用类似欧式距离或黎曼距离等概念，映射的方法也多为线性，如PCA。而t-SNE不同，它是用联合概率来描述样本点的相似程度，是非线性的。</p>
<p>SNE算法最早由 <a href="http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding" target="_blank" rel="external">Hinton &amp; Roweis</a> 在2002年提出。他们定义高维空间两个点<script type="math/tex">\bf{x_i}</script>和<script type="math/tex">\bf{x_j}</script>相似度的联合概率分布<script type="math/tex">P = p_{ij}</script>，和要映射到的低维空间对应两点<script type="math/tex">\bf{y_i}</script>和<script type="math/tex">\bf{y_j}</script>的联合概率<script type="math/tex">Q = q_{ij}</script>. 利用<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="external">Kullback-Leibler divergence</a>衡量分布$P$和$Q$的相似性，进而最小化这一距离来确定低维空间的映射点<script type="math/tex">\bf{y_i}</script>.</p>
<p>考虑到SNE算法存在三个缺陷,</p>
<ol>
<li>KL-divergence 不是对称的;</li>
<li>从高维空间映射到低维空间，距离会发生变化;</li>
<li>高斯分布不是长尾的，对小概率的异常点描述能力较差;</li>
</ol>
<p>为了解决这一问题，<a href="http://jmlr.csail.mit.edu/papers/v9/vandermaaten08a.html" target="_blank" rel="external">Van der Maaten &amp; Hinton</a> 在2008年提出了改进算法，利用自由度为1的t分布替换高斯分布来定义低维空间的距离，避免异常点，保留高维空间两点的距离关系；设计了具有对称性的概率分布函数。最后，分布$P$和$Q$，KL-divergence $\mathrm{KL}$ 以及对应的梯度公式<script type="math/tex">\partial{\mathrm{KL}}/\partial{y_i}</script>如下，</p>
<h5 id="高维空间分布-P"><a href="#高维空间分布-P" class="headerlink" title="高维空间分布$P$"></a>高维空间分布$P$</h5><script type="math/tex; mode=display">
\begin{equation}
p_{j|i} = \frac{\exp{(-\lVert{\bf{x_{i}} - \bf{x_{j}}}\rVert}^2 / 2\sigma^2_{i})}{\sum_{k\neq i }{\exp{(-\lVert{\bf{x_{i}} - \bf{x_{k}}}\rVert}^2 / 2\sigma^2_{i})}},
\end{equation}</script><p>其中，<script type="math/tex">p_{j|i}</script>表示<script type="math/tex">\bf{x_i}</script>接受<script type="math/tex">\bf{x_j}</script>为其同类点的条件概率，相应的<script type="math/tex">p_{ij}</script>由下式给出，</p>
<script type="math/tex; mode=display">
\begin{equation}
p_{ij} = \frac{p_{j|i} + p_{i|j}} {2N}.
\end{equation}</script><p>其中$N$表示样本点的个数。</p>
<h5 id="低维空间分布-Q"><a href="#低维空间分布-Q" class="headerlink" title="低维空间分布$Q$"></a>低维空间分布$Q$</h5><script type="math/tex; mode=display">
\begin{equation}
q_{ij} = \frac{(1 + {\lVert {\bf{y_{i}} - \bf{y_{j}}} \rVert}^2)^{-1}}{\sum_{k\neq m}{(1 + {\lVert {\bf{y_{k}} - \bf{y_{m}}} \rVert}^2)^{-1}}}.
\end{equation}</script><h5 id="KL-divergence"><a href="#KL-divergence" class="headerlink" title="KL-divergence"></a>KL-divergence</h5><script type="math/tex; mode=display">
\begin{equation}
\mathrm{KL}(P||Q) = \sum_{i\neq j}{p_{ij}\log{\frac{p_{ij}}{q_{ij}}}}.
\end{equation}</script><h5 id="partial-difference-of-KL-over-bf-y-i"><a href="#partial-difference-of-KL-over-bf-y-i" class="headerlink" title="partial difference of KL over $\bf{y_i}$"></a>partial difference of KL over $\bf{y_i}$</h5><script type="math/tex; mode=display">
\begin{equation}
\frac{\partial{\mathrm{KL}}}{\partial{\bf{y_{i}}}} = 4\sum_{j}{(p_{ij}-q_{ij})(y_{i}-y_{j})(1+(\lVert{y_{i} - y_{j}})^{2})^{-1}},
\end{equation}</script><p>基于以上公式，利用梯度进行优化，求解局部最优，即可对$\bf{y_i}$进行定位。</p>
<h4 id="t-SNE的python实现"><a href="#t-SNE的python实现" class="headerlink" title="t-SNE的python实现"></a>t-SNE的python实现</h4><p>python的<a href="http://scikit-learn.org/" target="_blank" rel="external">Scikit-learn</a>提供了TSNE类，用于自动化实现数据可视化，代码样例如下，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dimension_reduction_tSNE</span><span class="params">(code,params=None)</span>:</span></div><div class="line">	tsne = TSNE()</div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> params.keys():</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            setattr(tsne, key, params[<span class="string">'key'</span>])</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            <span class="keyword">continue</span></div><div class="line">    code_dim = tsne.fit_transform(code)</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> code_dim</div><div class="line"></div><div class="line">code = xxx</div><div class="line">params=&#123;<span class="string">'n_components'</span>: <span class="number">2</span>, <span class="string">'learning_rate'</span>: <span class="number">100</span>&#125;</div><div class="line"></div><div class="line">code_dim = dimension_reduction_tSNE(code, params)</div></pre></td></tr></table></figure></p>
<p>其中<code>code</code>表示高维空间的样本矩阵，每行对应一个样本。参数<code>params</code>里的<code>n_components</code>表示低维空间的维数。</p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ul>
<li><a href="http://bindog.github.io/blog/2016/06/04/from-sne-to-tsne-to-largevis/" target="_blank" rel="external">从SNE到t-SNE再到LargeVis</a></li>
<li><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" target="_blank" rel="external">t-distributed stochastic neighbor embedding</a></li>
<li><a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution" target="_blank" rel="external">Student’s t-distrubiton</a> </li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.mazhixian.me/2017/09/11/Data-visualization-with-t-SNE/" data-id="cjb4omtdq000zbg67i4rzsmfc" class="article-share-link">Partager</a>
      
        <a href="http://www.mazhixian.me/2017/09/11/Data-visualization-with-t-SNE/#disqus_thread" class="article-comment-link">Commentaires</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/">deep-learning</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Mot-clés</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/MATLAB/">MATLAB</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learning/">deep-learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/life/">life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/signal-processing/">signal-processing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/strange-hobbies/">strange-hobbies</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Nuage de mot-clés</h3>
    <div class="widget tagcloud">
      <a href="/tags/MATLAB/" style="font-size: 10px;">MATLAB</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/deep-learning/" style="font-size: 20px;">deep-learning</a> <a href="/tags/hexo/" style="font-size: 13.33px;">hexo</a> <a href="/tags/life/" style="font-size: 20px;">life</a> <a href="/tags/python/" style="font-size: 16.67px;">python</a> <a href="/tags/signal-processing/" style="font-size: 13.33px;">signal-processing</a> <a href="/tags/strange-hobbies/" style="font-size: 10px;">strange-hobbies</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Articles récents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/12/13/Install-tensorflow-with-gpu-library-CUDA-on-Ubuntu-16-04-x64/">Install tensorflow with gpu library CUDA on Ubuntu 16.04 x64</a>
          </li>
        
          <li>
            <a href="/2017/11/18/20171118/">Happy or sad?</a>
          </li>
        
          <li>
            <a href="/2017/10/10/20171010/">Convolution in signal processing VS. convolution in CNN</a>
          </li>
        
          <li>
            <a href="/2017/09/30/20170930/">喝醉了，说说故事</a>
          </li>
        
          <li>
            <a href="/2017/09/26/20170926/">为什么我们乐意对陌生人敞开心扉？</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Jason Ma<br>
      Propulsé by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'mazhixian';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>