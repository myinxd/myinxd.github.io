
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jason&#39;s blog">
    <title>Tag: deep-learning - Jason&#39;s blog</title>
    <meta name="author" content="Jason Ma">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <meta name="description" content="Valar morghulis, valar dohaeris.">
<meta property="og:type" content="blog">
<meta property="og:title" content="Jason&#39;s blog">
<meta property="og:url" content="http://www.mazhixian.me/all-tags/deep-learning/index.html">
<meta property="og:site_name" content="Jason&#39;s blog">
<meta property="og:description" content="Valar morghulis, valar dohaeris.">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jason&#39;s blog">
<meta name="twitter:description" content="Valar morghulis, valar dohaeris.">
    
    
        
    
    
        <meta property="og:image" content="http://www.mazhixian.me/assets/images/profile.jpg"/>
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-pz4cc6y13wt2trzqa8l3n9v0yykr0sstdaheem7qj628nhjmhp9pfawvqawz.min.css">
    <!--STYLES END--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="1">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Jason&#39;s blog</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
            <img class="header-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="1">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="/#about">
                    <img class="sidebar-profile-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Jason Ma</h4>
                
                    <h5 class="sidebar-profile-bio"><p>We are in the same story.</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="Categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="Search"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Search</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/myinxd" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fa fa-lg fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:zx@mazhixian.me" target="_blank" rel="noopener" title="Mail">
                    
                        <i class="sidebar-button-icon fa fa-lg fa-envelope-o" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/404.html"
                            
                            title="Naive"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-hourglass-start" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Naive</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="RSS"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="1"
                 class="
                        hasCoverMetaIn
                        ">
                
    <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/01/21/resnet-with-tensorflow/">
                            Residual network I -- block and bottleneck
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-01-21T14:15:00+08:00">
	
		    Jan 21, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>ResNet (Residual Neural Network)由何凯明等在15年提出，这里做个笔记，参考了<a href="https://www.amazon.cn/dp/B06X8Z4BS9/" target="_blank" rel="external">TensorFlow 实战</a>的6.4节。ResNet的结构能加速超深射精网络的训练，能有效避免过拟合，而且推广性非常好。</p>
<p>深度神经网络存在<code>Degradation</code>的问题，即随着网络层数的增加，其准确率会从逐渐提升到饱和，进而下降。这一问题在我们近期的工作也出现了，我们在增加了一层全连接后，网络的准确率反而下降了。为了解决这一问题，ResNet提出了一种新的网络思路，<strong>即允许原始输入信息直接传输到后面的层中</strong>。假定某段网络的输入是<script type="math/tex">\mathrm{x}</script>，期望输出是<script type="math/tex">H(\mathrm{x})</script>，如果直接将<script type="math/tex">\mathrm{x}</script>传到输出作为初始结果，那么目标函数变为</p>
<script type="math/tex; mode=display">
F(\mathrm{x}) = H(\mathrm{x}) - \mathrm{x}</script><p>如下图所示为一个ResNet的残差学习单元，即将学习目标从<script type="math/tex">H(\mathrm{x})</script>改为期望与输入的残差。这种结构也被称为<code>shortcut</code>或<code>skip connections</code>.</p>
<p><img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180119/fig3.png?raw=true" alt="Resnet example"></p>
<p>考虑到神经网络相当于对原始数据进行了压缩，存在信息损失的问题，而ResNet结构将部分输入信息直接传递到输出端，可以保留部分的信息。并且ResNet的训练误差会随着层数增加而逐渐减少，并且在测试集上也有很好的表现。</p>
<p>引自TensorFlow实战，在ResNet的第二篇论文<em>Identity mapping in deep residual networks</em>中，提出了ResNet V2。想对于ResNet V1, 将激活函数<code>ReLU</code>改为了<code>Identity mapping</code>，即y=x。同时，ResNet V2在每一层都使用了Batch Normalization，提升网络的范化能力。</p>
<h3 id="ResNet的block的理解"><a href="#ResNet的block的理解" class="headerlink" title="ResNet的block的理解"></a>ResNet的block的理解</h3><p>在ResNet的实现中，包含多个<code>block</code>，每个<code>block</code>又由多个<code>bottleneck</code>组成，称为<strong>残差学习单元</strong>，ResNet的残差运算即在每个bottleneck内实现。因为要获取输入<script type="math/tex">\mathrm{x}</script>与block输出的残差，每个<code>bottleneck</code>虽然做了多次卷积和激活运算，但其输出的<code>shape</code>应该保持不变，因此内部的卷积运算的<code>stride</code>和<code>padding</code>都有特殊的设置 (数学推导可以参考<a href="http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html" target="_blank" rel="external">此文</a>)。以ResNet-50的第二个block为例，假设该block内包含两个bottleneck单元，则有，</p>
<h5 id="Bottleneck-One"><a href="#Bottleneck-One" class="headerlink" title="Bottleneck One"></a>Bottleneck One</h5><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Layer</th>
<th style="text-align:center">Input</th>
<th style="text-align:center">Kernel</th>
<th style="text-align:center">Stride</th>
<th style="text-align:center">Padding</th>
<th style="text-align:center">Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Conv1,1</td>
<td style="text-align:center">56x56x64</td>
<td style="text-align:center">1x1x64</td>
<td style="text-align:center">1</td>
<td style="text-align:center">SAME</td>
<td style="text-align:center">56x56x64</td>
</tr>
<tr>
<td style="text-align:center">Conv1,2</td>
<td style="text-align:center">56x56x64</td>
<td style="text-align:center">3x3x64</td>
<td style="text-align:center">1</td>
<td style="text-align:center">SAME</td>
<td style="text-align:center">56x56x64</td>
</tr>
<tr>
<td style="text-align:center">Conv1,3</td>
<td style="text-align:center">56x56x64</td>
<td style="text-align:center">1x1x256</td>
<td style="text-align:center">1</td>
<td style="text-align:center">SAME</td>
<td style="text-align:center">56x56x256</td>
</tr>
</tbody>
</table>
</div>
<h5 id="Bottleneck-Two"><a href="#Bottleneck-Two" class="headerlink" title="Bottleneck Two"></a>Bottleneck Two</h5><div class="table-container">
<table>
<thead>
<tr>
<th>Layer</th>
<th style="text-align:center">Input</th>
<th style="text-align:center">Kernel</th>
<th style="text-align:center">Stride</th>
<th style="text-align:center">Padding</th>
<th style="text-align:center">Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>Conv2,1</td>
<td style="text-align:center">56x56x256</td>
<td style="text-align:center">1x1x64</td>
<td style="text-align:center">1</td>
<td style="text-align:center">SAME</td>
<td style="text-align:center">56x56x64</td>
</tr>
<tr>
<td>Conv2,2</td>
<td style="text-align:center">56x56x64</td>
<td style="text-align:center">3x3x64</td>
<td style="text-align:center">1</td>
<td style="text-align:center">SAME</td>
<td style="text-align:center">56x56x64</td>
</tr>
<tr>
<td>Conv2,3</td>
<td style="text-align:center">56x56x64</td>
<td style="text-align:center">1x1x256</td>
<td style="text-align:center"><em>2</em></td>
<td style="text-align:center">SAME</td>
<td style="text-align:center"><em>28x28x256</em></td>
</tr>
</tbody>
</table>
</div>
<p>注意<code>Bottleneck Two</code>的<code>Conv2,3</code>层的<code>stride</code>为2,因此输出的尺度大小相当于做了factor为2的降采样，因此该bottleneck不做残差的运算，其输出直接作为下一个block的输入。</p>
<p>ResNet-V2相对于ResNet-V1增加了<code>Batch Normalization</code>和<code>L2</code>正则，如何在block中体现？ 且听下回分解。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/01/21/resnet-with-tensorflow/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/01/19/adjustable-learning-rate-for-deep-learning-by-tensorflow/">
                            Adjustable learning rate for deep learning by TensorFlow
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-01-19T09:11:34+08:00">
	
		    Jan 19, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>趁着最近空闲一点，继续纠结TensorFlow学习率调整的问题。学习率对网路训练的影响还是挺大的，初始化时可以采用大的学习率，帮助网络快速收敛。但网络的梯度是非线性的，随着迭代次数的增加，梯度的导数趋于变小，如果继续保持大学习率，会出现优化目标在最优解附近波动的情况，如下图所示。此时，为了接近收敛点，需要调整学习率。</p>
<p><img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180119/fig1.png?raw=true" alt="LR example"></p>
<p>参考<a href="https://www.amazon.cn/dp/B06WGP12TV/ref=sr_1_1?ie=UTF8&amp;qid=1516326593&amp;sr=8-1&amp;keywords=Tensorflow" target="_blank" rel="external">TensorFlow: 实战Google深度学习框架</a>这本书，利用<strong>指数衰减</strong>的方法设置<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="external">梯度下降算法</a>的学习率，TensofFLow中集成了这一算法，即<code>tf.train.exponential_decay</code>。其实现了</p>
<script type="math/tex; mode=display">
L^{t+1}_{R} = L^{t}_{R} \cdot R_{d}^{S_g/S_d},</script><p>其中$L_R$表示学习率，$R_d$表示衰减率，$S_g$和$S_d$分别表示总Epochs和每个Epochs中的Batches。从而没迭代一轮，学习率便更新一次。</p>
<p>在TensorFlow中可以用如下代码实现指数衰减的学习率更新。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">init_lr = tf.Variable(<span class="number">0.</span>, name=<span class="string">"LR"</span>)</div><div class="line">global_step = tf.Variable(<span class="number">0.</span>, name=<span class="string">"global_step"</span>)</div><div class="line">decay_step = tf.Variable(<span class="number">0.</span>, name=<span class="string">"decay_step"</span>)</div><div class="line">decay_rate = tf.Variable(<span class="number">0.</span>, name=<span class="string">"decay_rate"</span>)</div><div class="line">learning_rate = tf.train.exponential_decay(</div><div class="line">    learning_rate = init_lr ,</div><div class="line">    global_step = global_step,</div><div class="line">    decay_steps = decay_step,</div><div class="line">    decay_rate = decay_rate,</div><div class="line">    staircase=<span class="keyword">False</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">	)</div><div class="line"><span class="comment"># Test</span></div><div class="line">lr_init = <span class="number">0.1</span></div><div class="line">epochs = <span class="number">200</span></div><div class="line">batches = <span class="number">100.</span></div><div class="line">d_rate = <span class="number">0.9</span></div><div class="line">epoch = np.arange(<span class="number">0</span>,epochs,<span class="number">1</span>)</div><div class="line">lr = np.zeros(epoch.shape)</div><div class="line"><span class="comment"># Init a session</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line">sess = tf.InteractiveSession()</div><div class="line">sess.run(init_op)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> epoch.astype(int):</div><div class="line">	lr[i] = sess.run(learning_rate, </div><div class="line">                     feed_dict=&#123;init_lr: lr_init,</div><div class="line">                                global_step: i,</div><div class="line">                                decay_step: batches,</div><div class="line">                                decay_rate: d_rate</div><div class="line">                                &#125;)</div></pre></td></tr></table></figure></p>
<p><a href="https://github.com/myinxd/canal-images/blob/master/images/blog-180119/Learning_rate_adjust_demo.ipynb" target="_blank" rel="external">这里</a>提供了一个样例，其输出结果如下图，通过设置<code>tf.exponential_decay</code>的参数<code>staircase</code>可以控制learning rate是否为阶梯型或者平滑的。</p>
<p><img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180119/fig2.png?raw=true" alt="Learning rate result"></p>
<p>而在训练网络，优化目标函数的过程中，可以参考官方手册，用如下语句进行梯度更新。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</div><div class="line">starter_learning_rate = <span class="number">0.1</span></div><div class="line">learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,</div><div class="line">                                           <span class="number">100000</span>, <span class="number">0.96</span>, staircase=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># Passing global_step to minimize() will increment it at each step.</span></div><div class="line">learning_step = (</div><div class="line">    tf.train.GradientDescentOptimizer(learning_rate)</div><div class="line">    .minimize(...my loss..., global_step=global_step)</div><div class="line">)</div></pre></td></tr></table></figure></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] <a href="https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay" target="_blank" rel="external">exponential decay</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/01/19/adjustable-learning-rate-for-deep-learning-by-tensorflow/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/01/03/monte-carlo-simulation/">
                            monte carlo simulation
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-01-03T20:24:28+08:00">
	
		    Jan 03, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <h2 id="To-be-continue…"><a href="#To-be-continue…" class="headerlink" title="To be continue…"></a>To be continue…</h2><p>蒙特卡罗方法帮助产生服从样本分布的新样本，并且实现参数的估计，是一种基于贝叶斯统计的算法。机器学习中的许多重要工具都基于从某种分布中采样，以及用这些样本对目标量做一个蒙特卡罗估计。</p>
<h3 id="Why-sampling-and-Monte-Carlo"><a href="#Why-sampling-and-Monte-Carlo" class="headerlink" title="Why sampling and Monte Carlo?"></a>Why sampling and Monte Carlo?</h3><p>当无法精确计算和或积分时，通常可以使用蒙特卡罗采样来近似。这种想法把和或积分视作某分布下的期望，然后通过估计对应的平均值来近似这个期望。而这里的估计用到了中心极限定理，即$\hat{s}_n$收敛到以s为均值，以$Var[f(x)]/n$为方差的正态分布。而马尔可夫链蒙特卡罗方法(MCMC)，就是用一种方式构建一个收敛到目标分布的估计序列。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/01/03/monte-carlo-simulation/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2017/12/13/Install-tensorflow-with-gpu-library-CUDA-on-Ubuntu-16-04-x64/">
                            Install tensorflow with gpu library CUDA on Ubuntu 16.04 x64
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2017-12-13T14:33:31+08:00">
	
		    Dec 13, 2017
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-excerpt" itemprop="articleBody">
                    <p>System and Software info<br>
                    
                        <a href="/2017/12/13/Install-tensorflow-with-gpu-library-CUDA-on-Ubuntu-16-04-x64/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2017/09/14/Tensorflow-namespace-and-network-restoration/">
                            Tensorflow namespace and network restoration
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2017-09-14T16:20:37+08:00">
	
		    Sep 14, 2017
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-excerpt" itemprop="articleBody">
                    <p>昨天讨论了基于TensorFlow的迁移学习，提到了网络的存储和恢复的问题，然而我并没有说清楚，而且网络的恢复要考虑的问题其实挺多的。。。<br>
                    
                        <a href="/2017/09/14/Tensorflow-namespace-and-network-restoration/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2017/09/13/Transfer-learning-with-TensorFlow/">
                            Transfer learning with TensorFlow
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2017-09-13T16:07:45+08:00">
	
		    Sep 13, 2017
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-excerpt" itemprop="articleBody">
                    <p>Today, let&#39;s talk about transfer learning based on TensorFlow. Firstly, what is transfer learning? It is a strategy of building your own deeplearning based project with existing well-trained networks, so as to avoid some risks like overfitting, time-consuming and etc.<br>
                    
                        <a href="/2017/09/13/Transfer-learning-with-TensorFlow/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2017/09/11/Data-visualization-with-t-SNE/">
                            Data visualization with t-SNE
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2017-09-11T14:21:55+08:00">
	
		    Sep 11, 2017
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-excerpt" itemprop="articleBody">
                    <p>今天我们讨论数据可视化(data visualization)算法t-SNE (t-distributed Stochastic Neighbor Embedding)，该方法的目的是映射高维数据向量到低维，并保留向量的相似性或者距离。通常我们描述距离会使用类似欧式距离或黎曼距离等概念，映射的方法也多为线性，如PCA。而t-SNE不同，它是用联合概率来描述样本点的相似程度，是非线性的。<br>
                    
                        <a href="/2017/09/11/Data-visualization-with-t-SNE/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2017/09/04/Two-typical-tensorflow-exceptions/">
                            Two typical tensorflow exceptions
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2017-09-04T20:32:22+08:00">
	
		    Sep 04, 2017
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-excerpt" itemprop="articleBody">
                    <p>几天没写东西了，想写自己的故事，又太矫情。今天的故事不长，主要是程序中碰到的两个Bug，记录一下解决方法。这两天在做CAE的pretrain和fine-tuning，白天给老板解释了一遍，感觉他听懂了。。。<br>
                    
                        <a href="/2017/09/04/Two-typical-tensorflow-exceptions/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2017/09/01/Deconvolution-or-Transpose-opposite-operation-of-CNN/">
                            Deconvolution or Transpose: opposite operation of CNN
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2017-09-01T15:26:45+08:00">
	
		    Sep 01, 2017
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-excerpt" itemprop="articleBody">
                    <p>写之前吐个槽，因为昨晚(2017-08-29)开着阳台门睡觉，今天妥妥感冒了。也因为好久没生病，估计要触底反弹了(手动捂脸)。言归正传，最近在用AutoEncoder (AE) 做样本的预训练和特征表示学习，有一些体会, 今天想聊聊反向卷积神经网络。先挖个坑，等明天把程序写出来可能会写得更有调理一些。[Update: 烧了两天，回来填坑。。。]<br>
                    
                        <a href="/2017/09/01/Deconvolution-or-Transpose-opposite-operation-of-CNN/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
        
        <li class="pagination-number">page 1 of 1</li>
    </ul>
</div>

</section>


                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Jason Ma. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Jason Ma</h4>
        
            <div id="about-card-bio"><p>We are in the same story.</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Astronomer? Software engineer</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Shanghai
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-peofhqjkzcghmndknakluequy1y6owxdwpaqyju9ntl9zxnk7rdolb3rjjoj.min.js"></script>
<!--SCRIPTS END--><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



    </body>
</html>
