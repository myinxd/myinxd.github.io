
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jason&#39;s blog">
    <title>Tag: deep-learning - Jason&#39;s blog</title>
    <meta name="author" content="Jason Ma">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <meta name="description" content="Valar morghulis, valar dohaeris.">
<meta property="og:type" content="blog">
<meta property="og:title" content="Jason&#39;s blog">
<meta property="og:url" content="http://www.mazhixian.me/all-tags/deep-learning/index.html">
<meta property="og:site_name" content="Jason&#39;s blog">
<meta property="og:description" content="Valar morghulis, valar dohaeris.">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jason&#39;s blog">
<meta name="twitter:description" content="Valar morghulis, valar dohaeris.">
    
    
        
    
    
        <meta property="og:image" content="http://www.mazhixian.me/assets/images/profile.jpg"/>
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-pz4cc6y13wt2trzqa8l3n9v0yykr0sstdaheem7qj628nhjmhp9pfawvqawz.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-112822605-1', 'auto');
        ga('send', 'pageview');
    </script>


    
    <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?20f3bcc8683b066ff79f4e36134e3982";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="1">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Jason&#39;s blog</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
            <img class="header-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="1">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="/#about">
                    <img class="sidebar-profile-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Jason Ma</h4>
                
                    <h5 class="sidebar-profile-bio"><p>We are in the same story.</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="Categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="Search"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Search</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/myinxd" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fa fa-lg fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:zx@mazhixian.me" target="_blank" rel="noopener" title="Mail">
                    
                        <i class="sidebar-button-icon fa fa-lg fa-envelope-o" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/404.html"
                            
                            title="Naive"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-hourglass-start" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Naive</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="RSS"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="1"
                 class="
                        hasCoverMetaIn
                        ">
                
    <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/15/recurrent-neural-network-and-lstm/">
                            Recurrent neural network and LSTM
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-15T09:56:04+08:00">
	
		    Mar 15, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>简单记录一下循环神经网络 (recurrent neural network, RNN)， 另一种RNN，主要关注时间序列的预测、分类和识别等问题。这里卖个瓜，前面有讨论过残差神经网络，感兴趣地可以去围观，链接见文末。<br>本文首先讨论RNN的motivation及其特点；然后是为了解决长程依赖 (long-term dependencies) 而提出的long short term memory (LSTM) 结构； 最后是二者在tensorflow中的简单样例。参考文献很多，这里强烈案例下面两篇文章！</p>
<ul>
<li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM</a></li>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
</ul>
<h4 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a>Recurrent neural network</h4><p>循环神经网络的动机是<em>刻画一个时间序列当前输入与此前信息的联系</em>，从网络结构上，循环神经网络通过称为<strong>循环体</strong>的模块 (如下图) 实现对信息的记忆，即该层在<script type="math/tex">t-1</script>时刻的输出状态<script type="math/tex">\mathbf{h_{t-1}}</script>会被记录，并作为<script type="math/tex">t</script>时刻该模块输入的一部分，以级联的形式与<script type="math/tex">\mathbf{x_t}</script>构成此刻的输入 <script type="math/tex">[\mathbf{h_{t-1}}, \mathbf{x_{t}}]</script>.</p>
<p>显然，循环体中的循环理论上是无穷的，但在实际应用中会限制循环的次数以避免<strong>梯度消失 (gradient vanishing) </strong>的问题，用<code>num_step</code>来定义，即循环体的基本模块被复制并展开为<code>num_step</code>个。如<br>文献[4]所述，循环体结构是RNN的基础，在RNN中对于复制展开的循环体，其参数是共享的。这一点与卷积神经网络中的权值共享有类似之处。在<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">这篇</a>文章里给出了非常多的RNN的应用场景，我很喜欢里面关于手写体识别的问题，体现了权值共享的效果。</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/images/fig-rnn.png?raw=true" height="140" width="500">
</center>

<p>设<script type="math/tex">t</script>时刻循环体的输入为<script type="math/tex">\mathbf{x}(t)</script>，$t-1$时刻循环体的输出状态为<script type="math/tex">\mathbf{h(t)}</script>，则RNN中<script type="math/tex">t</script>时刻的输出<script type="math/tex">\mathbf{h}(t)</script>为，</p>
<script type="math/tex; mode=display">
\begin{equation}
\mathbf{h}(t) = \rm{tanh}\left(W\cdot[\mathbf{h}(t-1),\mathbf{x}(t)] + b\right).
\end{equation}</script><p>其中<script type="math/tex">W</script>是权值矩阵，其shape为<script type="math/tex">[\mathrm{len}(\mathbf{h}) + \mathrm{len}(\mathbf{x}), \mathrm{len}(\mathbf{h})]</script>, <script type="math/tex">b</script>为偏置。这里采用的激活函数是tanh，将数据限制到<script type="math/tex">[-1,1]</script>之间。</p>
<p>那么，为什么用tanh，而不是有high reputation的ReLU? 知乎的<a href="https://www.zhihu.com/question/61265076/answer/186644426" target="_blank" rel="external">这个讨论</a>给出了很棒的解释，参考Hinton<a href="https://arxiv.org/abs/1504.00941" target="_blank" rel="external">论文</a>中的观点 <em>ReLUs seem inappropriate for RNNs because they can have very large outputs so they might be expected to be far more likely to explode than units thathave bounded values.</em> ReLU将输出的值限制在<script type="math/tex">[0, \infty)</script>之间，而RNN中循环体之间的权值是共享的，经过公式(1)的多次作用，相当于对<script type="math/tex">W</script>做了连乘，ReLU函数会导致梯度爆炸的问题。因此，采用tanh可以将每层的输出空控制在限定的范围内，既避免了梯度消失，也避免了梯度爆炸的问题。</p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>在时间序列的预测中存在长期依赖 (long-term dependencies) 的问题，即网络需要记住离时间<script type="math/tex">t</script>很远的某个时刻的信息，固定的<code>num_step</code>将不适用于这一情形，并且长时间间隔下的梯度消失问题将无法处理。因此，需要对RNN的循环体模块进行修改，即长短时记忆网络 (long short term memory, LSTM). </p>
<p>LSTM的基本模块如下图所示，参考<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM</a>的解释，<em>The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt.</em> 即LSTM的核心是组成为<code>cell state</code>，用于存储和记忆上文的信息，类似传送带的功能。</p>
<p>而<code>cell state</code>的更新，通过三个逻辑门以及<script type="math/tex">\mathbf{x_t}</script>和<script type="math/tex">\mathbf{h_{t-1}}</script>共同完成。它们分别称为 (1) forget gate, (2) input gate, (3) output gate. 采用sigmoid函数将数值规范化到<script type="math/tex">[0,1]</script>区间，并与待处理信号进行点乘，本质上实现软判决.</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/images/fig-lstm.png?raw=true" height="400" width="640">
</center>

<p>与RNN类似，首先将<script type="math/tex">t-1</script>时刻LSTM cell的输出<script type="math/tex">\mathbf{h}_{t-1}</script>与<script type="math/tex">t</script>时刻的输入<script type="math/tex">\mathbf{x}_{t}</script>进行级联，逐一通过三个门，</p>
<ul>
<li>遗忘门 (Forget gate)<script type="math/tex; mode=display">
\begin{equation}
  \mathbf{f_t} = \sigma\left(W_f \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_f\right) 
\end{equation}</script></li>
<li>输入门 (Input gate)<script type="math/tex; mode=display">
\begin{equation}
\mathbf{i_t} = \sigma\left(W_i \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_i \right).
\end{equation}</script></li>
<li>输出门 (Output gate)<script type="math/tex; mode=display">
\begin{equation}
\mathbf{o_t} = \sigma\left(W_o \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_o \right).
\end{equation}</script></li>
</ul>
<p>其中遗忘门的作用是<strong>抛弃cell state中不需要的信息</strong>，与<script type="math/tex">\mathbf{C_{t-1}}</script>作用； 输入门则是<strong>决定cell state中待更新的信息</strong>，与<script type="math/tex">\mathbf{\tilde{C}_t}</script>，即state candidates作用；输出门则从更新后的<code>cell state</code>中<strong>决定输出的状态</strong>。结合以上三个门结构，便可以更新<code>cell state</code>以及<code>cell output</code>,</p>
<ul>
<li>cell state update<script type="math/tex; mode=display">
\begin{align}
\mathbf{\tilde{C_{t}}} &= \rm{tanh}\left(W_c \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_c \right) \\ 
\mathbf{C_t} &= \mathbf{f_t}\cdot\mathbf{C_{t-1}} + \mathbf{i_t} \cdot \mathbf{\tilde{C_{t}}}
\end{align}</script></li>
<li>cell output upadte<script type="math/tex; mode=display">
\begin{equation}
\mathbf{h_{t}} = \rm{tanh}(\mathbf{C_{t}}) \cdot \mathbf{o_t}
\end{equation}</script></li>
</ul>
<h4 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h4><p>参考《TensorFlow实战》第7章的LSTM基于<a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz" target="_blank" rel="external">PTB数据集</a>的语言预测样例，以及TensorFlow的<a href="https://www.tensorflow.org/tutorials/recurrent" target="_blank" rel="external">Tutorial</a>，设计了一个小实验，对比RNN和LSTM的performance，以及激活函数对于RNN的影响。(详细的notebooks见这里: <a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook-rnn-tanh.ipynb" target="_blank" rel="external">RNN</a>, <a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook-lstm.ipynb" target="_blank" rel="external">LSTM</a>)</p>
<p>这里给出<code>tf.contrib.rnn</code>中提供的用于搭建RNN和LSTM cell的类的实例化方法，以及如何构建多个Recurrent层，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># RNN</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell</span><span class="params">(num_units, activation, reuse=None)</span>:</span></div><div class="line">   <span class="keyword">return</span> tf.contrib.rnn.BasicLSTMCell(</div><div class="line">       num_units=num_units,  </div><div class="line">       activation=activation,</div><div class="line">       reuse=reuse)</div><div class="line"></div><div class="line"><span class="comment"># LSTM</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell</span><span class="params">(num_units, forget_bias=<span class="number">0.0</span>, state_in_tuple=True, reuse=None)</span>:</span></div><div class="line">	<span class="keyword">return</span> tf.contrib.rnn.BasicLSTMCell(</div><div class="line">    	num_units=size, </div><div class="line">        forget_bias=forget_bias, </div><div class="line">        state_is_tuple=state_in_tuple,</div><div class="line">        reuse=reuse)</div><div class="line"></div><div class="line"><span class="comment"># Multiple layers</span></div><div class="line">attn_cell = rnn_cell</div><div class="line">numlayers = <span class="number">2</span></div><div class="line">cell = tf.contrib.rnn.MultiRNNCell(</div><div class="line">	[attn_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(numlayers)],</div><div class="line">    state_is_tuple=<span class="keyword">True</span>)</div></pre></td></tr></table></figure></p>
<p>另外，在PTB的TF教程里，设置了可变的学习率以及梯度的clipping用于抑制梯度爆炸 (gradient explosion) 的问题，代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Adjustable learning rate</span></div><div class="line">new_lr = tf.placeholder(tf.float32, shape=[], name=<span class="string">"new_learning_rate"</span>)</div><div class="line">lr_update = tf.assign(self._lr, self._new_lr) <span class="comment"># use tf.assign to transfer the updated lr</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">assign_lr</span><span class="params">(session, lr_value)</span>:</span></div><div class="line">	sess.run(self._lr_update, feed_dict=&#123;new_lr: lr_value&#125;)</div><div class="line"></div><div class="line"><span class="comment"># Gradient clipping</span></div><div class="line">...</div><div class="line">max_grad_norm = <span class="number">5.0</span> <span class="comment"># maximum gradient</span></div><div class="line">tvars = tf.trainable_variables()  <span class="comment"># Get all trainable variables</span></div><div class="line">grads, _ = tf.clip_by_global_norm(</div><div class="line">	tf.gradients(cost, tvars), max_grad_norm)</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(self._lr)</div><div class="line">train_op = optimizer.apply_gradients(</div><div class="line">    zip(grads, tvars),</div><div class="line">    global_step = tf.contrib.framework.get_or_create_global_step())</div></pre></td></tr></table></figure></p>
<p>下面来比较一下RNN和LSTM的效果。比较的指标是perplexity (复杂度)，用于刻画该模型能够估计出某一句话的概率，其数值越小，模型表现越好。</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/images/fig-rnn-lstm-cmp.png?raw=true" height="300" width="500">
</center>

<p>容易看出，LSTM的表现是优于RNN的。除此之外，采用tanh函数的RNN要显著好于采用ReLU，在训练中也出现了<em>RuntimeWarning: overflow encountered in exp</em>的警告，说明出现了gradient explosion的问题。最后，我尝试增加了RNN的层数，但是效果并没有变好，也许是参数多了？也有可能是我偷懒了，没多训测试几次。。。</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM</a><br>[2] <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a><br>[3] <a href="https://www.tensorflow.org/tutorials/recurrent" target="_blank" rel="external">Tensorflow tutorial</a><br>[4] TensorFlow实战Google深度学习框架<br>[5] TensorFlow实战<br>[6] <a href="https://www.zhihu.com/question/61265076/answer/186644426" target="_blank" rel="external">RNN中为什么要采用tanh而不是ReLu作为激活函数？</a></p>
<h3 id="广告位"><a href="#广告位" class="headerlink" title="广告位"></a>广告位</h3><p><a href="http://www.mazhixian.me/2018/01/21/resnet-with-tensorflow/">Residual network I — block and bottleneck</a><br><a href="http://www.mazhixian.me/2018/01/27/resnet-with-tensorflow-2/">Residual network II — realize with tensorflow</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/15/recurrent-neural-network-and-lstm/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/02/04/train-selected-variables-in-tensorflow-graph/">
                            Train selected variables in tensorflow graph
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-02-04T02:02:44+08:00">
	
		    Feb 04, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>写之前吐个槽，我又把<code>tf.nn.softmax_cross_entropy_with_logits</code>的参数赋反了，折腾了一晚上。。。这篇文章主要讨论TensorFlow中训练指定变量的问题。这篇<a href="http://blog.csdn.net/shwan_ma/article/details/78881961" target="_blank" rel="external">博客</a>给了个非常巧妙的方法，简单记录一下。</p>
<h4 id="1-查看可训练的参数及其index"><a href="#1-查看可训练的参数及其index" class="headerlink" title="1. 查看可训练的参数及其index"></a>1. 查看可训练的参数及其index</h4><p>在<code>tf.trainiable_variables</code>里存储了可以用于训练的变量，利用如下方法可以打印出它们的信息，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">variables_names = [v.name <span class="keyword">for</span> v <span class="keyword">in</span> tf.trainable_variables()]</div><div class="line">values = sess.run(variables_names)</div><div class="line">i = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> zip(variables_names, values):</div><div class="line">    print(i, <span class="string">"Variable: "</span>, k)</div><div class="line">    print(<span class="string">"Shape: "</span>, v.shape)</div><div class="line">    i += <span class="number">1</span></div></pre></td></tr></table></figure></p>
<h4 id="2-建立train-options，并为其提供不同的trainable-lists"><a href="#2-建立train-options，并为其提供不同的trainable-lists" class="headerlink" title="2. 建立train options，并为其提供不同的trainable lists"></a>2. 建立train options，并为其提供不同的trainable lists</h4><p>假设有两个loss function，分别对应网络中不同区域的变量，为了实现梯度的有效传递，可以利用如下方法，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">loss1 = ...</div><div class="line">loss2 = ...</div><div class="line">var_list1 = tf.trainable_variables()[<span class="number">0</span>:<span class="number">10</span>]</div><div class="line">var_list2 = tf.trainable_variables()[<span class="number">10</span>:]</div><div class="line">train_op1 = tf.train.AdamOptimizer(learning_rate).minimize(loss1, var_list=var_list1)</div><div class="line">train_op2 = tf.train.AdamOptimizer(learning_rate).minimize(lose2, var_list=var_list2)</div></pre></td></tr></table></figure></p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><p>[1] <a href="http://blog.csdn.net/shwan_ma/article/details/78881961" target="_blank" rel="external">[tensorflow] 在不同层上设置不同的学习率，fine-tuning</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/02/04/train-selected-variables-in-tensorflow-graph/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/01/31/transpose-convolution-by-tensorflow/">
                            Transpose convolution by tensorflow--odd kernel shape
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-01-31T10:07:44+08:00">
	
		    Jan 31, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>The auto-encoder has been applied widely for unsupervised learning, which is usually composed of two symmetric parts namely encoder and decoder. It is easy to realize an autoencoder only with fully-connected layers, i.e., DNN, but which is not that clear in CNN. </p>
<p>For convolution case, the layer in the decoder maintains the shape and kernel configurations for its symmetric layer in the encoder, thus the deconvolution, or <a href="deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic">transpose convolution</a> operation will be used instead of the convolution operation.</p>
<p>TensorFlow provides a method namedly <code>conv2d_transpose</code> in both <code>tf.nn</code> module and <code>tf.contrib.layers</code> module, which are very convenient. However, for <code>tf.contrib.layers.conv2d_transpose</code>, if the output shape of the transpose convolutution is odd when convolution stride setting as 2, it cannot control the output shape to desired one. </p>
<p>For example, denote a [None, 9, 9, 1] 4D-tensor $X$, convolved by a kernel of size [3, 3] with a 2 step stride and halp padding (SAME), the output 4D tensor $y$ will be [None, 5, 5, 1]. However, the transpose convolution from y by the same parameters setting generates $x’$ into a [None, 10, 10, 1] tensor, not [None, 9, 9, 1].  </p>
<p>To handle this, I provide a naive but effective way, see as follows,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> tensorflow.contrib.layers <span class="keyword">as</span> layers</div><div class="line"></div><div class="line">x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>])</div><div class="line">y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">1</span>])</div><div class="line">kernel_size = [<span class="number">3</span>, <span class="number">3</span>]</div><div class="line">stride = <span class="number">2</span></div><div class="line"></div><div class="line">x_r = layers.conv2d_transpose(</div><div class="line">        inputs=x,</div><div class="line">        num_outputs=x.get_shape().as_list()[<span class="number">1</span>],</div><div class="line">        kernel_size=kenerl_size,</div><div class="line">        padding=<span class="string">'SAME'</span>,</div><div class="line">        stride=stride,</div><div class="line">        scope=<span class="string">'conv2d_transpose'</span></div><div class="line">        )</div><div class="line"></div><div class="line">x_r = x_r[:, <span class="number">0</span>:<span class="number">-1</span>, <span class="number">0</span>:<span class="number">-1</span>, :]</div></pre></td></tr></table></figure>
<p>Above solution played well in my code, though ths crop may introduce bias..</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/01/31/transpose-convolution-by-tensorflow/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/01/27/upsampling-for-2D-convolution-by-tensorflow/">
                            Upsampling for 2D convolution by tensorflow
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-01-27T16:56:22+08:00">
	
		    Jan 27, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>A convolutional auto-encoder is usually composed of two sysmmetric parts, i.e., the encoder and decoder. By TensorFlow, it is easy to build the encoder part using modules like <a href="https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/layers" target="_blank" rel="external">tf.contrib.layers</a> or <a href="https://www.tensorflow.org/api_docs/python/tf/nn" target="_blank" rel="external">tf.nn</a>, which encapsulate methods for convolution, downsampling, and dense operations. </p>
<p>However, as for the decoder part, TF does not provide method like <strong>upsampling</strong>, which is the reverse operation of downsampling (<code>avg_pool2, max_pool2</code>). This is because <strong>max pooling</strong> is applied more frequently than <strong>average pooling</strong>, while recover an image from max-pooled matrix is difficult for lossing of locations of the max points. </p>
<p>For the average-pooled feature maps, there is a simple way to realize upsampling without high-level API like <a href="https://keras.io" target="_blank" rel="external">keras</a>, but with basic functions of TF itself.</p>
<p>Now, suppose the input is a 4-D tenser whose shape is <code>[1, 4, 4, 1]</code> and sampling rate is <code>[1, 2, 2, 1]</code>, then the upsampled matrix is also a 4-D tenser of shape <code>[1, 8, 8, 1]</code>. Following lines can realize this operation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">x = tf.ones([<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">1</span>])</div><div class="line">k = tf.ones([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]) <span class="comment"># note k.shape = [rows, cols, depth_in, depth_output]</span></div><div class="line">output_shape=[<span class="number">1</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">1</span>]</div><div class="line">y = tf.nn.conv2d_transpose(</div><div class="line">    value=x,</div><div class="line">    filter=k,</div><div class="line">    output_shape=output_shape,</div><div class="line">    strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">    padding=<span class="string">'SAME'</span></div><div class="line">        )</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    print(sess.run(y))</div></pre></td></tr></table></figure>
<p>Then, y is the upsampled matrix.</p>
<p>You may also realize upsampling by the <code>resize_images</code> function of module<code>tf.image</code>, which is,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">y = tf.image.resize_images(</div><div class="line">    images=x,</div><div class="line">    size=[<span class="number">1</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">1</span>],</div><div class="line">    method=ResizeMethod.NEAREST_NEIGHBOR</div><div class="line">        )</div></pre></td></tr></table></figure></p>
<p>Enjoy yourself.</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] <a href="http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic" target="_blank" rel="external">Transposed convolution arithmetic</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/01/27/upsampling-for-2D-convolution-by-tensorflow/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/01/27/resnet-with-tensorflow-2/">
                            Residual network II -- realization by tensorflow
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-01-27T09:51:45+08:00">
	
		    Jan 27, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>来填ResNet的坑，residual network的原理已经在<a href="http://www.mazhixian.me/2018/01/21/resnet-with-tensorflow/">上一篇</a>里做了介绍，这一篇来讨论如何用<a href="https://www.tensorflow.org" target="_blank" rel="external">TensorFlow</a>实现。</p>
<p>虽然TF提供了slim这个库，可以很方便地搭建网络，但考虑到移植和扩展性，还是决定用<code>tf.contrib.layers</code>的函数和tf基本的函数来写。我们知道，ResNet的核心模块是<strong>Bottleneck</strong>，如下图所示，每个bottleneck的输入会通过两条路径在输出汇聚，计算残差，作为下一层的输入。</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180127/fig_resnet.png?raw=true" height="150" width="450">
</center>

<p>多个botleneck组合成一个block，通常会在每个block的最后一个bottleneck进行降采样，以缩小特征图大小。</p>
<p>具体的实现可以参考我的<a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook_resnet13_digit_classification.ipynb" target="_blank" rel="external">notebook</a>, 下面贴一个在手写体识别样本上的测试结果，对比了<a href="http://www.mazhixian.me/2018/01/25/how-to-apply-the-batch-normalized-net/">这篇文章</a>里讨论的DNN网络。</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180127/fig_bn_res_cmp.png?raw=true" height="280" width="400">
</center>

<p>可以看出ResNet的效果还是非常显著的。但是得强调一下，由于网络显著加深，训练时占用的显存资源非常大，普通的GPU非常吃力。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/01/27/resnet-with-tensorflow-2/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/01/25/how-to-apply-the-batch-normalized-net/">
                            How to apply the batch-normalized net
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-01-25T10:24:59+08:00">
	
		    Jan 25, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>继续填<a href="http://www.mazhixian.me/2018/01/23/batch-normalization-with-tensorflow/">这篇文章</a>的坑，如何测试和应用包含了Batch Normalization层的网络？ 在训练过程中，每个BN层直接从输入样本中求取<code>mean</code>和<code>variance</code>量，不是通过学习获取的固定值。因此，在测试网络时，需要人工提供这两个值。</p>
<p>在BN的文章里的处理方法是，对所有参与训练的<strong>mini-batch</strong>的均值和方差进行收集，采用无偏估计的方式估计总体样本的均值和方差，来表征测试样本的均值和方差，其公式如下，</p>
<script type="math/tex; mode=display">
\begin{align}
E[x] &= E[\mu_B], \notag \\
\mathrm{Var}[x] &= \frac{m}{m-1} \cdot E[{\sigma_B}^2], \notag
\end{align}</script><p>进而，BN layer的输出定义为，</p>
<script type="math/tex; mode=display">
y = \frac{\gamma}{\sqrt{\mathrm{Var}[x]+\epsilon}}\cdot x + (\beta - \frac{\gamma E[x]}{\sqrt{\mathrm{Var}[x]+\epsilon}}).</script><p>那么有如下几个问题需要解决，</p>
<ol>
<li>训练和测试过程中如何给BN传递<code>mean</code>和<code>variance</code>？即如何在计算图上体现这一运算？</li>
<li>如何动态收集每个mini-batch的mean和variance，用于总体样本的无偏估计moving_mean, moving_variance</li>
</ol>
<p>针对以上问题，TensorFlow的解决思路是设定<code>is_training</code>这个flag，如果为真，则每个mini-batch都会计算均值和方差，训练网络; 如果为假，则进入测试流程。</p>
<h4 id="基于tf-nn-batch-normalization的底层实现"><a href="#基于tf-nn-batch-normalization的底层实现" class="headerlink" title="基于tf.nn.batch_normalization的底层实现"></a>基于tf.nn.batch_normalization的底层实现</h4><p>TF提供了<code>tf.nn.batch_normalization</code>函数从底层搭建网络，其直接参考了Ioeff\&amp;Szegdy的论文，这里需要利用<code>tf.nn.moments</code>求取mini-batch的均值和方差，详细的实现代码参考<a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook-batch-norm-tf.ipynb" target="_blank" rel="external">这里</a>.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'BatchNorm'</span>):  </div><div class="line">	axis = list(range(len(x.get_shape()) - <span class="number">1</span>))</div><div class="line">    mean,var = tf.nn.moments(x_h, axis)</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'gamma'</span>):</div><div class="line">        gamma = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=mean.get_shape()))</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'beta'</span>):</div><div class="line">        beta = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=mean.get_shape()))</div><div class="line">    y = tf.nn.batch_normalization(</div><div class="line">           x = x_h,</div><div class="line">           mean = mean,</div><div class="line">           variance = var,</div><div class="line">           offset = beta,</div><div class="line">           scale = gamma,</div><div class="line">           variance_epsilon = <span class="number">1e-5</span>,</div><div class="line">           name= <span class="string">'BN'</span>)</div></pre></td></tr></table></figure></p>
<h4 id="基于tf-contrib-layers-batch-norm的实现"><a href="#基于tf-contrib-layers-batch-norm的实现" class="headerlink" title="基于tf.contrib.layers.batch_norm的实现"></a>基于tf.contrib.layers.batch_norm的实现</h4><p>在<a href="https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/layers/batch_normalization" target="_blank" rel="external">tf.contrib.layers</a>提供了<code>batch_norm</code>方法，该方法是对<code>tf.nn.batch_normalization</code>的封装，增加了如<code>center</code>，<code>is_training</code>等变量，并对BN的基础算法做了更新，用滑动平均来实现均值和房车的估计。</p>
<p>那么，如何实现包含BN层的网络的训练和测试？ 其核心是<strong>利用is_training作为flag控制输入给BN的mean和variance的来源，以及如何将moving_mean和moving_variance</strong>加入网络的训练过程中。</p>
<p>TF官方的建议方法解释是，<br><em>Note: when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_op. For example:</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</div><div class="line">  <span class="keyword">with</span> tf.control_dependencies(update_ops):</div><div class="line">    train_op = optimizer.minimize(loss)</div></pre></td></tr></table></figure></p>
<p>参考<a href="(http://ruishu.io/2016/12/27/batchnorm/">这篇博客</a>，作者对此做了更棒的解释！！！！！<br><em>When you execute an operation (such as train_step), only the subgraph components relevant to train_step will be executed. Unfortunately, the update_moving_averages operation is not a parent of train_step in the computational graph, so we will never update the moving averages!</em></p>
<p>作者的解决方法：<em>Personally, I think it makes more sense to attach the update ops to the train_step itself. So I modified the code a little and created the following training function</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</div><div class="line">    <span class="keyword">with</span> tf.control_dependencies(update_ops):</div><div class="line">        <span class="comment"># Ensures that we execute the update_ops before performing the train_step</span></div><div class="line">        train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(loss)</div><div class="line">    sess = tf.Session()</div><div class="line">    sess.run(tf.global_variables_initializer())</div></pre></td></tr></table></figure></p>
<p>以上代码在<code>tf.slim.batch_norm</code>中也有体现，<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/slim" target="_blank" rel="external">slim</a>是对tf的一个更高层的封装，利用slim实现的ResNet-v2-152可以参考<a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook_resnet.ipynb" target="_blank" rel="external">这里</a>。</p>
<p>最后，贴上基于<code>tf.contrib.layers.batch_norm</code>的实现样例，更详细的实现见我的<a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook-bn-test.ipynb" target="_blank" rel="external">notebook</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> tensorflow.contrib.layers <span class="keyword">as</span> layers</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'BatchNorm'</span>):  </div><div class="line">	y = layers.batch_norm(</div><div class="line">    	x_h,</div><div class="line">        center=<span class="keyword">True</span>,</div><div class="line">        scale=<span class="keyword">True</span>,</div><div class="line">        is_training=is_training)</div><div class="line">        </div><div class="line"><span class="comment"># Train step </span></div><div class="line"><span class="comment"># note: should add update_ops to the train graph</span></div><div class="line">update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</div><div class="line"><span class="keyword">with</span> tf.control_dependencies(update_ops):</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</div><div class="line">        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)</div></pre></td></tr></table></figure></p>
<h4 id="MLP是否采用BN的结果对比"><a href="#MLP是否采用BN的结果对比" class="headerlink" title="MLP是否采用BN的结果对比"></a>MLP是否采用BN的结果对比</h4><p>最后，贴一个是否采用BN层的结果对比，效果还是比较显著的。但是我也发现由于我设置的网络层数和FC长度都比较可观，随着Epochs增大，BN的优势并没有那么明显了。。。</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180125/fig_bn_cmp.png?raw=true" height="360" width="480">
</center>

<p>Enjoy it !! 我终于把这个问题看懂了，开心</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] <a href="http://proceedings.mlr.press/v37/ioffe15.html" target="_blank" rel="external">Ioffe, S. and Szegedy, C., 2015, June. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (pp. 448-456).</a><br>[2] <a href="http://blog.csdn.net/jiruiyang/article/details/77202674" target="_blank" rel="external">tensorflow 中batch normalize 的使用</a><br>[3] <a href="https://github.com/tensorflow/tensorflow/issues/7469" target="_blank" rel="external">docs: batch normalization usage in slim #7469</a><br>[4] <a href="https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/layers/batch_normalization" target="_blank" rel="external">tf.layers.batch_normalization</a><br>[5] <a href="http://ruishu.io/2016/12/27/batchnorm/" target="_blank" rel="external">TENSORFLOW GUIDE: BATCH NORMALIZATION</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/01/25/how-to-apply-the-batch-normalized-net/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/01/24/tensorflow-graph-and-tensorboard/">
                            TensorFlow graph and TensorBoard
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-01-24T14:55:13+08:00">
	
		    Jan 24, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>继续挖坑，因为Batch normalization的inference还没有解决，不知道怎么在tf.Tensor中更新数值，参考<a href="https://www.tensorflow.org/api_docs/python/tf/layers/BatchNormalization" target="_blank" rel="external">tf.layer.BatchNormalization</a>的思路，需要利用<code>is_training</code>来选择feed给BN的<code>mean</code>和<code>variance</code>。因此，需要理解TensorFlow的<strong>graph</strong>概念。除此之外，TF提供了<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">TensorBoard</a>作为计算图可视化的工具，也是值得借鉴的。</p>
<h3 id="计算图—Graph"><a href="#计算图—Graph" class="headerlink" title="计算图—Graph"></a>计算图—Graph</h3><p>TensorFlow是一个通过计算图的形式来表达张量之间通过计算相互转化的过程[1]。作为TensoFlow的基本概念，TF中的所有计算都会转化为计算图上的节点，节点之间的边用于描述计算之间的依赖关系。TF中不同的计算图之间维护的节点和边是独立的。通过tf.GraphKeys可以对图进行维护，稍后会介绍。</p>
<p>利用<code>tf.Graph</code>类实例化计算图，有如下的样例，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">g1 = tf.Graph()</div><div class="line"><span class="keyword">with</span> g1.as_default():</div><div class="line">	v = tf.get_variable&#123;<span class="string">"v"</span>, initializer=tf.zeros_initializer(shape=[<span class="number">1</span>])&#125;</div></pre></td></tr></table></figure></p>
<h3 id="TensorBoard-可视化"><a href="#TensorBoard-可视化" class="headerlink" title="TensorBoard 可视化"></a>TensorBoard 可视化</h3><p>TensorBoard可以有效地展示TF在运行过程中的计算图、各种指标随时间的变化趋势以及训练中使用的图像等信息，其利用TF运行过程中输出的<strong>日志文件</strong>可视化程序的运行状态。TensorBoard和TensorFlow运行在不同的进程中，TB会自动读取TF的日志文件，并呈现当前的运行状态。根据TensorBoard的<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">官方文档</a>，其实现过程有如下步骤，</p>
<h4 id="1-搭建网络"><a href="#1-搭建网络" class="headerlink" title="1. 搭建网络"></a>1. 搭建网络</h4><p><em>First, create the TensorFlow graph that you’d like to collect summary data from, and decide which nodes you would like to annotate with summary operations.</em></p>
<p>利用TensorFlow搭建网络，例如基于MLP的手写体识别网络，提供一个<a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook_deep_mnist_for_experts_tf.ipynb" target="_blank" rel="external">样例</a>，可以参考。</p>
<h4 id="2-Select-variables-to-be-summarized"><a href="#2-Select-variables-to-be-summarized" class="headerlink" title="2. Select variables to be summarized"></a>2. Select variables to be summarized</h4><p>通常我们关注训练过程中待学习的参数 (如weights, biases等) 的变化和收敛情况，通过<code>tf.summary.scalar</code>和<code>tf.summary.histogram</code>可以很方便地收集这些数据。下面给出一个样例，参考了[3]，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span><span class="params">(var)</span>:</span></div><div class="line">  <span class="string">"""Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""</span></div><div class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">'summaries'</span>):</div><div class="line">    mean = tf.reduce_mean(var)</div><div class="line">    tf.summary.scalar(<span class="string">'mean'</span>, mean)</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'stddev'</span>):</div><div class="line">      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</div><div class="line">    tf.summary.scalar(<span class="string">'stddev'</span>, stddev)</div><div class="line">    tf.summary.scalar(<span class="string">'max'</span>, tf.reduce_max(var))</div><div class="line">    tf.summary.scalar(<span class="string">'min'</span>, tf.reduce_min(var))</div><div class="line">    tf.summary.histogram(<span class="string">'histogram'</span>, var)</div></pre></td></tr></table></figure></p>
<p>而对于单个<code>Variable</code>，可以用<code>tf.summary.scalar(name,variable)</code>的形式进行收集。</p>
<h4 id="3-Merge-summary-data"><a href="#3-Merge-summary-data" class="headerlink" title="3. Merge summary data"></a>3. Merge summary data</h4><p>定义好要收集的信息，需要对他们进行汇总，此时用<code>tf.summary.merge_all</code>进行汇总，例如<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">merged = tf.summary.merge_all()</div></pre></td></tr></table></figure></p>
<h4 id="4-FileWriter-for-saving-the-log"><a href="#4-FileWriter-for-saving-the-log" class="headerlink" title="4. FileWriter for saving the log"></a>4. FileWriter for saving the log</h4><p>为了将训练中收集的数据保存到日志中，可以用<code>tf.summary.FileWriter</code>来实现，此时需要提供存放日志的文件夹位置，即<code>logdir</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">logdir = <span class="string">'./log'</span></div><div class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(logdir):</div><div class="line">    os.mkdir(logdir)</div><div class="line">train_writer = tf.summary.FileWriter(logdir + <span class="string">'/train'</span>,</div><div class="line">                                      sess.graph)</div><div class="line">test_writer = tf.summary.FileWriter(logdir + <span class="string">'/test'</span>)</div></pre></td></tr></table></figure></p>
<p>此时，利用<code>sess.run(tf.global_variables_initializer())</code>初始化所有Tensor和Variable,便可以开始训练网络。</p>
<h4 id="5-Running"><a href="#5-Running" class="headerlink" title="5. Running"></a>5. Running</h4><p><code>merged</code>的类型为<code>tensorflow.python.framework.ops.Tensor</code>，属于tf.Graph的一部分。因此也需要在<code>session</code>中运行，并且要提供<code>feed_dict</code>。相应的python实现如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">summary, _ = sess.run(</div><div class="line">                [merged, train_step], </div><div class="line">                feed_dict=feed_dict)</div><div class="line">train_writer.add_summary(summary)</div></pre></td></tr></table></figure></p>
<h4 id="6-利用TensorBoard可视化"><a href="#6-利用TensorBoard可视化" class="headerlink" title="6. 利用TensorBoard可视化"></a>6. 利用TensorBoard可视化</h4><p>TensorFlow提供了<code>tensorboard.py</code>脚本用于可视化。利用如下指令启动并运行TensorBoard, 其默认端口为6006，这里利用<code>--port</code>显式表达</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ tensorboard --logdir=<span class="string">'./log --port=6006'</span></div></pre></td></tr></table></figure>
<p>注：在执行此条指令的时候，可能会报错<em>command not found: tensorboard</em>，解决方法参考<a href="http://blog.csdn.net/uestc_c2_403/article/details/73457368" target="_blank" rel="external">本文</a></p>
<p>以下给出运行的样例，我写了一个<a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook-tensorboard-mlp.ipynb" target="_blank" rel="external">notebook</a>，欢迎来PR :)</p>
<ul>
<li>Graph及FC1和train节点</li>
</ul>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180124/fig_graph.png?raw=true" height="360" width="450">
</center>

<ul>
<li>Scalars and histograms</li>
</ul>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180124/fig_scalars.png?raw=true" height="450" width="800">
</center>


<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] <a href="https://www.amazon.cn/dp/B06WGP12TV/" target="_blank" rel="external">TensorFlow实战Google深度学习框架</a><br>[2] <a href="https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter" target="_blank" rel="external">tf.summary.FileWriter</a><br>[3] <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">TensorBoard: Visualizing Learning</a><br>[3] <a href="http://blog.csdn.net/uestc_c2_403/article/details/73457368" target="_blank" rel="external">tensorboard在linux下的启动问题</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/01/24/tensorflow-graph-and-tensorboard/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/01/21/resnet-with-tensorflow/">
                            Residual network I -- block and bottleneck
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-01-21T14:15:00+08:00">
	
		    Jan 21, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>ResNet (Residual Neural Network)由何凯明等在15年提出，这里做个笔记，参考了<a href="https://www.amazon.cn/dp/B06X8Z4BS9/" target="_blank" rel="external">TensorFlow 实战</a>的6.4节。ResNet的结构能加速超深神经网络的训练，能有效避免过拟合，而且推广性非常好。</p>
<p>深度神经网络存在<code>Degradation</code>的问题，即随着网络层数的增加，其准确率会从逐渐提升到饱和，进而下降。这一问题在我们近期的工作也出现了，我们在增加了一层全连接后，网络的准确率反而下降了。为了解决这一问题，ResNet提出了一种新的网络思路，<strong>即允许原始输入信息直接传输到后面的层中</strong>。假定某段网络的输入是<script type="math/tex">\mathrm{x}</script>，期望输出是<script type="math/tex">H(\mathrm{x})</script>，如果直接将<script type="math/tex">\mathrm{x}</script>传到输出作为初始结果，那么目标函数变为</p>
<script type="math/tex; mode=display">
\begin{equation}
F(\mathrm{x}) = H(\mathrm{x}) - \mathrm{x}
\end{equation}</script><p>如下图所示为一个ResNet的残差学习单元，即将学习目标从<script type="math/tex">H(\mathrm{x})</script>改为期望与输入的残差。这种结构也被称为<code>shortcut</code>或<code>skip connections</code>.</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180119/fig3.png?raw=true" height="200" width="300">
</center>

<p>考虑到神经网络相当于对原始数据进行了压缩，存在信息损失的问题，而ResNet结构将部分输入信息直接传递到输出端，可以保留部分的信息。并且ResNet的训练误差会随着层数增加而逐渐减少，并且在测试集上也有很好的表现。</p>
<p>引自TensorFlow实战，在ResNet的第二篇论文<em>Identity mapping in deep residual networks</em>中，提出了ResNet V2。想对于ResNet V1, 将激活函数<code>ReLU</code>改为了<code>Identity mapping</code>，即y=x。同时，ResNet V2在每一层都使用了Batch Normalization，提升网络的范化能力。</p>
<h3 id="ResNet的block的理解"><a href="#ResNet的block的理解" class="headerlink" title="ResNet的block的理解"></a>ResNet的block的理解</h3><p>在ResNet的实现中，包含多个<code>block</code>，每个<code>block</code>又由多个<code>bottleneck</code>组成，称为<strong>残差学习单元</strong>，ResNet的残差运算即在每个bottleneck内实现。因为要获取输入<script type="math/tex">\mathrm{x}</script>与block输出的残差，每个<code>bottleneck</code>虽然做了多次卷积和激活运算，但其输出的<code>shape</code>应该保持不变，因此内部的卷积运算的<code>stride</code>和<code>padding</code>都有特殊的设置 (数学推导可以参考<a href="http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html" target="_blank" rel="external">此文</a>)。以ResNet-50的第二个block为例，假设该block内包含两个bottleneck单元，则有，</p>
<h5 id="Bottleneck-One"><a href="#Bottleneck-One" class="headerlink" title="Bottleneck One"></a>Bottleneck One</h5><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Layer</th>
<th style="text-align:center">Input</th>
<th style="text-align:center">Kernel</th>
<th style="text-align:center">Stride</th>
<th style="text-align:center">Padding</th>
<th style="text-align:center">Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Conv1,1</td>
<td style="text-align:center">56x56x64</td>
<td style="text-align:center">1x1x64</td>
<td style="text-align:center">1</td>
<td style="text-align:center">SAME</td>
<td style="text-align:center">56x56x64</td>
</tr>
<tr>
<td style="text-align:center">Conv1,2</td>
<td style="text-align:center">56x56x64</td>
<td style="text-align:center">3x3x64</td>
<td style="text-align:center">1</td>
<td style="text-align:center">SAME</td>
<td style="text-align:center">56x56x64</td>
</tr>
<tr>
<td style="text-align:center">Conv1,3</td>
<td style="text-align:center">56x56x64</td>
<td style="text-align:center">1x1x256</td>
<td style="text-align:center">1</td>
<td style="text-align:center">SAME</td>
<td style="text-align:center">56x56x256</td>
</tr>
</tbody>
</table>
</div>
<h5 id="Bottleneck-Two"><a href="#Bottleneck-Two" class="headerlink" title="Bottleneck Two"></a>Bottleneck Two</h5><div class="table-container">
<table>
<thead>
<tr>
<th>Layer</th>
<th style="text-align:center">Input</th>
<th style="text-align:center">Kernel</th>
<th style="text-align:center">Stride</th>
<th style="text-align:center">Padding</th>
<th style="text-align:center">Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>Conv2,1</td>
<td style="text-align:center">56x56x256</td>
<td style="text-align:center">1x1x64</td>
<td style="text-align:center">1</td>
<td style="text-align:center">SAME</td>
<td style="text-align:center">56x56x64</td>
</tr>
<tr>
<td>Conv2,2</td>
<td style="text-align:center">56x56x64</td>
<td style="text-align:center">3x3x64</td>
<td style="text-align:center"><em>2</em></td>
<td style="text-align:center">SAME</td>
<td style="text-align:center">28x28x64</td>
</tr>
<tr>
<td>Conv2,3</td>
<td style="text-align:center">28x28x64</td>
<td style="text-align:center">1x1x256</td>
<td style="text-align:center">1</td>
<td style="text-align:center">SAME</td>
<td style="text-align:center"><em>28x28x256</em></td>
</tr>
</tbody>
</table>
</div>
<p>注意<code>Bottleneck Two</code>的<code>Conv2,3</code>层的<code>stride</code>为2,因此输出的尺度大小相当于做了factor为2的降采样，因此该bottleneck不做残差的运算，其输出直接作为下一个block的输入。</p>
<p>ResNet-V2相对于ResNet-V1增加了<code>Batch Normalization</code>和<code>L2</code>正则，如何在block中体现？ 且听下回分解。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/01/21/resnet-with-tensorflow/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/01/19/adjustable-learning-rate-for-deep-learning-by-tensorflow/">
                            Adjustable learning rate for deep learning by TensorFlow
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-01-19T09:11:34+08:00">
	
		    Jan 19, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>趁着最近空闲一点，继续纠结TensorFlow学习率调整的问题。学习率对网路训练的影响还是挺大的，初始化时可以采用大的学习率，帮助网络快速收敛。但网络的梯度是非线性的，随着迭代次数的增加，梯度的导数趋于变小，如果继续保持大学习率，会出现优化目标在最优解附近波动的情况，如下图所示。此时，为了接近收敛点，需要调整学习率。</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180119/fig1.png?raw=true" height="200" width="360">
</center>

<p>参考<a href="https://www.amazon.cn/dp/B06WGP12TV/ref=sr_1_1?ie=UTF8&amp;qid=1516326593&amp;sr=8-1&amp;keywords=Tensorflow" target="_blank" rel="external">TensorFlow: 实战Google深度学习框架</a>这本书，利用<strong>指数衰减</strong>的方法设置<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="external">梯度下降算法</a>的学习率，TensofFLow中集成了这一算法，即<code>tf.train.exponential_decay</code>。其实现了</p>
<script type="math/tex; mode=display">
L^{t+1}_{R} = L^{t}_{R} \cdot R_{d}^{S_g/S_d},</script><p>其中$L_R$表示学习率，$R_d$表示衰减率，$S_g$和$S_d$分别表示总Epochs和每个Epochs中的Batches。从而没迭代一轮，学习率便更新一次。</p>
<p>在TensorFlow中可以用如下代码实现指数衰减的学习率更新。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">init_lr = tf.Variable(<span class="number">0.</span>, name=<span class="string">"LR"</span>)</div><div class="line">global_step = tf.Variable(<span class="number">0.</span>, name=<span class="string">"global_step"</span>)</div><div class="line">decay_step = tf.Variable(<span class="number">0.</span>, name=<span class="string">"decay_step"</span>)</div><div class="line">decay_rate = tf.Variable(<span class="number">0.</span>, name=<span class="string">"decay_rate"</span>)</div><div class="line">learning_rate = tf.train.exponential_decay(</div><div class="line">    learning_rate = init_lr ,</div><div class="line">    global_step = global_step,</div><div class="line">    decay_steps = decay_step,</div><div class="line">    decay_rate = decay_rate,</div><div class="line">    staircase=<span class="keyword">False</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">	)</div><div class="line"><span class="comment"># Test</span></div><div class="line">lr_init = <span class="number">0.1</span></div><div class="line">epochs = <span class="number">200</span></div><div class="line">batches = <span class="number">100.</span></div><div class="line">d_rate = <span class="number">0.9</span></div><div class="line">epoch = np.arange(<span class="number">0</span>,epochs,<span class="number">1</span>)</div><div class="line">lr = np.zeros(epoch.shape)</div><div class="line"><span class="comment"># Init a session</span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line">sess = tf.InteractiveSession()</div><div class="line">sess.run(init_op)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> epoch.astype(int):</div><div class="line">	lr[i] = sess.run(learning_rate, </div><div class="line">                     feed_dict=&#123;init_lr: lr_init,</div><div class="line">                                global_step: i,</div><div class="line">                                decay_step: batches,</div><div class="line">                                decay_rate: d_rate</div><div class="line">                                &#125;)</div></pre></td></tr></table></figure></p>
<p><a href="https://github.com/myinxd/canal-images/blob/master/images/blog-180119/Learning_rate_adjust_demo.ipynb" target="_blank" rel="external">这里</a>提供了一个样例，其输出结果如下图，通过设置<code>tf.exponential_decay</code>的参数<code>staircase</code>可以控制learning rate是否为阶梯型或者平滑的。</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180119/fig2.png?raw=true" height="360" width="480">
</center>

<p>而在训练网络，优化目标函数的过程中，可以参考官方手册，用如下语句进行梯度更新。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</div><div class="line">starter_learning_rate = <span class="number">0.1</span></div><div class="line">learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,</div><div class="line">                                           <span class="number">100000</span>, <span class="number">0.96</span>, staircase=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># Passing global_step to minimize() will increment it at each step.</span></div><div class="line">learning_step = (</div><div class="line">    tf.train.GradientDescentOptimizer(learning_rate)</div><div class="line">    .minimize(...my loss..., global_step=global_step)</div><div class="line">)</div></pre></td></tr></table></figure></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] <a href="https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay" target="_blank" rel="external">exponential decay</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/01/19/adjustable-learning-rate-for-deep-learning-by-tensorflow/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/01/03/monte-carlo-simulation/">
                            monte carlo simulation
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-01-03T20:24:28+08:00">
	
		    Jan 03, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <h2 id="To-be-continue…"><a href="#To-be-continue…" class="headerlink" title="To be continue…"></a>To be continue…</h2><p>蒙特卡罗方法帮助产生服从样本分布的新样本，并且实现参数的估计，是一种基于贝叶斯统计的算法。机器学习中的许多重要工具都基于从某种分布中采样，以及用这些样本对目标量做一个蒙特卡罗估计。</p>
<h3 id="Why-sampling-and-Monte-Carlo"><a href="#Why-sampling-and-Monte-Carlo" class="headerlink" title="Why sampling and Monte Carlo?"></a>Why sampling and Monte Carlo?</h3><p>当无法精确计算和或积分时，通常可以使用蒙特卡罗采样来近似。这种想法把和或积分视作某分布下的期望，然后通过估计对应的平均值来近似这个期望。而这里的估计用到了中心极限定理，即$\hat{s}_n$收敛到以s为均值，以$Var[f(x)]/n$为方差的正态分布。而马尔可夫链蒙特卡罗方法(MCMC)，就是用一种方式构建一个收敛到目标分布的估计序列。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/01/03/monte-carlo-simulation/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
        
          <li class="pagination-next">
            <a class="btn btn--default btn--small" href="/all-tags/deep-learning/page/2/">
              <span>OLDER POSTS</span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
        <li class="pagination-number">page 1 of 2</li>
    </ul>
</div>

</section>


                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Jason Ma. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Jason Ma</h4>
        
            <div id="about-card-bio"><p>We are in the same story.</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Astronomer? Software engineer</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Shanghai
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-peofhqjkzcghmndknakluequy1y6owxdwpaqyju9ntl9zxnk7rdolb3rjjoj.min.js"></script>
<!--SCRIPTS END--><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



    </body>
</html>
