
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Jason&#39;s blog">
    <title>Archives - Jason&#39;s blog</title>
    <meta name="author" content="Jason Ma">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <meta name="description" content="Valar morghulis, valar dohaeris.">
<meta property="og:type" content="blog">
<meta property="og:title" content="Jason&#39;s blog">
<meta property="og:url" content="http://www.mazhixian.me/all-archives/index.html">
<meta property="og:site_name" content="Jason&#39;s blog">
<meta property="og:description" content="Valar morghulis, valar dohaeris.">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jason&#39;s blog">
<meta name="twitter:description" content="Valar morghulis, valar dohaeris.">
    
    
        
    
    
        <meta property="og:image" content="http://www.mazhixian.me/assets/images/profile.jpg"/>
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-pz4cc6y13wt2trzqa8l3n9v0yykr0sstdaheem7qj628nhjmhp9pfawvqawz.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-112822605-1', 'auto');
        ga('send', 'pageview');
    </script>


    
    <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?20f3bcc8683b066ff79f4e36134e3982";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="1">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Jason&#39;s blog</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
            <img class="header-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="1">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="/#about">
                    <img class="sidebar-profile-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Jason Ma</h4>
                
                    <h5 class="sidebar-profile-bio"><p>We are in the same story.</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="Categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="Search"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Search</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/myinxd" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fa fa-lg fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:zx@mazhixian.me" target="_blank" rel="noopener" title="Mail">
                    
                        <i class="sidebar-button-icon fa fa-lg fa-envelope-o" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/404.html"
                            
                            title="Naive"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-hourglass-start" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Naive</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="RSS"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="1"
                 class="
                        hasCoverMetaIn
                        ">
                
    <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/04/15/variational-autoencoder-and-the-conditional-case/">
                            Variational autoencoder and the conditional case
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-04-15T09:05:32+08:00">
	
		    Apr 15, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>今天讨论一下变分自动编码器 (Variational Auto-Encoder, VAE)及添加了条件约束的conditional VAE，包括其与传统AE的区别、推导思路和基于TensorFlow的实现。</p>
<p>传统的自动编码器主要用于特征提取、降维和压缩，是一种无监督的机器学习工具。其由编码器 (encoder)和解码器 (decoder)组成。编码器类似于分类问题中的特征提取模块，对样本进行特征提取并压缩成一个长度很小的向量；解码器则对这一特征向量进行解码以恢复出原始样本信息。通常衡量一个自动编码器好坏的指标 (或目标函数) 为最小均方误差 (mean squared error, MSE)，</p>
<script type="math/tex; mode=display">
\begin{equation}
    L_{MSE} = \frac{1}{N}\sum^{N}_{i=1}{(X_\mathrm{in} - X_\mathrm{out})^2},
\end{equation}</script><p>其中<script type="math/tex">X_\mathrm{in}</script> 和 <script type="math/tex">X_\mathrm{out}</script>表示编码器的输入和解码器的输出。</p>
<p>传统AE的问题在于，解码器只能根据编码器在样本上提取的特征向量来输出恢复的样本，很难直接通过生成特征向量产生新的样本 (当然特征是可以通过类似Monte Carlo模拟或者GMM等模型来生成的，感兴趣可以看我的这个<a href="https://github.com/myinxd/dnnae-gmm" target="_blank" rel="external">repo</a>) ，所以传统的AE不完全是生成模型。</p>
<p>为了解决这一问题，Kingma &amp; Welling 在<a href="https://arxiv.org/abs/1312.6114" target="_blank" rel="external"><em>Auto-Encoding Variational Bayes</em></a>文中提出了变分自动编码器的概念。虽然与AE的组成类似，但二者是完全不同的，<strong>VAE是基于Bayes理论的，利用编解码器来抽象参数的非线性，最大化解码器输出相对于输入样本的似然性。</strong> 简而言之，就是试图获取数据本身的分布，从这个分布产生新的样本。</p>
<center>
<img src="https://github.com/myinxd/conditional_vae/blob/master/images/fig1.png?raw=true" height="320" width="640">
</center>

<h3 id="Variational-auto-encoder"><a href="#Variational-auto-encoder" class="headerlink" title="Variational auto-encoder"></a>Variational auto-encoder</h3><p>VAE首先定义一个称为latent variable的高维随机变量<script type="math/tex">z</script> (z的维数远小于<script type="math/tex">X</script>)，其分布为<script type="math/tex">P(z)</script>。然后定义映射<script type="math/tex">f: z\times \theta -> X</script>，即由参数<script type="math/tex">\theta</script>约束的映射<script type="math/tex">f</script>在服从<script type="math/tex">P(z)</script>分布的<script type="math/tex">z</script>的作用下能够近似真实样本<script type="math/tex">X</script> (如公式2，其中<script type="math/tex">P(X|z;\theta)</script>表示<script type="math/tex">f(z|\theta)</script>的概率分布)。通过<script type="math/tex">z</script>便可以刻画样本<script type="math/tex">x</script>。而由于<script type="math/tex">z</script>是无法直接观测的，所以称为隐变量。</p>
<script type="math/tex; mode=display">
\begin{equation}
P(X) = \int{P(X|z;\theta)P(z)dz}.
\end{equation}</script><p>VAE假设<script type="math/tex">P(z)</script>~<script type="math/tex">N(0, I)</script>的标准正态分布，通过对参数<script type="math/tex">\theta</script>进行估计，最大化公式(2)。<strong>因此，VAE网络实际上是用来求解<script type="math/tex">P(X|z;\theta)</script>这个分布的，与传统意义上的AE的理解是不同的。</strong>正如Tutorial on Variational Autoencoder里提到的，<em>They are called “autoencoders” only because the final training objective that derives from this setup does have an encoder and a decoder, and resembles a traditional autoencoder.</em> </p>
<p>VAE的Encoder可以理解为<script type="math/tex">P(z|X)</script>，而Decoder理解为<script type="math/tex">P(X|z)</script>，其目标便是最大化Decoder的输出<script type="math/tex">P(X|z)</script>或者<script type="math/tex">log(P(X|z))</script>. 由于<script type="math/tex">z</script>是无法观测的，分布<script type="math/tex">P(z|X)</script>无法直接求解，VAE巧妙地利用了变分的方法，定义分布<script type="math/tex">Q(z|X)</script>来近似<script type="math/tex">P(z|X)</script>。最终有<script type="math/tex">Q(z|X)</script>为Encoder，而<script type="math/tex">E_{z\sim Q}{P(X|z)}</script>表示Decoder。</p>
<p>通常用KL divergence来描述两个分布之间的相似程度，KL散度越接近于零，二者越相似。定义<script type="math/tex">\mathrm{KL}(Q(z)||P(z|X))</script>,</p>
<script type="math/tex; mode=display">
\begin{equation}
\mathrm{KL}(Q(z|X)||P(z|X)) = E_{z\sim Q}\left({Q(z|X)\log{\frac{Q(z)}{P(z|X)}}}\right)
\end{equation}</script><p>利用Bayes定理，<script type="math/tex">P(z|X) = P(X|z)P(z)/P(x)</script>，推导得到 (具体过程参考文献2)，</p>
<script type="math/tex; mode=display">
\begin{equation}
\log{P(X)} - \mathrm{KL}(Q(z|X)||P(z|X)) = E_{z \sim Q}\log{P(X|z)} - \mathrm{KL}(Q(z|X)||P(z))
\end{equation}</script><p>因为有<script type="math/tex">\mathrm{KL}(Q(z|X)||P(z|X))\leq 0</script>，所以上式定义了<script type="math/tex">logP(X)</script>的下界，即</p>
<script type="math/tex; mode=display">
\begin{equation}
\log{P(X)} \leq E_{z\sim Q}\log{P(X|z)} - \mathrm{KL}(Q(z|X)||P(z))
\end{equation}</script><p>显然，公式(3)的右边两项分别对应VAE中Decoder和Encoder的损失函数，也即要最大化的目标，这与GAN网络的思路非常类似。<script type="math/tex">E_{z\sim Q}logP(X|z)</script> 通常可以用最小均方误差定义，也可以用交叉熵来描述。这里详细推导一下<script type="math/tex">\mathrm{KL}(Q(z|X)||P(z))</script>，以帮助其在TensorFlow下的实现。这里假设<script type="math/tex">Q(z|X)</script>服从正态分布<script type="math/tex">N(\mu_Q, \sigma_Q\times I)</script>, <script type="math/tex">P(z)</script>是服从标准正态分布的。</p>
<script type="math/tex; mode=display">
\begin{align}
\mathrm{KL}\left(Q(z|X)||P(z)\right) &= \int{Q(z|X)\log{\frac{Q(z|X)}{P(z)}}dz} \notag \\
&= \int{Q(z|X)\log{\frac{\frac{1}{\sqrt{2\pi}\sigma_Q}e^{-\frac{(X-\mu_Q)^2}{2\sigma^2_Q}}}{\frac{1}{\sqrt{2\pi}\sigma_P}e^{-\frac{(X-\mu_P)^2}{2\sigma^2_P}}}}}dz \notag \\
&= \int{Q(z|X)}\left(\log{\frac{\sigma_P}{\sigma_Q}}-\frac{(X-\mu_Q)^2}{2\sigma^2_Q} + \frac{(X-\mu_P)^2}{2\sigma^2_P} \right)dz \notag \\
&= \log{\sigma_P} - \log{\sigma_Q} - 0.5 + \int{Q(z|X)}\frac{(X-\mu_Q + \mu_Q - \mu_P)^2}{2\sigma^2_P}dz \notag \\
&= \log{\sigma_P} - \log{\sigma_Q} - 0.5 + \notag \\
& \frac{1}{2\sigma^2_P}\int{Q(z|X)}\left[(x-\mu_Q)^2 + (\mu_Q - \mu_P)^2 -2(x-\mu_Q)(\mu_Q-\mu_P)\right]dz \notag \\
& = \log{\sigma_P} - \log{\sigma_Q} - 0.5  + \frac{\sigma^2_Q}{2\sigma^2_P} + \frac{(\mu_Q - \mu_P)^2}{2\sigma^2_P} \notag \\
& = -0.5 - \log{\sigma_Q} + \frac{\sigma^2_Q}{2} + \frac{(\mu_Q - \mu_P)^2}{2} \\
\end{align}</script><h3 id="Conditional-Variational-auto-encoder"><a href="#Conditional-Variational-auto-encoder" class="headerlink" title="Conditional Variational auto-encoder"></a>Conditional Variational auto-encoder</h3><p>VAE只能生成服从样本分布的模拟，而对于MNIST这类样本有明确的标签的情形，能否通过设定某种条件，让VAE生成指定类别的手写体图像呢？基于这个motivation,人们提出了conditional VAE。假定样本的标签为<script type="math/tex">Y</script>，以<script type="math/tex">Y</script>作为Encoder和Decoder额外的约束 (如图1(c))，即可实现以上要求。相应的，条件变分自动编码器的似然函数变为,</p>
<script type="math/tex; mode=display">
\begin{equation}
\log{P(X|Y)} - \mathrm{KL}(Q(z|X,Y)||P(z|X,Y)) = E_{z\sim Q}\log P(X|z,Y) - \mathrm{KL}(Q(z|X,Y)||P(z)) \notag
\end{equation}</script><p>Conditional VAE的约束与VAE基本相同，依然可以分为针对Encoder和Decoder的两个部分。</p>
<h4 id="VAE和Conditional的TensorFlow实现"><a href="#VAE和Conditional的TensorFlow实现" class="headerlink" title="VAE和Conditional的TensorFlow实现"></a>VAE和Conditional的TensorFlow实现</h4><p>简单介绍一下VAE的TensorFlow实现，具体的Notebook参考<a href="https://github.com/myinxd/conditional_vae" target="_blank" rel="external">这里</a>。</p>
<ul>
<li><p>Init</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">batch_size = <span class="number">64</span></div><div class="line">X_in = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">'X_in'</span>)</div><div class="line">X_out = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">'X_out'</span>)</div><div class="line">keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name=<span class="string">'keep_prob'</span>)</div><div class="line">n_latent = <span class="number">32</span></div></pre></td></tr></table></figure>
</li>
<li><p>Encoder</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder</span><span class="params">(X_in, keep_prob)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>, reuse=<span class="keyword">None</span>):</div><div class="line">        x = tf.layers.dense(X_in, units=<span class="number">128</span>, activation=tf.nn.relu)</div><div class="line">        x = tf.nn.dropout(x, keep_prob)        </div><div class="line">        x = tf.layers.dense(x, units=<span class="number">64</span>, activation=tf.nn.relu)</div><div class="line">        x = tf.nn.dropout(x, keep_prob)        </div><div class="line">        <span class="comment"># The latent layer</span></div><div class="line">        mu = tf.layers.dense(x, units=n_latent)</div><div class="line">        sigma = <span class="number">1e-6</span> + tf.nn.softplus(tf.layers.dense(x, units=n_latent)) <span class="comment"># softplus to avoid negative sigma</span></div><div class="line">        <span class="comment"># reparameterization</span></div><div class="line">        epsilon = tf.random_normal(tf.shape(mu))</div><div class="line">        z  = mu + tf.multiply(epsilon, sigma)        </div><div class="line">        <span class="keyword">return</span> z, mu, sigma</div></pre></td></tr></table></figure>
</li>
<li><p>Decoder</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span><span class="params">(z, keep_prob)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"decoder"</span>, reuse=<span class="keyword">None</span>):</div><div class="line">        x = tf.layers.dense(z, units=<span class="number">64</span>, activation=tf.nn.relu)</div><div class="line">        x = tf.nn.dropout(x, keep_prob)</div><div class="line">        x = tf.layers.dense(x, units=<span class="number">128</span>, activation=tf.nn.relu)</div><div class="line">        x = tf.nn.dropout(x, keep_prob)</div><div class="line">        x = tf.layers.dense(x, units=<span class="number">784</span>, activation=tf.nn.sigmoid) </div><div class="line">        <span class="comment"># sigmoid to contrain the output to [0,1)</span></div><div class="line">        <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
</li>
<li><p>Network instance</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">z, mu, sigma = encoder(X_in, keep_prob)</div><div class="line">dec = decoder(z, keep_prob)</div></pre></td></tr></table></figure>
</li>
<li><p>Loss function</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">loss_d = - tf.reduce_sum(</div><div class="line">    X_out * tf.log(<span class="number">1e-8</span>+dec) + (<span class="number">1.0</span> - X_out) * tf.log(<span class="number">1e-8</span>+ <span class="number">1.0</span> - dec), <span class="number">1</span>)</div><div class="line">loss_e = <span class="number">0.5</span> * tf.reduce_sum(</div><div class="line">    tf.square(mu) + tf.square(sigma) - tf.log(<span class="number">1e-8</span> + tf.square(sigma)) - <span class="number">1</span>, <span class="number">1</span>)</div><div class="line"><span class="comment"># 注意这里的符号。。。</span></div><div class="line">loss = tf.reduce_mean(loss_d + loss_e)</div><div class="line">optimizer = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss)</div></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="E-z-sim-Q-log-P-X-z-的不同定义及结果对比"><a href="#E-z-sim-Q-log-P-X-z-的不同定义及结果对比" class="headerlink" title="E_{z\sim Q}log(P(X|z))的不同定义及结果对比"></a><script type="math/tex">E_{z\sim Q}log(P(X|z))</script>的不同定义及结果对比</h4><p>最后贴一个结果，我对比了一下采用交叉熵和均方误差两种损失函数表述<script type="math/tex">E_{z\sim Q}log(P(X|z))</script>的结果，在参数相同的情况下，看起来MSE效果好一些，如下图。</p>
<center>
<img src="https://github.com/myinxd/conditional_vae/blob/master/images/fig2.png?raw=true" height="300" width="600">
</center>


<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul>
<li><a href="https://arxiv.org/abs/1312.6114" target="_blank" rel="external">Auto-Encoding Variational Bayes</a></li>
<li><a href="https://arxiv.org/abs/1606.05908" target="_blank" rel="external">Tutorial on Variational Autoencoders</a></li>
<li><a href="https://blog.csdn.net/qq_39388410/article/details/79129197" target="_blank" rel="external">Variational Autoencoder（变分自编码）</a></li>
</ul>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/04/15/variational-autoencoder-and-the-conditional-case/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/04/14/generative-adversarial-network-and-its-conditional-case/">
                            Generative adversarial network and its conditional case
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-04-14T22:36:34+08:00">
	
		    Apr 14, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>本来想讨论InfoGAN的，先留个坑吧。今天先讨论标准的GAN，即生成对抗网络。GAN最先由Ian Goodfellow在<a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="external">2014</a>年提出，跟<a href="https://arxiv.org/abs/1312.6114" target="_blank" rel="external">Variational Auto-Encoder (VAE)</a>的时间差不多，二者都是非常好的生成网络，在无监督学习中发挥了重要的作用。</p>
<h3 id="Basic-theory"><a href="#Basic-theory" class="headerlink" title="Basic theory"></a>Basic theory</h3><p>生成对抗网络属于一个minmax game，其目标是<em>Learn a generator, whose distribution <script type="math/tex">P_G(x)</script> matches the real data distribution <script type="math/tex">P_\mathrm{data}(x)</script></em>，即实现一个生成器<script type="math/tex">G(x)</script>用于从随机分布 (一般为高斯) 的噪声<script type="math/tex">z</script>中生成与目标样本<script type="math/tex">x_\mathrm{data}</script>类似的模拟$x_g$。</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180415/fig1.png?raw=true" height="360" width="540">
</center>

<p>为了衡量<script type="math/tex">x_g</script>与<script type="math/tex">x_\mathrm{data}</script>的相似性，设计一个称为<strong>Discriminator</strong>的对抗网络<script type="math/tex">D(x)</script>，该网络的输入可以是真实的<script type="math/tex">x_\mathrm{data}</script>，也可以是有generator生成的伪造的样本<script type="math/tex">x_{g}</script>。我们要求<script type="math/tex">D(x)</script>能够分辨出输入给它的样本的真实性。因此discriminator的输出应该是样本真实的概率，</p>
<script type="math/tex; mode=display">
\begin{equation}
D(x) = \frac{P_\mathrm{data}(x)}{P_\mathrm{data}(x)+P_{G}(x)}
\end{equation}</script><p>可以看出，当P<em>{G}(x)与P</em>\mathrm{data}(x)接近时，$D(x) \sim 1/2$，即discriminator无法判断其输入来自真实样本还是伪造的样本。因此<script type="math/tex">G(x)</script>与<script type="math/tex">D(x)</script>构成了一对相互对抗的网络，即生成对抗。相应的目标函数为，</p>
<script type="math/tex; mode=display">
\begin{equation}
\min\limits_{G}\max\limits_{D} V(D,G) = 
\mathrm{E}_{x \sim P_\mathrm{data}[\log(D(x))]} + \mathrm{E}_{z}[1 - \log(D(G(x)))]
\end{equation}</script><p>参考Goodfellow的论文，该目标函数的优化求解分为两步，</p>
<h4 id="Step1"><a href="#Step1" class="headerlink" title="Step1"></a>Step1</h4><ul>
<li>随机生成minibatch个z noise作为generator的输入，得到对应的输出<script type="math/tex">x_g</script>；</li>
<li>随机选取minibatch个real data样本<script type="math/tex">x_\mathrm{data}</script>；</li>
<li>计算<script type="math/tex">L_D = \mathrm{E}_{x \sim P_\mathrm{data}[\log(D(x))]} + \mathrm{E}_{z}[1 - \log(D(G(x)))]</script>；</li>
<li>将<script type="math/tex">L_D</script>沿梯度方向传递给Discriminator的参数，进行参数学习。</li>
</ul>
<h4 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h4><ul>
<li>随机生成minibatch个z noise作为generator的输入，得到对应的输出<script type="math/tex">x_g</script>；</li>
<li>计算<script type="math/tex">L_G = \mathrm{E}_{z}[1 - \log(D(G(x)))]</script>；</li>
<li>将<script type="math/tex">L_G</script>沿梯度方向传递给Generator的参数，进行参数学习。</li>
</ul>
<p>交替重复以上两步，直到<script type="math/tex">L_D</script>和<script type="math/tex">L_G</script>收敛，即 <em>After several steps of training, if G and D have enough capacity, they will reach a point at which both cannot improve</em>.</p>
<h3 id="Conditional-case"><a href="#Conditional-case" class="headerlink" title="Conditional case"></a>Conditional case</h3><p>以上是最原始的GAN，是一种无监督的网络。那么，以MNIST手写体数据库为例，如果我们想得到一个能够生成特定数字的生成器，应该如何做？ 这一问题可以理解为GAN的有监督学习，即conditional GAN。这里插一句，InfoGAN可以在无标签的情况下通过将$z$分解为noise+latent两部分，无监督地学到具有样本的context semantic representations.绝对秒杀原始的GAN。。。</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180415/fig2.png?raw=true" height="300" width="450">
</center>

<p>条件GAN的思路是什么呢？如上图所示，从左往右分别是原始的GAN、条件GAN两个网络。conditional GAN在generator和discriminator的输入部分均添加了一个新的变量$y$，即样本的标签，作为一种固定的约束，指导网络学习到样本内部的区别。<strong>但是网络的目标函数和训练过程的变化很小。</strong> 这个思路，和conditional variational auto-encoder非常相似。更新后的目标函数如下所示，</p>
<script type="math/tex; mode=display">
\begin{equation}
\min\limits_{G}\max\limits_{D} V(D,G) = 
\mathrm{E}_{x \sim P_\mathrm{data}[\log(D(x|y))]} + \mathrm{E}_{z|y}[1 - \log(D(G(x|y)))]
\end{equation}</script><h3 id="GAN的TensorFlow实现"><a href="#GAN的TensorFlow实现" class="headerlink" title="GAN的TensorFlow实现"></a>GAN的TensorFlow实现</h3><p>首先吐个槽,GAN的调参真的不是一般的复杂，我发现Generator和Discriminator互搏的时候，经常训着训着，loss就偏了，最后的输出也很诡异。然后是激活函数的选择、dropout的keep_prob，以及z的长度，batch的大小都会有影响，我提供了<a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook-mnist-gan.ipynb" target="_blank" rel="external">GAN</a>和<a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook-mnist-gan-condtional.ipynb" target="_blank" rel="external">Conditional GAN</a>的notebook，感兴趣可以自己去试试看。。。下面我分步骤介绍一下Conditional GAN的实现。</p>
<ul>
<li><p>Initialization</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">batch_size = <span class="number">64</span></div><div class="line">z_len = <span class="number">100</span></div><div class="line">z_noise = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, z_len], name=<span class="string">'z_noise'</span>)</div><div class="line">y = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>], name=<span class="string">'y'</span>)</div><div class="line">x_data = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">'x_data'</span>)</div><div class="line">keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name=<span class="string">'keep_prob'</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>Generator</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(z_noise, y, keep_prob, namescope=<span class="string">'generator'</span>)</span>:</span></div><div class="line">    <span class="string">"""The generator"""</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(namescope):</div><div class="line">        net = tf.concat([z_noise, y], axis=<span class="number">1</span>)</div><div class="line">        net = tf.layers.dense(net, units=<span class="number">150</span>, activation=tf.nn.relu, name=<span class="string">'g_fc1'</span>)</div><div class="line">        net = tf.nn.dropout(net, keep_prob=keep_prob)</div><div class="line">        net = tf.layers.dense(net, units=<span class="number">300</span>, activation=tf.nn.relu, name=<span class="string">'g_fc2'</span>)</div><div class="line">        net = tf.nn.dropout(net, keep_prob=keep_prob)</div><div class="line">        net = tf.layers.dense(net, units=<span class="number">784</span>, activation=tf.nn.sigmoid, name=<span class="string">'g_fc3'</span>)</div><div class="line">    <span class="keyword">return</span> net</div></pre></td></tr></table></figure>
</li>
<li><p>Discriminator</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(d_in, y, z_len, keep_prob, namescope=<span class="string">'discriminator'</span>, reuse=True)</span>:</span></div><div class="line">    <span class="string">"""The discriminator"""</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(namescope):</div><div class="line">        net = tf.concat([d_in, y], axis=<span class="number">1</span>)</div><div class="line">        net = tf.layers.dense(net, units=<span class="number">300</span>, activation=tf.nn.relu, name=<span class="string">'d_fc1'</span>, reuse=reuse)</div><div class="line">        net = tf.nn.dropout(net, keep_prob=keep_prob)</div><div class="line">        net = tf.layers.dense(net, units=<span class="number">150</span>, activation=tf.nn.relu, name=<span class="string">'d_fc2'</span>, reuse=reuse)</div><div class="line">        net = tf.nn.dropout(net, keep_prob=keep_prob)</div><div class="line">        net = tf.layers.dense(net, units=<span class="number">1</span>, activation=tf.nn.sigmoid, name=<span class="string">'d_fc4'</span>, reuse=reuse)</div><div class="line">        <span class="keyword">return</span> net</div></pre></td></tr></table></figure>
</li>
<li><p>Network instance</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># generate the network</span></div><div class="line">x_g = generator(z_noise, y, keep_prob)</div><div class="line">d_g = discriminator(x_g, y, z_len, keep_prob, reuse=<span class="keyword">False</span>)</div><div class="line">d_data = discriminator(x_data, y, z_len, keep_prob)</div></pre></td></tr></table></figure>
</li>
<li><p>Loss and optimizer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># get variables</span></div><div class="line">varlist = tf.trainable_variables() <span class="comment"># 查看待训练的参数，为了获取G和D两个网络的参数列表</span></div><div class="line"><span class="comment"># The objective</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"loss"</span>):</div><div class="line">    loss_d = - (tf.reduce_mean(tf.log(<span class="number">1e-8</span> + d_data)) + tf.reduce_mean(tf.log(<span class="number">1e-8</span> + <span class="number">1</span> - d_g)))</div><div class="line">    loss_g = - tf.reduce_mean(tf.log(<span class="number">1e-8</span> + d_g))</div><div class="line">    train_op_g = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_g, var_list=varlist[<span class="number">0</span>:<span class="number">6</span>])</div><div class="line">    train_op_d = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_d, var_list=varlist[<span class="number">6</span>:])</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Conditional-GAN在MNIST上的测试结果"><a href="#Conditional-GAN在MNIST上的测试结果" class="headerlink" title="Conditional GAN在MNIST上的测试结果"></a>Conditional GAN在MNIST上的测试结果</h3><p>针对MNIST手写体数据库，实现了一个可以根据标签生成指定数字的Conditional GAN，网络的配置如下表，参考了这篇<a href="https://blog.csdn.net/sparkexpert/article/details/70147409" target="_blank" rel="external">博客</a>。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Subnet</th>
<th style="text-align:center">Layer</th>
<th style="text-align:center">Nodes</th>
<th style="text-align:center">Activation</th>
<th style="text-align:center">Dropout</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Generator</td>
<td style="text-align:center">input [z, y]</td>
<td style="text-align:center">32+10</td>
<td style="text-align:center">—-</td>
<td style="text-align:center">—-</td>
</tr>
<tr>
<td style="text-align:left">Generator</td>
<td style="text-align:center">FC</td>
<td style="text-align:center">150</td>
<td style="text-align:center">relu</td>
<td style="text-align:center">T</td>
</tr>
<tr>
<td style="text-align:left">Generator</td>
<td style="text-align:center">FC</td>
<td style="text-align:center">300</td>
<td style="text-align:center">relu</td>
<td style="text-align:center">T</td>
</tr>
<tr>
<td style="text-align:left">Generator</td>
<td style="text-align:center">FC</td>
<td style="text-align:center">784</td>
<td style="text-align:center">sigmoid</td>
<td style="text-align:center">F</td>
</tr>
<tr>
<td style="text-align:left">Discriminator</td>
<td style="text-align:center">input</td>
<td style="text-align:center">784+10</td>
<td style="text-align:center">—-</td>
<td style="text-align:center">—-</td>
</tr>
<tr>
<td style="text-align:left">Discriminator</td>
<td style="text-align:center">FC</td>
<td style="text-align:center">300</td>
<td style="text-align:center">relu</td>
<td style="text-align:center">T</td>
</tr>
<tr>
<td style="text-align:left">Discriminator</td>
<td style="text-align:center">FC</td>
<td style="text-align:center">150</td>
<td style="text-align:center">relu</td>
<td style="text-align:center">T</td>
</tr>
<tr>
<td style="text-align:left">Discriminator</td>
<td style="text-align:center">FC</td>
<td style="text-align:center">1</td>
<td style="text-align:center">sigmoid</td>
<td style="text-align:center">F</td>
</tr>
</tbody>
</table>
</div>
<p>下面贴一下实验结果 (生成的手写体图像，每行对应一个数字)，可以看出，随着迭代次数的增加，生成的数字越来约清晰，且准确性在提升。</p>
<center>
<img src="https://github.com/myinxd/canal-images/blob/master/images/blog-180415/fig3.png?raw=true" height="450" width="700">
</center>

<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul>
<li><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="external">Ian Goodfellow, <em>Generative Adversarial Network</em></a></li>
<li><a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="external">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a></li>
<li><a href="https://blog.csdn.net/sparkexpert/article/details/70147409" target="_blank" rel="external">tensorflow 1.01中GAN(生成对抗网络)手写字体生成例子(MINST)的测试</a></li>
<li><a href="https://blog.csdn.net/u012223913/article/details/75051516?locationNum=1&amp;fps=1" target="_blank" rel="external">【tensorflow学习】最简单的GAN 实现</a></li>
</ul>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/04/14/generative-adversarial-network-and-its-conditional-case/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/04/08/difference-between-tf-layers-dense-and-tf-contrib-layers-fully-connected/">
                            tf.layers.dense 和 tf.contrib.layers.fully_connected的区别
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-04-08T18:53:03+08:00">
	
		    Apr 08, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>TensorFlow的<code>tf.layers</code>和c<code>tf.contrib.layers</code>都提供了相关用于搭建神经网络的模块，但同一版本额和不同版本之间均存在区别，TF的更新真的是好快。。。</p>
<p>今天主要讨论一下<code>tf.layers.dense</code>和<code>tf.contrib.layers.fully_connected</code>的区别，二者都可以用于构建全连接层。参考tensorflow的文档，二者的参数如下，</p>
<ul>
<li><p>tf.layers.dense</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">tf.layers.dense(</div><div class="line">    inputs,</div><div class="line">    units,</div><div class="line">    activation=<span class="keyword">None</span>,</div><div class="line">    use_bias=<span class="keyword">True</span>,</div><div class="line">    kernel_initializer=<span class="keyword">None</span>,</div><div class="line">    bias_initializer=tf.zeros_initializer(),</div><div class="line">    kernel_regularizer=<span class="keyword">None</span>,</div><div class="line">    bias_regularizer=<span class="keyword">None</span>,</div><div class="line">    activity_regularizer=<span class="keyword">None</span>,</div><div class="line">    kernel_constraint=<span class="keyword">None</span>,</div><div class="line">    bias_constraint=<span class="keyword">None</span>,</div><div class="line">    trainable=<span class="keyword">True</span>,</div><div class="line">    name=<span class="keyword">None</span>,</div><div class="line">    reuse=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
</li>
<li><p>tf.contrib.layers.fully-connected</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">fully_connected(</div><div class="line">    inputs,</div><div class="line">    num_outputs,</div><div class="line">    activation_fn=tf.nn.relu,</div><div class="line">    normalizer_fn=<span class="keyword">None</span>,</div><div class="line">    normalizer_params=<span class="keyword">None</span>,</div><div class="line">    weights_initializer=initializers.xavier_initializer(),</div><div class="line">    weights_regularizer=<span class="keyword">None</span>,</div><div class="line">    biases_initializer=tf.zeros_initializer(),</div><div class="line">    biases_regularizer=<span class="keyword">None</span>,</div><div class="line">    reuse=<span class="keyword">None</span>,</div><div class="line">    variables_collections=<span class="keyword">None</span>,</div><div class="line">    outputs_collections=<span class="keyword">None</span>,</div><div class="line">    trainable=<span class="keyword">True</span>,</div><div class="line">    scope=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
</li>
</ul>
<p>可以看出，<code>tf.layers.dense</code>相对更简单，没有提供默认的<code>activation</code>和<code>kernel_initializer</code>, 而后者这两个参数都做了默认的初始化。使用时一定要显示说明这些，否则会出现不可控的错误。。。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/layers/dense" target="_blank" rel="external">tf.layers.dense</a></li>
<li><a href="https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/layers/fully_connected" target="_blank" rel="external">tf.contrib.layers.fully_connected</a></li>
</ul>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/04/08/difference-between-tf-layers-dense-and-tf-contrib-layers-fully-connected/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/26/range-based-for-loop-by-cpp/">
                            Range based for loop by cpp
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-26T22:03:19+08:00">
	
		    Mar 26, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>最近落了好多没写了，感觉已经废了。继续C++的笔记，关于<em>range-based for loop</em>的使用，一个C++11的标准。首先我会给出样例，然后针对gcc的报错，说明Ubuntu16.04-LTS下gcc的更新方法。</p>
<h4 id="range-based-for-loop"><a href="#range-based-for-loop" class="headerlink" title="range-based for loop"></a>range-based for loop</h4><p>C++11标准中增加了一种新的for循环的方式，称为range-based for loop，即基于范围的for循环，这个类似于python中直接进行列表索引的for循环。请看下面的样例，分别是C++和python的for循环写法，</p>
<h5 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="keyword">int</span> numArray[<span class="number">5</span>] = &#123;<span class="number">0</span>, <span class="number">11</span>, <span class="number">22</span>, <span class="number">33</span>, <span class="number">44</span>&#125;;</div><div class="line"></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> num : numArray)</div><div class="line">	    <span class="built_in">cout</span> &lt;&lt; num &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="python"><a href="#python" class="headerlink" title="python"></a>python</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">numArray = [<span class="number">0</span>, <span class="number">11</span>, <span class="number">22</span>, <span class="number">33</span>, <span class="number">44</span>]</div><div class="line"></div><div class="line"><span class="keyword">for</span> num <span class="keyword">in</span> numArray:</div><div class="line">    print(num)</div></pre></td></tr></table></figure>
<p>二者的输出结果均为如下形式，<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">0</div><div class="line">11</div><div class="line">22</div><div class="line">33</div><div class="line">44</div></pre></td></tr></table></figure></p>
<p>显然，相对于C99的for循环，基于范围的这种for循环能够减少多行代码量。</p>
<h4 id="g-update"><a href="#g-update" class="headerlink" title="g++ update"></a>g++ update</h4><p>为了应用C++11/14的新标准，<code>gcc/g++</code>也需要做相应的更新，由于我还在Ubuntu-16.04 LTS的坑里，gcc的最高版本为5.4。如果编译时看到如下的警告，则需要对gcc进行更新。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">warning: range-based ‘<span class="keyword">for</span>’ loops only available with -std=c++11 or -std=gnu++11</div></pre></td></tr></table></figure></p>
<p>更新方法如下，<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo add-apt-repository ppa:ubuntu-toolchain-r/<span class="built_in">test</span></div><div class="line">sudo apt update</div><div class="line">sudo apt install gcc-7 g++-7</div></pre></td></tr></table></figure></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://www.linuxidc.com/Linux/2016-11/136840.htm" target="_blank" rel="external">Ubuntu升级GCC版本</a></li>
<li>Sams Teach Yourself C++ in One Hour a Day</li>
</ul>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/26/range-based-for-loop-by-cpp/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/25/configure-permalinks-in-wordpress-with-nginx-and-avoid-404/">
                            Configure permalinks in wordpress with nginx and avoid 404
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-25T15:18:36+08:00">
	
		    Mar 25, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>简单记录一下wordpress配置静态链接后，页面无法重定向，网页404的解决方法。可参考的posts特别多，我罗列了一些在references里。我会从三个方面来说这个问题，(1) wordpress静态链接的配置； (2) 导致404的原因及解决方法； (3) 自定义链接插件</p>
<h4 id="wordpress静态链接的配置"><a href="#wordpress静态链接的配置" class="headerlink" title="wordpress静态链接的配置"></a>wordpress静态链接的配置</h4><p>Wordpress可供配置的静态链接很多</p>
<h4 id="自定义链接插件"><a href="#自定义链接插件" class="headerlink" title="自定义链接插件"></a>自定义链接插件</h4><p>custom permalinks plugin</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li><a href="https://blog.csdn.net/hanshileiai/article/details/45580457" target="_blank" rel="external">ubuntu 下 wordpress 设置 Permalink 为 自定义结构后出现404页面 nginx - 404 not found page for permalinks</a></li>
<li><a href="https://wordpress.org/plugins/custom-permalinks/" target="_blank" rel="external">Custom Permalinks plugin</a></li>
<li><a href="http://www.mazhixian.me/2018/01/23/multiple-websites-with-nginx/">multiple websites with nginx</a></li>
</ol>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/25/configure-permalinks-in-wordpress-with-nginx-and-avoid-404/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/17/confusion-matrix-and-generation-with-tensorflow/">
                            Confusion matrix and generation with TensorFlow
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-17T14:45:29+08:00">
	
		    Mar 17, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>简单记录一下多分类问题中的一种评价方法及其可视化，混淆矩阵 (confusion matrix, CM)。首先给出其定义及作用，然后给出样例。</p>
<h4 id="Confusion-matrix"><a href="#Confusion-matrix" class="headerlink" title="Confusion matrix"></a>Confusion matrix</h4><p>混淆矩阵通过比较分类器的<strong>预测</strong>和<strong>真实</strong>标签，评估分类器效果，通常用于多分类问题。TF的<a href="https://www.tensorflow.org/tutorials/audio_recognition" target="_blank" rel="external">audio recognition tutorial</a>里对混淆矩阵的评价是, <em>This matrix can be more useful than just a single accuracy score because it gives a good summary of what mistakes the network is making.</em> 即混淆矩阵可以帮助分析分类器在哪些类上的表现最差。相对于单一的准确率这种衡量指标，更加直观。</p>
<p>混淆矩阵是一个二维的方阵，横轴代表真实标签，枞轴代表预测标签，其中的元素<script type="math/tex">CM_{ij}</script>代表实际为第<script type="math/tex">i</script>，被分成第<script type="math/tex">j</script>类的样本数目。<strong>显然矩阵的非零元素集中在对角线时，分类器的表现更优异。</strong></p>
<h4 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h4><p>下面给出一个样例，这是一个六分类的问题，结果来自我们近期的工作。其中A的样本量约1000, B的样本量约10000，A random是随机产生预测标签的CM矩阵。如下所示，A的CM map中，多数样本集中在对角线；B的表现也不错，但(6,6)的色块显然淡了很多，说明分类器对第六类的分类效果不好；而A random，因为是随机生成的，其CM的各个元素的样本数比较均衡，因此分类准确率也非常差。</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/images/fig-confmat.png?raw=true" height="200" width="720">
</center>

<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] <a href="https://www.tensorflow.org/tutorials/audio_recognition" target="_blank" rel="external">Simple Audio Recognition</a><br>[2] <a href="https://www.tensorflow.org/api_docs/python/tf/confusion_matrix" target="_blank" rel="external">tf.confusion_matrix</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/17/confusion-matrix-and-generation-with-tensorflow/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/17/install-macOS-10-12-with-virtualbox/">
                            Install macOS 10.9 with virtualbox-5.1 on Unbuntu 1710
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-17T14:07:15+08:00">
	
		    Mar 17, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>首先，建议不要作死，还是去买mac吧，太折腾人了！！然后进入正题，如何在Ubuntu 17.10上利用virtualbox 5.1安装MacOS 10.9虚拟机。涉及CPUID问题、vdmk转vdi，vdi的resize，以及EFI的问题。好几点都不太懂，慢慢摸索吧。 </p>
<p>再吐个槽，不要随意自己更新virtualbox，因为如果是UEFI安装的ubuntu，是没法将Virtualbox的modprov部署进系统image的，然后virtualbox就彻底挂了。。。解决方法是升级系统或者重装。</p>
<h4 id="镜像去哪里找"><a href="#镜像去哪里找" class="headerlink" title="镜像去哪里找"></a>镜像去哪里找</h4><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>具体的配置参考文献里有很多，我就不赘述了。。。这里主要说明一下CPUID的问题。。。我的CPU是i3 7100，按道理说其架构不适合装黑苹果，不过可以通过修改虚拟机的cpuid来解决这个问题。。。网上的方法太坑爹了，固定了cpuid，其实这个要自己去查的，每一代intel架构不太一样。查询方法如下，<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">VBoxManage list hostcpuids</div><div class="line"><span class="comment"># It will print</span></div><div class="line">Host CPUIDs:</div><div class="line"></div><div class="line">Leaf no.  EAX      EBX      ECX      EDX</div><div class="line">00000000  00000016 756e6547 6c65746e 49656e69</div><div class="line">00000001  000906e9 00100800 7ffafbbf bfebfbff</div><div class="line">00000002  76036301 00f0b5ff 00000000 00c30000</div><div class="line">00000003  00000000 00000000 00000000 00000000</div><div class="line">00000004  1c004121 01c0003f 0000003f 00000000</div><div class="line"><span class="comment"># 找到00000001这一行，</span></div><div class="line">替换到xxx.vbox中</div></pre></td></tr></table></figure></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">VBoxManage modifyvm <span class="string">"macOS"</span> --cpuidset 00000001 000306a9 00100800 3d9ae3bf bfebfbff</div><div class="line">VBoxManage setextradata <span class="string">"macOS"</span> <span class="string">"VBoxInternal/Devices/efi/0/Config/DmiSystemProduct"</span> <span class="string">"iMac11,3"</span></div><div class="line">VBoxManage setextradata <span class="string">"macOS"</span> <span class="string">"VBoxInternal/Devices/efi/0/Config/DmiSystemVersion"</span> <span class="string">"1.0"</span></div><div class="line">VBoxManage setextradata <span class="string">"macOS"</span> <span class="string">"VBoxInternal/Devices/efi/0/Config/DmiBoardProduct"</span> <span class="string">"Iloveapple"</span></div><div class="line">VBoxManage setextradata <span class="string">"macOS"</span> <span class="string">"VBoxInternal/Devices/smc/0/Config/DeviceKey"</span> <span class="string">"ourhardworkbythesewordsguardedpleasedontsteal(c)AppleComputerInc"</span></div><div class="line">VBoxManage setextradata <span class="string">"macOS"</span> <span class="string">"VBoxInternal/Devices/smc/0/Config/GetKeyFromRealSMC"</span> 1</div></pre></td></tr></table></figure>
<p>更新以后如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&lt;ExtraDataItem name=&quot;VBoxInternal/Devices/efi/0/Config/DmiBoardProduct&quot; value=&quot;Iloveapple&quot;/&gt;</div><div class="line">&lt;ExtraDataItem name=&quot;VBoxInternal/Devices/efi/0/Config/DmiSystemProduct&quot; value=&quot;iMac11,3&quot;/&gt;</div><div class="line">&lt;ExtraDataItem name=&quot;VBoxInternal/Devices/efi/0/Config/DmiSystemVersion&quot; value=&quot;1.0&quot;/&gt;</div><div class="line">&lt;ExtraDataItem name=&quot;VBoxInternal/Devices/smc/0/Config/DeviceKey&quot; value=&quot;ourhardworkbythesewordsguardedpleasedontsteal(c)AppleComputerInc&quot;/&gt;</div><div class="line">&lt;ExtraDataItem name=&quot;VBoxInternal/Devices/smc/0/Config/GetKeyFromRealSMC&quot; value=&quot;1&quot;/&gt;</div><div class="line"></div><div class="line">&lt;CPU&gt;</div><div class="line">    &lt;PAE enabled=&quot;true&quot;/&gt;</div><div class="line">    &lt;LongMode enabled=&quot;true&quot;/&gt;</div><div class="line">    &lt;HardwareVirtExLargePages enabled=&quot;false&quot;/&gt;</div><div class="line">    &lt;CpuIdTree&gt;</div><div class="line">    &lt;CpuIdLeaf id=&quot;1&quot; eax=&quot;591593&quot; ebx=&quot;34605056&quot; ecx=&quot;2147154879&quot; edx=&quot;3219913727&quot;/&gt;</div><div class="line">    &lt;/CpuIdTree&gt;</div><div class="line">&lt;/CPU&gt;</div></pre></td></tr></table></figure></p>
<h3 id="resize"><a href="#resize" class="headerlink" title="resize"></a>resize</h3><p>VBoxManage clonehd “MavericksInstaller.vmdk” “MavericksInstaller.vdi” —format vdi<br>VBoxManage list hdds<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">UUID:           501b0eed-167e-4b38-96ff-93efa707b5fc</div><div class="line">Parent UUID:    base</div><div class="line">State:          created</div><div class="line">Type:           normal (base)</div><div class="line">Location:       xxx/macOS/MavericksInstaller.vdi</div><div class="line">Storage format: vdi</div><div class="line">Capacity:       10240 MBytes</div><div class="line">Encryption:     disabled</div></pre></td></tr></table></figure></p>
<p>VBoxManage modifyhd 501b0eed-167e-4b38-96ff-93efa707b5fc —resize 40960<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">UUID:           501b0eed-167e-4b38-96ff-93efa707b5fc</div><div class="line">Parent UUID:    base</div><div class="line">State:          created</div><div class="line">Type:           normal (base)</div><div class="line">Location:       xxx/macOS/MavericksInstaller.vdi</div><div class="line">Storage format: vdi</div><div class="line">Capacity:       40960 MBytes</div><div class="line">Encryption:     disabled</div></pre></td></tr></table></figure></p>
<h3 id="解决mediakit-报告设备上的空间不足以执行"><a href="#解决mediakit-报告设备上的空间不足以执行" class="headerlink" title="解决mediakit 报告设备上的空间不足以执行"></a>解决mediakit 报告设备上的空间不足以执行</h3><p>最简单粗暴的方法，重新挂载一个vdi，利用磁盘工具，抹掉，然后就行了。。。</p>
<h3 id="分辨率调整"><a href="#分辨率调整" class="headerlink" title="分辨率调整"></a>分辨率调整</h3><p>VBoxManage setextradata “macOS” VBoxInternal2/EfiGopMode 3</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul>
<li><a href="https://www.cnblogs.com/liming2017/p/7566953.html" target="_blank" rel="external">https://www.cnblogs.com/liming2017/p/7566953.html</a></li>
<li><a href="https://jingyan.baidu.com/article/1876c85274dcca890a13767a.html" target="_blank" rel="external">https://jingyan.baidu.com/article/1876c85274dcca890a13767a.html</a></li>
<li><a href="https://techsviewer.com/install-macos-sierra-virtualbox-windows/" target="_blank" rel="external">https://techsviewer.com/install-macos-sierra-virtualbox-windows/</a></li>
<li><a href="https://www.youtube.com/watch?v=pVc6rxk3OUM" target="_blank" rel="external">https://www.youtube.com/watch?v=pVc6rxk3OUM</a></li>
<li><a href="https://www.virtualbox.org/wiki/Download_Old_Builds_5_0" target="_blank" rel="external">https://www.virtualbox.org/wiki/Download_Old_Builds_5_0</a></li>
<li><a href="https://forums.virtualbox.org/viewtopic.php?f=6&amp;t=53880" target="_blank" rel="external">https://forums.virtualbox.org/viewtopic.php?f=6&amp;t=53880</a></li>
<li><a href="https://www.cnblogs.com/platero/p/4105808.html" target="_blank" rel="external">https://www.cnblogs.com/platero/p/4105808.html</a></li>
<li><a href="https://stackoverflow.com/questions/12349757/change-macos-x-guest-screen-resolution-for-virtualbox" target="_blank" rel="external">https://stackoverflow.com/questions/12349757/change-macos-x-guest-screen-resolution-for-virtualbox</a></li>
</ul>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/17/install-macOS-10-12-with-virtualbox/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/17/simple-speech-recognition-example-by-tensorflow/">
                            A simple audio recognition example by tensorflow
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-17T11:17:02+08:00">
	
		    Mar 17, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    
                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/17/simple-speech-recognition-example-by-tensorflow/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/15/recurrent-neural-network-and-lstm/">
                            Recurrent neural network and LSTM
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-15T09:56:04+08:00">
	
		    Mar 15, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>简单记录一下循环神经网络 (recurrent neural network, RNN)， 另一种RNN，主要关注时间序列的预测、分类和识别等问题。这里卖个瓜，前面有讨论过残差神经网络，感兴趣地可以去围观，链接见文末。<br>本文首先讨论RNN的motivation及其特点；然后是为了解决长程依赖 (long-term dependencies) 而提出的long short term memory (LSTM) 结构； 最后是二者在tensorflow中的简单样例。参考文献很多，这里强烈案例下面两篇文章！</p>
<ul>
<li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM</a></li>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
</ul>
<h4 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a>Recurrent neural network</h4><p>循环神经网络的动机是<em>刻画一个时间序列当前输入与此前信息的联系</em>，从网络结构上，循环神经网络通过称为<strong>循环体</strong>的模块 (如下图) 实现对信息的记忆，即该层在<script type="math/tex">t-1</script>时刻的输出状态<script type="math/tex">\mathbf{h_{t-1}}</script>会被记录，并作为<script type="math/tex">t</script>时刻该模块输入的一部分，以级联的形式与<script type="math/tex">\mathbf{x_t}</script>构成此刻的输入 <script type="math/tex">[\mathbf{h_{t-1}}, \mathbf{x_{t}}]</script>.</p>
<p>显然，循环体中的循环理论上是无穷的，但在实际应用中会限制循环的次数以避免<strong>梯度消失 (gradient vanishing) </strong>的问题，用<code>num_step</code>来定义，即循环体的基本模块被复制并展开为<code>num_step</code>个。如<br>文献[4]所述，循环体结构是RNN的基础，在RNN中对于复制展开的循环体，其参数是共享的。这一点与卷积神经网络中的权值共享有类似之处。在<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">这篇</a>文章里给出了非常多的RNN的应用场景，我很喜欢里面关于手写体识别的问题，体现了权值共享的效果。</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/images/fig-rnn.png?raw=true" height="140" width="500">
</center>

<p>设<script type="math/tex">t</script>时刻循环体的输入为<script type="math/tex">\mathbf{x}(t)</script>，$t-1$时刻循环体的输出状态为<script type="math/tex">\mathbf{h(t)}</script>，则RNN中<script type="math/tex">t</script>时刻的输出<script type="math/tex">\mathbf{h}(t)</script>为，</p>
<script type="math/tex; mode=display">
\begin{equation}
\mathbf{h}(t) = \rm{tanh}\left(W\cdot[\mathbf{h}(t-1),\mathbf{x}(t)] + b\right).
\end{equation}</script><p>其中<script type="math/tex">W</script>是权值矩阵，其shape为<script type="math/tex">[\mathrm{len}(\mathbf{h}) + \mathrm{len}(\mathbf{x}), \mathrm{len}(\mathbf{h})]</script>, <script type="math/tex">b</script>为偏置。这里采用的激活函数是tanh，将数据限制到<script type="math/tex">[-1,1]</script>之间。</p>
<p>那么，为什么用tanh，而不是有high reputation的ReLU? 知乎的<a href="https://www.zhihu.com/question/61265076/answer/186644426" target="_blank" rel="external">这个讨论</a>给出了很棒的解释，参考Hinton<a href="https://arxiv.org/abs/1504.00941" target="_blank" rel="external">论文</a>中的观点 <em>ReLUs seem inappropriate for RNNs because they can have very large outputs so they might be expected to be far more likely to explode than units thathave bounded values.</em> ReLU将输出的值限制在<script type="math/tex">[0, \infty)</script>之间，而RNN中循环体之间的权值是共享的，经过公式(1)的多次作用，相当于对<script type="math/tex">W</script>做了连乘，ReLU函数会导致梯度爆炸的问题。因此，采用tanh可以将每层的输出空控制在限定的范围内，既避免了梯度消失，也避免了梯度爆炸的问题。</p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>在时间序列的预测中存在长期依赖 (long-term dependencies) 的问题，即网络需要记住离时间<script type="math/tex">t</script>很远的某个时刻的信息，固定的<code>num_step</code>将不适用于这一情形，并且长时间间隔下的梯度消失问题将无法处理。因此，需要对RNN的循环体模块进行修改，即长短时记忆网络 (long short term memory, LSTM). </p>
<p>LSTM的基本模块如下图所示，参考<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM</a>的解释，<em>The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt.</em> 即LSTM的核心是组成为<code>cell state</code>，用于存储和记忆上文的信息，类似传送带的功能。</p>
<p>而<code>cell state</code>的更新，通过三个逻辑门以及<script type="math/tex">\mathbf{x_t}</script>和<script type="math/tex">\mathbf{h_{t-1}}</script>共同完成。它们分别称为 (1) forget gate, (2) input gate, (3) output gate. 采用sigmoid函数将数值规范化到<script type="math/tex">[0,1]</script>区间，并与待处理信号进行点乘，本质上实现软判决.</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/images/fig-lstm.png?raw=true" height="400" width="640">
</center>

<p>与RNN类似，首先将<script type="math/tex">t-1</script>时刻LSTM cell的输出<script type="math/tex">\mathbf{h}_{t-1}</script>与<script type="math/tex">t</script>时刻的输入<script type="math/tex">\mathbf{x}_{t}</script>进行级联，逐一通过三个门，</p>
<ul>
<li>遗忘门 (Forget gate)<script type="math/tex; mode=display">
\begin{equation}
  \mathbf{f_t} = \sigma\left(W_f \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_f\right) 
\end{equation}</script></li>
<li>输入门 (Input gate)<script type="math/tex; mode=display">
\begin{equation}
\mathbf{i_t} = \sigma\left(W_i \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_i \right).
\end{equation}</script></li>
<li>输出门 (Output gate)<script type="math/tex; mode=display">
\begin{equation}
\mathbf{o_t} = \sigma\left(W_o \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_o \right).
\end{equation}</script></li>
</ul>
<p>其中遗忘门的作用是<strong>抛弃cell state中不需要的信息</strong>，与<script type="math/tex">\mathbf{C_{t-1}}</script>作用； 输入门则是<strong>决定cell state中待更新的信息</strong>，与<script type="math/tex">\mathbf{\tilde{C}_t}</script>，即state candidates作用；输出门则从更新后的<code>cell state</code>中<strong>决定输出的状态</strong>。结合以上三个门结构，便可以更新<code>cell state</code>以及<code>cell output</code>,</p>
<ul>
<li>cell state update<script type="math/tex; mode=display">
\begin{align}
\mathbf{\tilde{C_{t}}} &= \rm{tanh}\left(W_c \cdot [\mathbf{h_{t-1}}, \mathbf{x_{t}}] + b_c \right) \\ 
\mathbf{C_t} &= \mathbf{f_t}\cdot\mathbf{C_{t-1}} + \mathbf{i_t} \cdot \mathbf{\tilde{C_{t}}}
\end{align}</script></li>
<li>cell output upadte<script type="math/tex; mode=display">
\begin{equation}
\mathbf{h_{t}} = \rm{tanh}(\mathbf{C_{t}}) \cdot \mathbf{o_t}
\end{equation}</script></li>
</ul>
<h4 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h4><p>参考《TensorFlow实战》第7章的LSTM基于<a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz" target="_blank" rel="external">PTB数据集</a>的语言预测样例，以及TensorFlow的<a href="https://www.tensorflow.org/tutorials/recurrent" target="_blank" rel="external">Tutorial</a>，设计了一个小实验，对比RNN和LSTM的performance，以及激活函数对于RNN的影响。(详细的notebooks见这里: <a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook-rnn-tanh.ipynb" target="_blank" rel="external">RNN</a>, <a href="https://github.com/myinxd/canal-notebooks/blob/master/deeplearning/notebook-lstm.ipynb" target="_blank" rel="external">LSTM</a>)</p>
<p>这里给出<code>tf.contrib.rnn</code>中提供的用于搭建RNN和LSTM cell的类的实例化方法，以及如何构建多个Recurrent层，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># RNN</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell</span><span class="params">(num_units, activation, reuse=None)</span>:</span></div><div class="line">   <span class="keyword">return</span> tf.contrib.rnn.BasicLSTMCell(</div><div class="line">       num_units=num_units,  </div><div class="line">       activation=activation,</div><div class="line">       reuse=reuse)</div><div class="line"></div><div class="line"><span class="comment"># LSTM</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell</span><span class="params">(num_units, forget_bias=<span class="number">0.0</span>, state_in_tuple=True, reuse=None)</span>:</span></div><div class="line">	<span class="keyword">return</span> tf.contrib.rnn.BasicLSTMCell(</div><div class="line">    	num_units=size, </div><div class="line">        forget_bias=forget_bias, </div><div class="line">        state_is_tuple=state_in_tuple,</div><div class="line">        reuse=reuse)</div><div class="line"></div><div class="line"><span class="comment"># Multiple layers</span></div><div class="line">attn_cell = rnn_cell</div><div class="line">numlayers = <span class="number">2</span></div><div class="line">cell = tf.contrib.rnn.MultiRNNCell(</div><div class="line">	[attn_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(numlayers)],</div><div class="line">    state_is_tuple=<span class="keyword">True</span>)</div></pre></td></tr></table></figure></p>
<p>另外，在PTB的TF教程里，设置了可变的学习率以及梯度的clipping用于抑制梯度爆炸 (gradient explosion) 的问题，代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Adjustable learning rate</span></div><div class="line">new_lr = tf.placeholder(tf.float32, shape=[], name=<span class="string">"new_learning_rate"</span>)</div><div class="line">lr_update = tf.assign(self._lr, self._new_lr) <span class="comment"># use tf.assign to transfer the updated lr</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">assign_lr</span><span class="params">(session, lr_value)</span>:</span></div><div class="line">	sess.run(self._lr_update, feed_dict=&#123;new_lr: lr_value&#125;)</div><div class="line"></div><div class="line"><span class="comment"># Gradient clipping</span></div><div class="line">...</div><div class="line">max_grad_norm = <span class="number">5.0</span> <span class="comment"># maximum gradient</span></div><div class="line">tvars = tf.trainable_variables()  <span class="comment"># Get all trainable variables</span></div><div class="line">grads, _ = tf.clip_by_global_norm(</div><div class="line">	tf.gradients(cost, tvars), max_grad_norm)</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(self._lr)</div><div class="line">train_op = optimizer.apply_gradients(</div><div class="line">    zip(grads, tvars),</div><div class="line">    global_step = tf.contrib.framework.get_or_create_global_step())</div></pre></td></tr></table></figure></p>
<p>下面来比较一下RNN和LSTM的效果。比较的指标是perplexity (复杂度)，用于刻画该模型能够估计出某一句话的概率，其数值越小，模型表现越好。</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/images/fig-rnn-lstm-cmp.png?raw=true" height="300" width="500">
</center>

<p>容易看出，LSTM的表现是优于RNN的。除此之外，采用tanh函数的RNN要显著好于采用ReLU，在训练中也出现了<em>RuntimeWarning: overflow encountered in exp</em>的警告，说明出现了gradient explosion的问题。最后，我尝试增加了RNN的层数，但是效果并没有变好，也许是参数多了？也有可能是我偷懒了，没多训测试几次。。。</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM</a><br>[2] <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a><br>[3] <a href="https://www.tensorflow.org/tutorials/recurrent" target="_blank" rel="external">Tensorflow tutorial</a><br>[4] TensorFlow实战Google深度学习框架<br>[5] TensorFlow实战<br>[6] <a href="https://www.zhihu.com/question/61265076/answer/186644426" target="_blank" rel="external">RNN中为什么要采用tanh而不是ReLu作为激活函数？</a></p>
<h3 id="广告位"><a href="#广告位" class="headerlink" title="广告位"></a>广告位</h3><p><a href="http://www.mazhixian.me/2018/01/21/resnet-with-tensorflow/">Residual network I — block and bottleneck</a><br><a href="http://www.mazhixian.me/2018/01/27/resnet-with-tensorflow-2/">Residual network II — realize with tensorflow</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/15/recurrent-neural-network-and-lstm/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2018/03/13/expectation-maximization-algorithm/">
                            Expectation maximization algorithm
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2018-03-13T15:16:35+08:00">
	
		    Mar 13, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>回顾一下GMM以及HMM等模型的求解方法，即著名的Expectation Maximization (EM) 算法。参考李航老师的《统计学习方法》，EM算法是一种迭代算法，用于含有隐含变量的概率模型参数的极大似然估计（MLE），或极大后验概率估计 (MPE).</p>
<h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>定义<script type="math/tex">Y</script>为观测随机变量，<script type="math/tex">Z</script>为隐含随机变量，则<script type="math/tex">Y</script>和<script type="math/tex">Z</script>一起构成完全数据 (complete-data)。假设待估计的概率模型参数为<script type="math/tex">\theta</script>，则观测数据的概率分布为<script type="math/tex">P(Y|\theta)</script>, 即为其似然函数，对应的对数似然函数为<script type="math/tex">L(\theta) = \log{P(Y|\theta)}</script>. 设<script type="math/tex">Y</script>和<script type="math/tex">Z</script>Z的联合概率分布为<script type="math/tex">P(Y,Z|\theta)</script>，那么完全数据的对数似然函数为<script type="math/tex">\log{P(Y,Z|\theta)}</script>.</p>
<p>EM算法的目的就是求解参数<script type="math/tex">\theta</script>，极大化观测量的对数似然函数<script type="math/tex">L(\theta)</script>。因为包含隐含量，所以采用迭代的方法，分为E (期望) 和M (极大化)两步，求解算法如下。</p>
<ol>
<li><p>初始化参数<script type="math/tex">\theta^{(0)}</script>;</p>
</li>
<li><p>E step: 记<script type="math/tex">\theta^{(i)}</script>为第<script type="math/tex">i</script>次迭代参数<script type="math/tex">\theta</script>的估计值，则第<script type="math/tex">i+1</script>次迭代，计算如下期望.</p>
<script type="math/tex; mode=display">
\begin{align}
Q(\theta,\theta^{(i)}) &= E_{z}[\log{P(Y,Z|\theta)}|Y,\theta^{(i)}] \notag \\
&=\sum_{z}{\log{P(Y,Z|\theta)P(Z|Y,\theta^{(i)})}},
\end{align}</script><p>其中<script type="math/tex">P(Z|Y,\theta^{(i)})</script>是在给定观测数据<script type="math/tex">Y</script>和当前估计的参数$\theta^{(i)}$下隐含变量<script type="math/tex">Z</script>的条件概率分布；<script type="math/tex">Q(\theta,\theta^{(i)})</script>定义为完全数据的对数似然函数在给定观测数据和当前参数下对隐含数据<script type="math/tex">Z</script>的条件概率分布<script type="math/tex">P(Z|Y,\theta^{(i)})</script>的期望，即<script type="math/tex">\log{P(Y|\theta)}</script>。</p>
</li>
<li><p>M step: 求使<script type="math/tex">Q(\theta, \theta^{(i)})</script>极大化的参数<script type="math/tex">\theta</script>，以确定第<script type="math/tex">i+1</script>次迭代的参数的估计值<script type="math/tex">\theta^{(i+1)}</script>, 即计算</p>
<script type="math/tex; mode=display">
\begin{align}
\theta^{(i+1)} = \mathrm{arg}\max\limits_{\theta}{Q(\theta,\theta^{(i)})}
\end{align}</script></li>
<li><p>重复E和M两步，直到收敛. </p>
</li>
</ol>
<p>李航老师也强调了一点：<strong>EM算法与初值的选择有关，选择不同的初值可能得到不同的参数估计值。</strong> 我在这个<a href="https://github.com/myinxd/canal-notebooks/blob/master/machinelearning/notebook-expectation-maximum-three-coiins.ipynb" target="_blank" rel="external">notebook</a>里给了一个样例，即“三硬币模型”。</p>
<h3 id="高斯混合模型的求解"><a href="#高斯混合模型的求解" class="headerlink" title="高斯混合模型的求解"></a>高斯混合模型的求解</h3><p>EM算法最经典的应用就是高斯混合模型的参数估计，该模型是语音信号处理的基础模型之一，其定义如下，</p>
<script type="math/tex; mode=display">
\begin{align}
P(\mathbf{y}|\mathbf{\theta}) = \sum_{k=1}^{K}{c_k\phi(\mathbf{y}|\mathbf{\theta_k)}}
\end{align}</script><p>其中，<script type="math/tex">c_k</script>是系数，<script type="math/tex">c_k\geq0</script>, <script type="math/tex">\sum_{k=1}^{K}{c_k=1}</script>; <script type="math/tex">\phi(\mathbf{y}|\mathbf{\theta_k})</script>是高斯分布，<script type="math/tex">\mathbf{\theta_k} = (\mathbf{\mu_k},\Sigma_k^2)</script>,</p>
<script type="math/tex; mode=display">
\begin{equation}
\phi(\mathbf{y}|\mathbf{\theta_k}) = \frac{1}{(2\pi)^{M/2}|\Sigma_k|}{-\exp{(\mathbf{y}-\mathbf{\mu_k})^{T}{\Sigma_{k}}^{-1}(\mathbf{y}-\mathbf{\mu_k})}}.
\end{equation}</script><p>在GMM模型中有，</p>
<ul>
<li>观测变量：<script type="math/tex">\mathbf{Y}</script>，</li>
<li>隐含变量：<script type="math/tex">\gamma_k\in{0,1}, k=1,~2,~,\cdots,~K</script>，表示<script type="math/tex">\mathbf{y}</script>是否来自第<script type="math/tex">k</script>个高斯分量</li>
<li>模型参数：<script type="math/tex">\mathbf{\theta} = {c_k, \mathbf{\theta_k}}, k=1,~2,~,\cdots,~K</script>.</li>
</ul>
<p>详细的推导见《统计学习方法》，这里给出GMM模型的E和M步骤.<br>设观测数据<script type="math/tex">\mathbf{Y} = \{\mathbf{y1},~\mathbf{y2},~\cdots,~\mathbf{y_N}\}</script></p>
<ul>
<li><p>E step</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{\gamma_{jk}} = \frac{c_k\phi(\mathbf{y_j}|\mathbf{\theta_k})}{\sum_{k=1}^{K}{c_k\phi(\mathbf{y_j}|\mathbf{\theta_k})}} 
\end{align}</script><p>其中<script type="math/tex">j=1,~2,~\cdots,~,N, k=1,~2,~\cdots,~K</script>.</p>
</li>
<li><p>M step</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{\mu}_{km} &= \sum_{j=1}^{N}{\hat{\gamma}_{jk}y_{jm}}/{\sum^{N}_{j=1}{\hat{\gamma}_{jk}}} 
\notag \\
\hat{\sigma}^{2}_{km} &= \sum_{j=1}^{N}{\hat{\gamma}_{jk}(y_{jm}-\mu_{km})^2}/{\sum_{j=1}^{N}{\hat{\gamma}_{jk}}} \notag \\
\hat{c}_{k} &= {\sum_{j=1}^{N}{\hat{\gamma}_{jk}}} / N
\end{align}</script><p>其中<script type="math/tex">m=1,~2,~\cdots,~,M, k=1,~2,~\cdots,~K</script>.</p>
</li>
</ul>
<p>最后，给出一个样例，简单的二维GMM，如下图，notebook见<a href="https://github.com/myinxd/canal-notebooks/blob/master/machinelearning/notebook-EM-GMM-2D.ipynb" target="_blank" rel="external">这里</a>.</p>
<center>
<img src="https://github.com/myinxd/canal-notebooks/blob/master/machinelearning/fig_gmm_2d.png?raw=true" height="600" width="600">
</center>

<p>可以看出，EM的估计效果还是不错的，并且提供初始化参数值的结果 (左下) 比随机初始化 (右下) 的结果要好。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] 李航，统计学系方法，2012 清华大学出版社<br>[2] <a href="https://stats.stackexchange.com/questions/70855/generating-random-variables-from-a-mixture-of-normal-distributions" target="_blank" rel="external">Generating random variables from a mixture of Normal distributions</a></p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/03/13/expectation-maximization-algorithm/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
        
          <li class="pagination-next">
            <a class="btn btn--default btn--small" href="/all-archives/page/2/">
              <span>OLDER POSTS</span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
        <li class="pagination-number">page 1 of 7</li>
    </ul>
</div>

</section>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Jason Ma. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/profile.jpg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Jason Ma</h4>
        
            <div id="about-card-bio"><p>We are in the same story.</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Astronomer? Software engineer</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Shanghai
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-peofhqjkzcghmndknakluequy1y6owxdwpaqyju9ntl9zxnk7rdolb3rjjoj.min.js"></script>
<!--SCRIPTS END--><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



    </body>
</html>
